digraph LLaMa { 
	graph [fontname="Arial", fontsize=10];
	node [shape="box"];

	0 [label="credit: 0
name: Input node
operator_type: Nop
"];
	1 [label="credit: 1
name: Output node
operator_type: Nop
"];
	2 [label="credit: 2
data type: Q8_0
name: token_embd.weight
shape: (4096, 32000) (34, 139264)
size: 4456448000
"];
	3 [label="credit: 3
name: token_embd.weight.fp32
operator_type: Quantize
"];
	4 [label="credit: 4
data type: FP32
name: token_embd.weight.fp32- out
shape: (4096, 32000) (34, 139264)
size: 4456448000
"];
	5 [label="credit: 5
data type: FP32
name: output_norm.weight
shape: (4096) (4)
size: 16384
"];
	6 [label="credit: 6
data type: Q8_0
name: output.weight
shape: (4096, 32000) (34, 139264)
size: 4456448000
"];
	7 [label="credit: 7
name: output.weight.fp32
operator_type: Quantize
"];
	8 [label="credit: 8
data type: FP32
name: output.weight.fp32- out
shape: (4096, 32000) (34, 139264)
size: 4456448000
"];
	9 [label="credit: 9
data type: FP32
name: blk.0.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	10 [label="credit: 10
data type: Q8_0
name: blk.0.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	11 [label="credit: 11
name: blk.0.attn_q.weight.fp32
operator_type: Quantize
"];
	12 [label="credit: 12
data type: FP32
name: blk.0.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	13 [label="credit: 13
data type: Q8_0
name: blk.0.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	14 [label="credit: 14
name: blk.0.attn_k.weight.fp32
operator_type: Quantize
"];
	15 [label="credit: 15
data type: FP32
name: blk.0.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	16 [label="credit: 16
data type: Q8_0
name: blk.0.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	17 [label="credit: 17
name: blk.0.attn_v.weight.fp32
operator_type: Quantize
"];
	18 [label="credit: 18
data type: FP32
name: blk.0.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	19 [label="credit: 19
data type: Q8_0
name: blk.0.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	20 [label="credit: 20
name: blk.0.attn_output.weight.fp32
operator_type: Quantize
"];
	21 [label="credit: 21
data type: FP32
name: blk.0.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	22 [label="credit: 22
data type: FP32
name: blk.0.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	23 [label="credit: 23
data type: Q8_0
name: blk.0.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	24 [label="credit: 24
name: blk.0.ffn_up.weight.fp32
operator_type: Quantize
"];
	25 [label="credit: 25
data type: FP32
name: blk.0.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	26 [label="credit: 26
data type: Q8_0
name: blk.0.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	27 [label="credit: 27
name: blk.0.ffn_gate.weight.fp32
operator_type: Quantize
"];
	28 [label="credit: 28
data type: FP32
name: blk.0.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	29 [label="credit: 29
data type: Q8_0
name: blk.0.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	30 [label="credit: 30
name: blk.0.ffn_down.weight.fp32
operator_type: Quantize
"];
	31 [label="credit: 31
data type: FP32
name: blk.0.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	32 [label="credit: 32
data type: FP32
name: blk.1.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	33 [label="credit: 33
data type: Q8_0
name: blk.1.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	34 [label="credit: 34
name: blk.1.attn_q.weight.fp32
operator_type: Quantize
"];
	35 [label="credit: 35
data type: FP32
name: blk.1.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	36 [label="credit: 36
data type: Q8_0
name: blk.1.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	37 [label="credit: 37
name: blk.1.attn_k.weight.fp32
operator_type: Quantize
"];
	38 [label="credit: 38
data type: FP32
name: blk.1.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	39 [label="credit: 39
data type: Q8_0
name: blk.1.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	40 [label="credit: 40
name: blk.1.attn_v.weight.fp32
operator_type: Quantize
"];
	41 [label="credit: 41
data type: FP32
name: blk.1.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	42 [label="credit: 42
data type: Q8_0
name: blk.1.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	43 [label="credit: 43
name: blk.1.attn_output.weight.fp32
operator_type: Quantize
"];
	44 [label="credit: 44
data type: FP32
name: blk.1.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	45 [label="credit: 45
data type: FP32
name: blk.1.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	46 [label="credit: 46
data type: Q8_0
name: blk.1.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	47 [label="credit: 47
name: blk.1.ffn_up.weight.fp32
operator_type: Quantize
"];
	48 [label="credit: 48
data type: FP32
name: blk.1.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	49 [label="credit: 49
data type: Q8_0
name: blk.1.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	50 [label="credit: 50
name: blk.1.ffn_gate.weight.fp32
operator_type: Quantize
"];
	51 [label="credit: 51
data type: FP32
name: blk.1.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	52 [label="credit: 52
data type: Q8_0
name: blk.1.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	53 [label="credit: 53
name: blk.1.ffn_down.weight.fp32
operator_type: Quantize
"];
	54 [label="credit: 54
data type: FP32
name: blk.1.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	55 [label="credit: 55
data type: FP32
name: blk.2.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	56 [label="credit: 56
data type: Q8_0
name: blk.2.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	57 [label="credit: 57
name: blk.2.attn_q.weight.fp32
operator_type: Quantize
"];
	58 [label="credit: 58
data type: FP32
name: blk.2.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	59 [label="credit: 59
data type: Q8_0
name: blk.2.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	60 [label="credit: 60
name: blk.2.attn_k.weight.fp32
operator_type: Quantize
"];
	61 [label="credit: 61
data type: FP32
name: blk.2.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	62 [label="credit: 62
data type: Q8_0
name: blk.2.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	63 [label="credit: 63
name: blk.2.attn_v.weight.fp32
operator_type: Quantize
"];
	64 [label="credit: 64
data type: FP32
name: blk.2.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	65 [label="credit: 65
data type: Q8_0
name: blk.2.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	66 [label="credit: 66
name: blk.2.attn_output.weight.fp32
operator_type: Quantize
"];
	67 [label="credit: 67
data type: FP32
name: blk.2.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	68 [label="credit: 68
data type: FP32
name: blk.2.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	69 [label="credit: 69
data type: Q8_0
name: blk.2.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	70 [label="credit: 70
name: blk.2.ffn_up.weight.fp32
operator_type: Quantize
"];
	71 [label="credit: 71
data type: FP32
name: blk.2.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	72 [label="credit: 72
data type: Q8_0
name: blk.2.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	73 [label="credit: 73
name: blk.2.ffn_gate.weight.fp32
operator_type: Quantize
"];
	74 [label="credit: 74
data type: FP32
name: blk.2.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	75 [label="credit: 75
data type: Q8_0
name: blk.2.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	76 [label="credit: 76
name: blk.2.ffn_down.weight.fp32
operator_type: Quantize
"];
	77 [label="credit: 77
data type: FP32
name: blk.2.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	78 [label="credit: 78
data type: FP32
name: blk.3.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	79 [label="credit: 79
data type: Q8_0
name: blk.3.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	80 [label="credit: 80
name: blk.3.attn_q.weight.fp32
operator_type: Quantize
"];
	81 [label="credit: 81
data type: FP32
name: blk.3.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	82 [label="credit: 82
data type: Q8_0
name: blk.3.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	83 [label="credit: 83
name: blk.3.attn_k.weight.fp32
operator_type: Quantize
"];
	84 [label="credit: 84
data type: FP32
name: blk.3.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	85 [label="credit: 85
data type: Q8_0
name: blk.3.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	86 [label="credit: 86
name: blk.3.attn_v.weight.fp32
operator_type: Quantize
"];
	87 [label="credit: 87
data type: FP32
name: blk.3.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	88 [label="credit: 88
data type: Q8_0
name: blk.3.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	89 [label="credit: 89
name: blk.3.attn_output.weight.fp32
operator_type: Quantize
"];
	90 [label="credit: 90
data type: FP32
name: blk.3.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	91 [label="credit: 91
data type: FP32
name: blk.3.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	92 [label="credit: 92
data type: Q8_0
name: blk.3.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	93 [label="credit: 93
name: blk.3.ffn_up.weight.fp32
operator_type: Quantize
"];
	94 [label="credit: 94
data type: FP32
name: blk.3.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	95 [label="credit: 95
data type: Q8_0
name: blk.3.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	96 [label="credit: 96
name: blk.3.ffn_gate.weight.fp32
operator_type: Quantize
"];
	97 [label="credit: 97
data type: FP32
name: blk.3.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	98 [label="credit: 98
data type: Q8_0
name: blk.3.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	99 [label="credit: 99
name: blk.3.ffn_down.weight.fp32
operator_type: Quantize
"];
	100 [label="credit: 100
data type: FP32
name: blk.3.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	101 [label="credit: 101
data type: FP32
name: blk.4.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	102 [label="credit: 102
data type: Q8_0
name: blk.4.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	103 [label="credit: 103
name: blk.4.attn_q.weight.fp32
operator_type: Quantize
"];
	104 [label="credit: 104
data type: FP32
name: blk.4.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	105 [label="credit: 105
data type: Q8_0
name: blk.4.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	106 [label="credit: 106
name: blk.4.attn_k.weight.fp32
operator_type: Quantize
"];
	107 [label="credit: 107
data type: FP32
name: blk.4.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	108 [label="credit: 108
data type: Q8_0
name: blk.4.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	109 [label="credit: 109
name: blk.4.attn_v.weight.fp32
operator_type: Quantize
"];
	110 [label="credit: 110
data type: FP32
name: blk.4.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	111 [label="credit: 111
data type: Q8_0
name: blk.4.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	112 [label="credit: 112
name: blk.4.attn_output.weight.fp32
operator_type: Quantize
"];
	113 [label="credit: 113
data type: FP32
name: blk.4.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	114 [label="credit: 114
data type: FP32
name: blk.4.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	115 [label="credit: 115
data type: Q8_0
name: blk.4.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	116 [label="credit: 116
name: blk.4.ffn_up.weight.fp32
operator_type: Quantize
"];
	117 [label="credit: 117
data type: FP32
name: blk.4.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	118 [label="credit: 118
data type: Q8_0
name: blk.4.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	119 [label="credit: 119
name: blk.4.ffn_gate.weight.fp32
operator_type: Quantize
"];
	120 [label="credit: 120
data type: FP32
name: blk.4.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	121 [label="credit: 121
data type: Q8_0
name: blk.4.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	122 [label="credit: 122
name: blk.4.ffn_down.weight.fp32
operator_type: Quantize
"];
	123 [label="credit: 123
data type: FP32
name: blk.4.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	124 [label="credit: 124
data type: FP32
name: blk.5.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	125 [label="credit: 125
data type: Q8_0
name: blk.5.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	126 [label="credit: 126
name: blk.5.attn_q.weight.fp32
operator_type: Quantize
"];
	127 [label="credit: 127
data type: FP32
name: blk.5.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	128 [label="credit: 128
data type: Q8_0
name: blk.5.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	129 [label="credit: 129
name: blk.5.attn_k.weight.fp32
operator_type: Quantize
"];
	130 [label="credit: 130
data type: FP32
name: blk.5.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	131 [label="credit: 131
data type: Q8_0
name: blk.5.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	132 [label="credit: 132
name: blk.5.attn_v.weight.fp32
operator_type: Quantize
"];
	133 [label="credit: 133
data type: FP32
name: blk.5.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	134 [label="credit: 134
data type: Q8_0
name: blk.5.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	135 [label="credit: 135
name: blk.5.attn_output.weight.fp32
operator_type: Quantize
"];
	136 [label="credit: 136
data type: FP32
name: blk.5.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	137 [label="credit: 137
data type: FP32
name: blk.5.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	138 [label="credit: 138
data type: Q8_0
name: blk.5.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	139 [label="credit: 139
name: blk.5.ffn_up.weight.fp32
operator_type: Quantize
"];
	140 [label="credit: 140
data type: FP32
name: blk.5.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	141 [label="credit: 141
data type: Q8_0
name: blk.5.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	142 [label="credit: 142
name: blk.5.ffn_gate.weight.fp32
operator_type: Quantize
"];
	143 [label="credit: 143
data type: FP32
name: blk.5.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	144 [label="credit: 144
data type: Q8_0
name: blk.5.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	145 [label="credit: 145
name: blk.5.ffn_down.weight.fp32
operator_type: Quantize
"];
	146 [label="credit: 146
data type: FP32
name: blk.5.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	147 [label="credit: 147
data type: FP32
name: blk.6.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	148 [label="credit: 148
data type: Q8_0
name: blk.6.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	149 [label="credit: 149
name: blk.6.attn_q.weight.fp32
operator_type: Quantize
"];
	150 [label="credit: 150
data type: FP32
name: blk.6.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	151 [label="credit: 151
data type: Q8_0
name: blk.6.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	152 [label="credit: 152
name: blk.6.attn_k.weight.fp32
operator_type: Quantize
"];
	153 [label="credit: 153
data type: FP32
name: blk.6.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	154 [label="credit: 154
data type: Q8_0
name: blk.6.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	155 [label="credit: 155
name: blk.6.attn_v.weight.fp32
operator_type: Quantize
"];
	156 [label="credit: 156
data type: FP32
name: blk.6.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	157 [label="credit: 157
data type: Q8_0
name: blk.6.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	158 [label="credit: 158
name: blk.6.attn_output.weight.fp32
operator_type: Quantize
"];
	159 [label="credit: 159
data type: FP32
name: blk.6.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	160 [label="credit: 160
data type: FP32
name: blk.6.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	161 [label="credit: 161
data type: Q8_0
name: blk.6.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	162 [label="credit: 162
name: blk.6.ffn_up.weight.fp32
operator_type: Quantize
"];
	163 [label="credit: 163
data type: FP32
name: blk.6.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	164 [label="credit: 164
data type: Q8_0
name: blk.6.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	165 [label="credit: 165
name: blk.6.ffn_gate.weight.fp32
operator_type: Quantize
"];
	166 [label="credit: 166
data type: FP32
name: blk.6.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	167 [label="credit: 167
data type: Q8_0
name: blk.6.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	168 [label="credit: 168
name: blk.6.ffn_down.weight.fp32
operator_type: Quantize
"];
	169 [label="credit: 169
data type: FP32
name: blk.6.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	170 [label="credit: 170
data type: FP32
name: blk.7.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	171 [label="credit: 171
data type: Q8_0
name: blk.7.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	172 [label="credit: 172
name: blk.7.attn_q.weight.fp32
operator_type: Quantize
"];
	173 [label="credit: 173
data type: FP32
name: blk.7.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	174 [label="credit: 174
data type: Q8_0
name: blk.7.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	175 [label="credit: 175
name: blk.7.attn_k.weight.fp32
operator_type: Quantize
"];
	176 [label="credit: 176
data type: FP32
name: blk.7.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	177 [label="credit: 177
data type: Q8_0
name: blk.7.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	178 [label="credit: 178
name: blk.7.attn_v.weight.fp32
operator_type: Quantize
"];
	179 [label="credit: 179
data type: FP32
name: blk.7.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	180 [label="credit: 180
data type: Q8_0
name: blk.7.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	181 [label="credit: 181
name: blk.7.attn_output.weight.fp32
operator_type: Quantize
"];
	182 [label="credit: 182
data type: FP32
name: blk.7.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	183 [label="credit: 183
data type: FP32
name: blk.7.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	184 [label="credit: 184
data type: Q8_0
name: blk.7.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	185 [label="credit: 185
name: blk.7.ffn_up.weight.fp32
operator_type: Quantize
"];
	186 [label="credit: 186
data type: FP32
name: blk.7.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	187 [label="credit: 187
data type: Q8_0
name: blk.7.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	188 [label="credit: 188
name: blk.7.ffn_gate.weight.fp32
operator_type: Quantize
"];
	189 [label="credit: 189
data type: FP32
name: blk.7.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	190 [label="credit: 190
data type: Q8_0
name: blk.7.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	191 [label="credit: 191
name: blk.7.ffn_down.weight.fp32
operator_type: Quantize
"];
	192 [label="credit: 192
data type: FP32
name: blk.7.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	193 [label="credit: 193
data type: FP32
name: blk.8.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	194 [label="credit: 194
data type: Q8_0
name: blk.8.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	195 [label="credit: 195
name: blk.8.attn_q.weight.fp32
operator_type: Quantize
"];
	196 [label="credit: 196
data type: FP32
name: blk.8.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	197 [label="credit: 197
data type: Q8_0
name: blk.8.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	198 [label="credit: 198
name: blk.8.attn_k.weight.fp32
operator_type: Quantize
"];
	199 [label="credit: 199
data type: FP32
name: blk.8.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	200 [label="credit: 200
data type: Q8_0
name: blk.8.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	201 [label="credit: 201
name: blk.8.attn_v.weight.fp32
operator_type: Quantize
"];
	202 [label="credit: 202
data type: FP32
name: blk.8.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	203 [label="credit: 203
data type: Q8_0
name: blk.8.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	204 [label="credit: 204
name: blk.8.attn_output.weight.fp32
operator_type: Quantize
"];
	205 [label="credit: 205
data type: FP32
name: blk.8.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	206 [label="credit: 206
data type: FP32
name: blk.8.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	207 [label="credit: 207
data type: Q8_0
name: blk.8.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	208 [label="credit: 208
name: blk.8.ffn_up.weight.fp32
operator_type: Quantize
"];
	209 [label="credit: 209
data type: FP32
name: blk.8.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	210 [label="credit: 210
data type: Q8_0
name: blk.8.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	211 [label="credit: 211
name: blk.8.ffn_gate.weight.fp32
operator_type: Quantize
"];
	212 [label="credit: 212
data type: FP32
name: blk.8.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	213 [label="credit: 213
data type: Q8_0
name: blk.8.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	214 [label="credit: 214
name: blk.8.ffn_down.weight.fp32
operator_type: Quantize
"];
	215 [label="credit: 215
data type: FP32
name: blk.8.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	216 [label="credit: 216
data type: FP32
name: blk.9.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	217 [label="credit: 217
data type: Q8_0
name: blk.9.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	218 [label="credit: 218
name: blk.9.attn_q.weight.fp32
operator_type: Quantize
"];
	219 [label="credit: 219
data type: FP32
name: blk.9.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	220 [label="credit: 220
data type: Q8_0
name: blk.9.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	221 [label="credit: 221
name: blk.9.attn_k.weight.fp32
operator_type: Quantize
"];
	222 [label="credit: 222
data type: FP32
name: blk.9.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	223 [label="credit: 223
data type: Q8_0
name: blk.9.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	224 [label="credit: 224
name: blk.9.attn_v.weight.fp32
operator_type: Quantize
"];
	225 [label="credit: 225
data type: FP32
name: blk.9.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	226 [label="credit: 226
data type: Q8_0
name: blk.9.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	227 [label="credit: 227
name: blk.9.attn_output.weight.fp32
operator_type: Quantize
"];
	228 [label="credit: 228
data type: FP32
name: blk.9.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	229 [label="credit: 229
data type: FP32
name: blk.9.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	230 [label="credit: 230
data type: Q8_0
name: blk.9.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	231 [label="credit: 231
name: blk.9.ffn_up.weight.fp32
operator_type: Quantize
"];
	232 [label="credit: 232
data type: FP32
name: blk.9.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	233 [label="credit: 233
data type: Q8_0
name: blk.9.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	234 [label="credit: 234
name: blk.9.ffn_gate.weight.fp32
operator_type: Quantize
"];
	235 [label="credit: 235
data type: FP32
name: blk.9.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	236 [label="credit: 236
data type: Q8_0
name: blk.9.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	237 [label="credit: 237
name: blk.9.ffn_down.weight.fp32
operator_type: Quantize
"];
	238 [label="credit: 238
data type: FP32
name: blk.9.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	239 [label="credit: 239
data type: FP32
name: blk.10.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	240 [label="credit: 240
data type: Q8_0
name: blk.10.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	241 [label="credit: 241
name: blk.10.attn_q.weight.fp32
operator_type: Quantize
"];
	242 [label="credit: 242
data type: FP32
name: blk.10.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	243 [label="credit: 243
data type: Q8_0
name: blk.10.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	244 [label="credit: 244
name: blk.10.attn_k.weight.fp32
operator_type: Quantize
"];
	245 [label="credit: 245
data type: FP32
name: blk.10.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	246 [label="credit: 246
data type: Q8_0
name: blk.10.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	247 [label="credit: 247
name: blk.10.attn_v.weight.fp32
operator_type: Quantize
"];
	248 [label="credit: 248
data type: FP32
name: blk.10.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	249 [label="credit: 249
data type: Q8_0
name: blk.10.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	250 [label="credit: 250
name: blk.10.attn_output.weight.fp32
operator_type: Quantize
"];
	251 [label="credit: 251
data type: FP32
name: blk.10.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	252 [label="credit: 252
data type: FP32
name: blk.10.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	253 [label="credit: 253
data type: Q8_0
name: blk.10.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	254 [label="credit: 254
name: blk.10.ffn_up.weight.fp32
operator_type: Quantize
"];
	255 [label="credit: 255
data type: FP32
name: blk.10.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	256 [label="credit: 256
data type: Q8_0
name: blk.10.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	257 [label="credit: 257
name: blk.10.ffn_gate.weight.fp32
operator_type: Quantize
"];
	258 [label="credit: 258
data type: FP32
name: blk.10.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	259 [label="credit: 259
data type: Q8_0
name: blk.10.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	260 [label="credit: 260
name: blk.10.ffn_down.weight.fp32
operator_type: Quantize
"];
	261 [label="credit: 261
data type: FP32
name: blk.10.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	262 [label="credit: 262
data type: FP32
name: blk.11.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	263 [label="credit: 263
data type: Q8_0
name: blk.11.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	264 [label="credit: 264
name: blk.11.attn_q.weight.fp32
operator_type: Quantize
"];
	265 [label="credit: 265
data type: FP32
name: blk.11.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	266 [label="credit: 266
data type: Q8_0
name: blk.11.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	267 [label="credit: 267
name: blk.11.attn_k.weight.fp32
operator_type: Quantize
"];
	268 [label="credit: 268
data type: FP32
name: blk.11.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	269 [label="credit: 269
data type: Q8_0
name: blk.11.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	270 [label="credit: 270
name: blk.11.attn_v.weight.fp32
operator_type: Quantize
"];
	271 [label="credit: 271
data type: FP32
name: blk.11.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	272 [label="credit: 272
data type: Q8_0
name: blk.11.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	273 [label="credit: 273
name: blk.11.attn_output.weight.fp32
operator_type: Quantize
"];
	274 [label="credit: 274
data type: FP32
name: blk.11.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	275 [label="credit: 275
data type: FP32
name: blk.11.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	276 [label="credit: 276
data type: Q8_0
name: blk.11.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	277 [label="credit: 277
name: blk.11.ffn_up.weight.fp32
operator_type: Quantize
"];
	278 [label="credit: 278
data type: FP32
name: blk.11.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	279 [label="credit: 279
data type: Q8_0
name: blk.11.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	280 [label="credit: 280
name: blk.11.ffn_gate.weight.fp32
operator_type: Quantize
"];
	281 [label="credit: 281
data type: FP32
name: blk.11.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	282 [label="credit: 282
data type: Q8_0
name: blk.11.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	283 [label="credit: 283
name: blk.11.ffn_down.weight.fp32
operator_type: Quantize
"];
	284 [label="credit: 284
data type: FP32
name: blk.11.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	285 [label="credit: 285
data type: FP32
name: blk.12.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	286 [label="credit: 286
data type: Q8_0
name: blk.12.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	287 [label="credit: 287
name: blk.12.attn_q.weight.fp32
operator_type: Quantize
"];
	288 [label="credit: 288
data type: FP32
name: blk.12.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	289 [label="credit: 289
data type: Q8_0
name: blk.12.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	290 [label="credit: 290
name: blk.12.attn_k.weight.fp32
operator_type: Quantize
"];
	291 [label="credit: 291
data type: FP32
name: blk.12.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	292 [label="credit: 292
data type: Q8_0
name: blk.12.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	293 [label="credit: 293
name: blk.12.attn_v.weight.fp32
operator_type: Quantize
"];
	294 [label="credit: 294
data type: FP32
name: blk.12.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	295 [label="credit: 295
data type: Q8_0
name: blk.12.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	296 [label="credit: 296
name: blk.12.attn_output.weight.fp32
operator_type: Quantize
"];
	297 [label="credit: 297
data type: FP32
name: blk.12.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	298 [label="credit: 298
data type: FP32
name: blk.12.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	299 [label="credit: 299
data type: Q8_0
name: blk.12.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	300 [label="credit: 300
name: blk.12.ffn_up.weight.fp32
operator_type: Quantize
"];
	301 [label="credit: 301
data type: FP32
name: blk.12.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	302 [label="credit: 302
data type: Q8_0
name: blk.12.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	303 [label="credit: 303
name: blk.12.ffn_gate.weight.fp32
operator_type: Quantize
"];
	304 [label="credit: 304
data type: FP32
name: blk.12.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	305 [label="credit: 305
data type: Q8_0
name: blk.12.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	306 [label="credit: 306
name: blk.12.ffn_down.weight.fp32
operator_type: Quantize
"];
	307 [label="credit: 307
data type: FP32
name: blk.12.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	308 [label="credit: 308
data type: FP32
name: blk.13.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	309 [label="credit: 309
data type: Q8_0
name: blk.13.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	310 [label="credit: 310
name: blk.13.attn_q.weight.fp32
operator_type: Quantize
"];
	311 [label="credit: 311
data type: FP32
name: blk.13.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	312 [label="credit: 312
data type: Q8_0
name: blk.13.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	313 [label="credit: 313
name: blk.13.attn_k.weight.fp32
operator_type: Quantize
"];
	314 [label="credit: 314
data type: FP32
name: blk.13.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	315 [label="credit: 315
data type: Q8_0
name: blk.13.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	316 [label="credit: 316
name: blk.13.attn_v.weight.fp32
operator_type: Quantize
"];
	317 [label="credit: 317
data type: FP32
name: blk.13.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	318 [label="credit: 318
data type: Q8_0
name: blk.13.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	319 [label="credit: 319
name: blk.13.attn_output.weight.fp32
operator_type: Quantize
"];
	320 [label="credit: 320
data type: FP32
name: blk.13.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	321 [label="credit: 321
data type: FP32
name: blk.13.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	322 [label="credit: 322
data type: Q8_0
name: blk.13.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	323 [label="credit: 323
name: blk.13.ffn_up.weight.fp32
operator_type: Quantize
"];
	324 [label="credit: 324
data type: FP32
name: blk.13.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	325 [label="credit: 325
data type: Q8_0
name: blk.13.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	326 [label="credit: 326
name: blk.13.ffn_gate.weight.fp32
operator_type: Quantize
"];
	327 [label="credit: 327
data type: FP32
name: blk.13.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	328 [label="credit: 328
data type: Q8_0
name: blk.13.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	329 [label="credit: 329
name: blk.13.ffn_down.weight.fp32
operator_type: Quantize
"];
	330 [label="credit: 330
data type: FP32
name: blk.13.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	331 [label="credit: 331
data type: FP32
name: blk.14.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	332 [label="credit: 332
data type: Q8_0
name: blk.14.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	333 [label="credit: 333
name: blk.14.attn_q.weight.fp32
operator_type: Quantize
"];
	334 [label="credit: 334
data type: FP32
name: blk.14.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	335 [label="credit: 335
data type: Q8_0
name: blk.14.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	336 [label="credit: 336
name: blk.14.attn_k.weight.fp32
operator_type: Quantize
"];
	337 [label="credit: 337
data type: FP32
name: blk.14.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	338 [label="credit: 338
data type: Q8_0
name: blk.14.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	339 [label="credit: 339
name: blk.14.attn_v.weight.fp32
operator_type: Quantize
"];
	340 [label="credit: 340
data type: FP32
name: blk.14.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	341 [label="credit: 341
data type: Q8_0
name: blk.14.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	342 [label="credit: 342
name: blk.14.attn_output.weight.fp32
operator_type: Quantize
"];
	343 [label="credit: 343
data type: FP32
name: blk.14.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	344 [label="credit: 344
data type: FP32
name: blk.14.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	345 [label="credit: 345
data type: Q8_0
name: blk.14.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	346 [label="credit: 346
name: blk.14.ffn_up.weight.fp32
operator_type: Quantize
"];
	347 [label="credit: 347
data type: FP32
name: blk.14.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	348 [label="credit: 348
data type: Q8_0
name: blk.14.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	349 [label="credit: 349
name: blk.14.ffn_gate.weight.fp32
operator_type: Quantize
"];
	350 [label="credit: 350
data type: FP32
name: blk.14.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	351 [label="credit: 351
data type: Q8_0
name: blk.14.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	352 [label="credit: 352
name: blk.14.ffn_down.weight.fp32
operator_type: Quantize
"];
	353 [label="credit: 353
data type: FP32
name: blk.14.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	354 [label="credit: 354
data type: FP32
name: blk.15.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	355 [label="credit: 355
data type: Q8_0
name: blk.15.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	356 [label="credit: 356
name: blk.15.attn_q.weight.fp32
operator_type: Quantize
"];
	357 [label="credit: 357
data type: FP32
name: blk.15.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	358 [label="credit: 358
data type: Q8_0
name: blk.15.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	359 [label="credit: 359
name: blk.15.attn_k.weight.fp32
operator_type: Quantize
"];
	360 [label="credit: 360
data type: FP32
name: blk.15.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	361 [label="credit: 361
data type: Q8_0
name: blk.15.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	362 [label="credit: 362
name: blk.15.attn_v.weight.fp32
operator_type: Quantize
"];
	363 [label="credit: 363
data type: FP32
name: blk.15.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	364 [label="credit: 364
data type: Q8_0
name: blk.15.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	365 [label="credit: 365
name: blk.15.attn_output.weight.fp32
operator_type: Quantize
"];
	366 [label="credit: 366
data type: FP32
name: blk.15.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	367 [label="credit: 367
data type: FP32
name: blk.15.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	368 [label="credit: 368
data type: Q8_0
name: blk.15.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	369 [label="credit: 369
name: blk.15.ffn_up.weight.fp32
operator_type: Quantize
"];
	370 [label="credit: 370
data type: FP32
name: blk.15.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	371 [label="credit: 371
data type: Q8_0
name: blk.15.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	372 [label="credit: 372
name: blk.15.ffn_gate.weight.fp32
operator_type: Quantize
"];
	373 [label="credit: 373
data type: FP32
name: blk.15.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	374 [label="credit: 374
data type: Q8_0
name: blk.15.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	375 [label="credit: 375
name: blk.15.ffn_down.weight.fp32
operator_type: Quantize
"];
	376 [label="credit: 376
data type: FP32
name: blk.15.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	377 [label="credit: 377
data type: FP32
name: blk.16.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	378 [label="credit: 378
data type: Q8_0
name: blk.16.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	379 [label="credit: 379
name: blk.16.attn_q.weight.fp32
operator_type: Quantize
"];
	380 [label="credit: 380
data type: FP32
name: blk.16.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	381 [label="credit: 381
data type: Q8_0
name: blk.16.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	382 [label="credit: 382
name: blk.16.attn_k.weight.fp32
operator_type: Quantize
"];
	383 [label="credit: 383
data type: FP32
name: blk.16.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	384 [label="credit: 384
data type: Q8_0
name: blk.16.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	385 [label="credit: 385
name: blk.16.attn_v.weight.fp32
operator_type: Quantize
"];
	386 [label="credit: 386
data type: FP32
name: blk.16.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	387 [label="credit: 387
data type: Q8_0
name: blk.16.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	388 [label="credit: 388
name: blk.16.attn_output.weight.fp32
operator_type: Quantize
"];
	389 [label="credit: 389
data type: FP32
name: blk.16.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	390 [label="credit: 390
data type: FP32
name: blk.16.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	391 [label="credit: 391
data type: Q8_0
name: blk.16.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	392 [label="credit: 392
name: blk.16.ffn_up.weight.fp32
operator_type: Quantize
"];
	393 [label="credit: 393
data type: FP32
name: blk.16.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	394 [label="credit: 394
data type: Q8_0
name: blk.16.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	395 [label="credit: 395
name: blk.16.ffn_gate.weight.fp32
operator_type: Quantize
"];
	396 [label="credit: 396
data type: FP32
name: blk.16.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	397 [label="credit: 397
data type: Q8_0
name: blk.16.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	398 [label="credit: 398
name: blk.16.ffn_down.weight.fp32
operator_type: Quantize
"];
	399 [label="credit: 399
data type: FP32
name: blk.16.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	400 [label="credit: 400
data type: FP32
name: blk.17.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	401 [label="credit: 401
data type: Q8_0
name: blk.17.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	402 [label="credit: 402
name: blk.17.attn_q.weight.fp32
operator_type: Quantize
"];
	403 [label="credit: 403
data type: FP32
name: blk.17.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	404 [label="credit: 404
data type: Q8_0
name: blk.17.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	405 [label="credit: 405
name: blk.17.attn_k.weight.fp32
operator_type: Quantize
"];
	406 [label="credit: 406
data type: FP32
name: blk.17.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	407 [label="credit: 407
data type: Q8_0
name: blk.17.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	408 [label="credit: 408
name: blk.17.attn_v.weight.fp32
operator_type: Quantize
"];
	409 [label="credit: 409
data type: FP32
name: blk.17.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	410 [label="credit: 410
data type: Q8_0
name: blk.17.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	411 [label="credit: 411
name: blk.17.attn_output.weight.fp32
operator_type: Quantize
"];
	412 [label="credit: 412
data type: FP32
name: blk.17.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	413 [label="credit: 413
data type: FP32
name: blk.17.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	414 [label="credit: 414
data type: Q8_0
name: blk.17.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	415 [label="credit: 415
name: blk.17.ffn_up.weight.fp32
operator_type: Quantize
"];
	416 [label="credit: 416
data type: FP32
name: blk.17.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	417 [label="credit: 417
data type: Q8_0
name: blk.17.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	418 [label="credit: 418
name: blk.17.ffn_gate.weight.fp32
operator_type: Quantize
"];
	419 [label="credit: 419
data type: FP32
name: blk.17.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	420 [label="credit: 420
data type: Q8_0
name: blk.17.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	421 [label="credit: 421
name: blk.17.ffn_down.weight.fp32
operator_type: Quantize
"];
	422 [label="credit: 422
data type: FP32
name: blk.17.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	423 [label="credit: 423
data type: FP32
name: blk.18.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	424 [label="credit: 424
data type: Q8_0
name: blk.18.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	425 [label="credit: 425
name: blk.18.attn_q.weight.fp32
operator_type: Quantize
"];
	426 [label="credit: 426
data type: FP32
name: blk.18.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	427 [label="credit: 427
data type: Q8_0
name: blk.18.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	428 [label="credit: 428
name: blk.18.attn_k.weight.fp32
operator_type: Quantize
"];
	429 [label="credit: 429
data type: FP32
name: blk.18.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	430 [label="credit: 430
data type: Q8_0
name: blk.18.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	431 [label="credit: 431
name: blk.18.attn_v.weight.fp32
operator_type: Quantize
"];
	432 [label="credit: 432
data type: FP32
name: blk.18.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	433 [label="credit: 433
data type: Q8_0
name: blk.18.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	434 [label="credit: 434
name: blk.18.attn_output.weight.fp32
operator_type: Quantize
"];
	435 [label="credit: 435
data type: FP32
name: blk.18.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	436 [label="credit: 436
data type: FP32
name: blk.18.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	437 [label="credit: 437
data type: Q8_0
name: blk.18.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	438 [label="credit: 438
name: blk.18.ffn_up.weight.fp32
operator_type: Quantize
"];
	439 [label="credit: 439
data type: FP32
name: blk.18.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	440 [label="credit: 440
data type: Q8_0
name: blk.18.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	441 [label="credit: 441
name: blk.18.ffn_gate.weight.fp32
operator_type: Quantize
"];
	442 [label="credit: 442
data type: FP32
name: blk.18.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	443 [label="credit: 443
data type: Q8_0
name: blk.18.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	444 [label="credit: 444
name: blk.18.ffn_down.weight.fp32
operator_type: Quantize
"];
	445 [label="credit: 445
data type: FP32
name: blk.18.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	446 [label="credit: 446
data type: FP32
name: blk.19.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	447 [label="credit: 447
data type: Q8_0
name: blk.19.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	448 [label="credit: 448
name: blk.19.attn_q.weight.fp32
operator_type: Quantize
"];
	449 [label="credit: 449
data type: FP32
name: blk.19.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	450 [label="credit: 450
data type: Q8_0
name: blk.19.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	451 [label="credit: 451
name: blk.19.attn_k.weight.fp32
operator_type: Quantize
"];
	452 [label="credit: 452
data type: FP32
name: blk.19.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	453 [label="credit: 453
data type: Q8_0
name: blk.19.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	454 [label="credit: 454
name: blk.19.attn_v.weight.fp32
operator_type: Quantize
"];
	455 [label="credit: 455
data type: FP32
name: blk.19.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	456 [label="credit: 456
data type: Q8_0
name: blk.19.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	457 [label="credit: 457
name: blk.19.attn_output.weight.fp32
operator_type: Quantize
"];
	458 [label="credit: 458
data type: FP32
name: blk.19.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	459 [label="credit: 459
data type: FP32
name: blk.19.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	460 [label="credit: 460
data type: Q8_0
name: blk.19.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	461 [label="credit: 461
name: blk.19.ffn_up.weight.fp32
operator_type: Quantize
"];
	462 [label="credit: 462
data type: FP32
name: blk.19.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	463 [label="credit: 463
data type: Q8_0
name: blk.19.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	464 [label="credit: 464
name: blk.19.ffn_gate.weight.fp32
operator_type: Quantize
"];
	465 [label="credit: 465
data type: FP32
name: blk.19.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	466 [label="credit: 466
data type: Q8_0
name: blk.19.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	467 [label="credit: 467
name: blk.19.ffn_down.weight.fp32
operator_type: Quantize
"];
	468 [label="credit: 468
data type: FP32
name: blk.19.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	469 [label="credit: 469
data type: FP32
name: blk.20.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	470 [label="credit: 470
data type: Q8_0
name: blk.20.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	471 [label="credit: 471
name: blk.20.attn_q.weight.fp32
operator_type: Quantize
"];
	472 [label="credit: 472
data type: FP32
name: blk.20.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	473 [label="credit: 473
data type: Q8_0
name: blk.20.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	474 [label="credit: 474
name: blk.20.attn_k.weight.fp32
operator_type: Quantize
"];
	475 [label="credit: 475
data type: FP32
name: blk.20.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	476 [label="credit: 476
data type: Q8_0
name: blk.20.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	477 [label="credit: 477
name: blk.20.attn_v.weight.fp32
operator_type: Quantize
"];
	478 [label="credit: 478
data type: FP32
name: blk.20.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	479 [label="credit: 479
data type: Q8_0
name: blk.20.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	480 [label="credit: 480
name: blk.20.attn_output.weight.fp32
operator_type: Quantize
"];
	481 [label="credit: 481
data type: FP32
name: blk.20.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	482 [label="credit: 482
data type: FP32
name: blk.20.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	483 [label="credit: 483
data type: Q8_0
name: blk.20.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	484 [label="credit: 484
name: blk.20.ffn_up.weight.fp32
operator_type: Quantize
"];
	485 [label="credit: 485
data type: FP32
name: blk.20.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	486 [label="credit: 486
data type: Q8_0
name: blk.20.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	487 [label="credit: 487
name: blk.20.ffn_gate.weight.fp32
operator_type: Quantize
"];
	488 [label="credit: 488
data type: FP32
name: blk.20.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	489 [label="credit: 489
data type: Q8_0
name: blk.20.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	490 [label="credit: 490
name: blk.20.ffn_down.weight.fp32
operator_type: Quantize
"];
	491 [label="credit: 491
data type: FP32
name: blk.20.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	492 [label="credit: 492
data type: FP32
name: blk.21.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	493 [label="credit: 493
data type: Q8_0
name: blk.21.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	494 [label="credit: 494
name: blk.21.attn_q.weight.fp32
operator_type: Quantize
"];
	495 [label="credit: 495
data type: FP32
name: blk.21.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	496 [label="credit: 496
data type: Q8_0
name: blk.21.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	497 [label="credit: 497
name: blk.21.attn_k.weight.fp32
operator_type: Quantize
"];
	498 [label="credit: 498
data type: FP32
name: blk.21.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	499 [label="credit: 499
data type: Q8_0
name: blk.21.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	500 [label="credit: 500
name: blk.21.attn_v.weight.fp32
operator_type: Quantize
"];
	501 [label="credit: 501
data type: FP32
name: blk.21.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	502 [label="credit: 502
data type: Q8_0
name: blk.21.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	503 [label="credit: 503
name: blk.21.attn_output.weight.fp32
operator_type: Quantize
"];
	504 [label="credit: 504
data type: FP32
name: blk.21.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	505 [label="credit: 505
data type: FP32
name: blk.21.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	506 [label="credit: 506
data type: Q8_0
name: blk.21.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	507 [label="credit: 507
name: blk.21.ffn_up.weight.fp32
operator_type: Quantize
"];
	508 [label="credit: 508
data type: FP32
name: blk.21.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	509 [label="credit: 509
data type: Q8_0
name: blk.21.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	510 [label="credit: 510
name: blk.21.ffn_gate.weight.fp32
operator_type: Quantize
"];
	511 [label="credit: 511
data type: FP32
name: blk.21.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	512 [label="credit: 512
data type: Q8_0
name: blk.21.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	513 [label="credit: 513
name: blk.21.ffn_down.weight.fp32
operator_type: Quantize
"];
	514 [label="credit: 514
data type: FP32
name: blk.21.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	515 [label="credit: 515
data type: FP32
name: blk.22.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	516 [label="credit: 516
data type: Q8_0
name: blk.22.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	517 [label="credit: 517
name: blk.22.attn_q.weight.fp32
operator_type: Quantize
"];
	518 [label="credit: 518
data type: FP32
name: blk.22.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	519 [label="credit: 519
data type: Q8_0
name: blk.22.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	520 [label="credit: 520
name: blk.22.attn_k.weight.fp32
operator_type: Quantize
"];
	521 [label="credit: 521
data type: FP32
name: blk.22.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	522 [label="credit: 522
data type: Q8_0
name: blk.22.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	523 [label="credit: 523
name: blk.22.attn_v.weight.fp32
operator_type: Quantize
"];
	524 [label="credit: 524
data type: FP32
name: blk.22.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	525 [label="credit: 525
data type: Q8_0
name: blk.22.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	526 [label="credit: 526
name: blk.22.attn_output.weight.fp32
operator_type: Quantize
"];
	527 [label="credit: 527
data type: FP32
name: blk.22.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	528 [label="credit: 528
data type: FP32
name: blk.22.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	529 [label="credit: 529
data type: Q8_0
name: blk.22.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	530 [label="credit: 530
name: blk.22.ffn_up.weight.fp32
operator_type: Quantize
"];
	531 [label="credit: 531
data type: FP32
name: blk.22.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	532 [label="credit: 532
data type: Q8_0
name: blk.22.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	533 [label="credit: 533
name: blk.22.ffn_gate.weight.fp32
operator_type: Quantize
"];
	534 [label="credit: 534
data type: FP32
name: blk.22.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	535 [label="credit: 535
data type: Q8_0
name: blk.22.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	536 [label="credit: 536
name: blk.22.ffn_down.weight.fp32
operator_type: Quantize
"];
	537 [label="credit: 537
data type: FP32
name: blk.22.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	538 [label="credit: 538
data type: FP32
name: blk.23.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	539 [label="credit: 539
data type: Q8_0
name: blk.23.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	540 [label="credit: 540
name: blk.23.attn_q.weight.fp32
operator_type: Quantize
"];
	541 [label="credit: 541
data type: FP32
name: blk.23.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	542 [label="credit: 542
data type: Q8_0
name: blk.23.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	543 [label="credit: 543
name: blk.23.attn_k.weight.fp32
operator_type: Quantize
"];
	544 [label="credit: 544
data type: FP32
name: blk.23.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	545 [label="credit: 545
data type: Q8_0
name: blk.23.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	546 [label="credit: 546
name: blk.23.attn_v.weight.fp32
operator_type: Quantize
"];
	547 [label="credit: 547
data type: FP32
name: blk.23.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	548 [label="credit: 548
data type: Q8_0
name: blk.23.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	549 [label="credit: 549
name: blk.23.attn_output.weight.fp32
operator_type: Quantize
"];
	550 [label="credit: 550
data type: FP32
name: blk.23.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	551 [label="credit: 551
data type: FP32
name: blk.23.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	552 [label="credit: 552
data type: Q8_0
name: blk.23.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	553 [label="credit: 553
name: blk.23.ffn_up.weight.fp32
operator_type: Quantize
"];
	554 [label="credit: 554
data type: FP32
name: blk.23.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	555 [label="credit: 555
data type: Q8_0
name: blk.23.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	556 [label="credit: 556
name: blk.23.ffn_gate.weight.fp32
operator_type: Quantize
"];
	557 [label="credit: 557
data type: FP32
name: blk.23.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	558 [label="credit: 558
data type: Q8_0
name: blk.23.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	559 [label="credit: 559
name: blk.23.ffn_down.weight.fp32
operator_type: Quantize
"];
	560 [label="credit: 560
data type: FP32
name: blk.23.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	561 [label="credit: 561
data type: FP32
name: blk.24.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	562 [label="credit: 562
data type: Q8_0
name: blk.24.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	563 [label="credit: 563
name: blk.24.attn_q.weight.fp32
operator_type: Quantize
"];
	564 [label="credit: 564
data type: FP32
name: blk.24.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	565 [label="credit: 565
data type: Q8_0
name: blk.24.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	566 [label="credit: 566
name: blk.24.attn_k.weight.fp32
operator_type: Quantize
"];
	567 [label="credit: 567
data type: FP32
name: blk.24.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	568 [label="credit: 568
data type: Q8_0
name: blk.24.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	569 [label="credit: 569
name: blk.24.attn_v.weight.fp32
operator_type: Quantize
"];
	570 [label="credit: 570
data type: FP32
name: blk.24.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	571 [label="credit: 571
data type: Q8_0
name: blk.24.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	572 [label="credit: 572
name: blk.24.attn_output.weight.fp32
operator_type: Quantize
"];
	573 [label="credit: 573
data type: FP32
name: blk.24.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	574 [label="credit: 574
data type: FP32
name: blk.24.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	575 [label="credit: 575
data type: Q8_0
name: blk.24.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	576 [label="credit: 576
name: blk.24.ffn_up.weight.fp32
operator_type: Quantize
"];
	577 [label="credit: 577
data type: FP32
name: blk.24.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	578 [label="credit: 578
data type: Q8_0
name: blk.24.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	579 [label="credit: 579
name: blk.24.ffn_gate.weight.fp32
operator_type: Quantize
"];
	580 [label="credit: 580
data type: FP32
name: blk.24.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	581 [label="credit: 581
data type: Q8_0
name: blk.24.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	582 [label="credit: 582
name: blk.24.ffn_down.weight.fp32
operator_type: Quantize
"];
	583 [label="credit: 583
data type: FP32
name: blk.24.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	584 [label="credit: 584
data type: FP32
name: blk.25.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	585 [label="credit: 585
data type: Q8_0
name: blk.25.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	586 [label="credit: 586
name: blk.25.attn_q.weight.fp32
operator_type: Quantize
"];
	587 [label="credit: 587
data type: FP32
name: blk.25.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	588 [label="credit: 588
data type: Q8_0
name: blk.25.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	589 [label="credit: 589
name: blk.25.attn_k.weight.fp32
operator_type: Quantize
"];
	590 [label="credit: 590
data type: FP32
name: blk.25.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	591 [label="credit: 591
data type: Q8_0
name: blk.25.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	592 [label="credit: 592
name: blk.25.attn_v.weight.fp32
operator_type: Quantize
"];
	593 [label="credit: 593
data type: FP32
name: blk.25.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	594 [label="credit: 594
data type: Q8_0
name: blk.25.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	595 [label="credit: 595
name: blk.25.attn_output.weight.fp32
operator_type: Quantize
"];
	596 [label="credit: 596
data type: FP32
name: blk.25.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	597 [label="credit: 597
data type: FP32
name: blk.25.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	598 [label="credit: 598
data type: Q8_0
name: blk.25.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	599 [label="credit: 599
name: blk.25.ffn_up.weight.fp32
operator_type: Quantize
"];
	600 [label="credit: 600
data type: FP32
name: blk.25.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	601 [label="credit: 601
data type: Q8_0
name: blk.25.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	602 [label="credit: 602
name: blk.25.ffn_gate.weight.fp32
operator_type: Quantize
"];
	603 [label="credit: 603
data type: FP32
name: blk.25.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	604 [label="credit: 604
data type: Q8_0
name: blk.25.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	605 [label="credit: 605
name: blk.25.ffn_down.weight.fp32
operator_type: Quantize
"];
	606 [label="credit: 606
data type: FP32
name: blk.25.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	607 [label="credit: 607
data type: FP32
name: blk.26.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	608 [label="credit: 608
data type: Q8_0
name: blk.26.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	609 [label="credit: 609
name: blk.26.attn_q.weight.fp32
operator_type: Quantize
"];
	610 [label="credit: 610
data type: FP32
name: blk.26.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	611 [label="credit: 611
data type: Q8_0
name: blk.26.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	612 [label="credit: 612
name: blk.26.attn_k.weight.fp32
operator_type: Quantize
"];
	613 [label="credit: 613
data type: FP32
name: blk.26.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	614 [label="credit: 614
data type: Q8_0
name: blk.26.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	615 [label="credit: 615
name: blk.26.attn_v.weight.fp32
operator_type: Quantize
"];
	616 [label="credit: 616
data type: FP32
name: blk.26.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	617 [label="credit: 617
data type: Q8_0
name: blk.26.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	618 [label="credit: 618
name: blk.26.attn_output.weight.fp32
operator_type: Quantize
"];
	619 [label="credit: 619
data type: FP32
name: blk.26.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	620 [label="credit: 620
data type: FP32
name: blk.26.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	621 [label="credit: 621
data type: Q8_0
name: blk.26.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	622 [label="credit: 622
name: blk.26.ffn_up.weight.fp32
operator_type: Quantize
"];
	623 [label="credit: 623
data type: FP32
name: blk.26.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	624 [label="credit: 624
data type: Q8_0
name: blk.26.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	625 [label="credit: 625
name: blk.26.ffn_gate.weight.fp32
operator_type: Quantize
"];
	626 [label="credit: 626
data type: FP32
name: blk.26.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	627 [label="credit: 627
data type: Q8_0
name: blk.26.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	628 [label="credit: 628
name: blk.26.ffn_down.weight.fp32
operator_type: Quantize
"];
	629 [label="credit: 629
data type: FP32
name: blk.26.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	630 [label="credit: 630
data type: FP32
name: blk.27.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	631 [label="credit: 631
data type: Q8_0
name: blk.27.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	632 [label="credit: 632
name: blk.27.attn_q.weight.fp32
operator_type: Quantize
"];
	633 [label="credit: 633
data type: FP32
name: blk.27.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	634 [label="credit: 634
data type: Q8_0
name: blk.27.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	635 [label="credit: 635
name: blk.27.attn_k.weight.fp32
operator_type: Quantize
"];
	636 [label="credit: 636
data type: FP32
name: blk.27.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	637 [label="credit: 637
data type: Q8_0
name: blk.27.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	638 [label="credit: 638
name: blk.27.attn_v.weight.fp32
operator_type: Quantize
"];
	639 [label="credit: 639
data type: FP32
name: blk.27.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	640 [label="credit: 640
data type: Q8_0
name: blk.27.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	641 [label="credit: 641
name: blk.27.attn_output.weight.fp32
operator_type: Quantize
"];
	642 [label="credit: 642
data type: FP32
name: blk.27.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	643 [label="credit: 643
data type: FP32
name: blk.27.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	644 [label="credit: 644
data type: Q8_0
name: blk.27.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	645 [label="credit: 645
name: blk.27.ffn_up.weight.fp32
operator_type: Quantize
"];
	646 [label="credit: 646
data type: FP32
name: blk.27.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	647 [label="credit: 647
data type: Q8_0
name: blk.27.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	648 [label="credit: 648
name: blk.27.ffn_gate.weight.fp32
operator_type: Quantize
"];
	649 [label="credit: 649
data type: FP32
name: blk.27.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	650 [label="credit: 650
data type: Q8_0
name: blk.27.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	651 [label="credit: 651
name: blk.27.ffn_down.weight.fp32
operator_type: Quantize
"];
	652 [label="credit: 652
data type: FP32
name: blk.27.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	653 [label="credit: 653
data type: FP32
name: blk.28.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	654 [label="credit: 654
data type: Q8_0
name: blk.28.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	655 [label="credit: 655
name: blk.28.attn_q.weight.fp32
operator_type: Quantize
"];
	656 [label="credit: 656
data type: FP32
name: blk.28.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	657 [label="credit: 657
data type: Q8_0
name: blk.28.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	658 [label="credit: 658
name: blk.28.attn_k.weight.fp32
operator_type: Quantize
"];
	659 [label="credit: 659
data type: FP32
name: blk.28.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	660 [label="credit: 660
data type: Q8_0
name: blk.28.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	661 [label="credit: 661
name: blk.28.attn_v.weight.fp32
operator_type: Quantize
"];
	662 [label="credit: 662
data type: FP32
name: blk.28.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	663 [label="credit: 663
data type: Q8_0
name: blk.28.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	664 [label="credit: 664
name: blk.28.attn_output.weight.fp32
operator_type: Quantize
"];
	665 [label="credit: 665
data type: FP32
name: blk.28.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	666 [label="credit: 666
data type: FP32
name: blk.28.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	667 [label="credit: 667
data type: Q8_0
name: blk.28.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	668 [label="credit: 668
name: blk.28.ffn_up.weight.fp32
operator_type: Quantize
"];
	669 [label="credit: 669
data type: FP32
name: blk.28.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	670 [label="credit: 670
data type: Q8_0
name: blk.28.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	671 [label="credit: 671
name: blk.28.ffn_gate.weight.fp32
operator_type: Quantize
"];
	672 [label="credit: 672
data type: FP32
name: blk.28.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	673 [label="credit: 673
data type: Q8_0
name: blk.28.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	674 [label="credit: 674
name: blk.28.ffn_down.weight.fp32
operator_type: Quantize
"];
	675 [label="credit: 675
data type: FP32
name: blk.28.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	676 [label="credit: 676
data type: FP32
name: blk.29.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	677 [label="credit: 677
data type: Q8_0
name: blk.29.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	678 [label="credit: 678
name: blk.29.attn_q.weight.fp32
operator_type: Quantize
"];
	679 [label="credit: 679
data type: FP32
name: blk.29.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	680 [label="credit: 680
data type: Q8_0
name: blk.29.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	681 [label="credit: 681
name: blk.29.attn_k.weight.fp32
operator_type: Quantize
"];
	682 [label="credit: 682
data type: FP32
name: blk.29.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	683 [label="credit: 683
data type: Q8_0
name: blk.29.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	684 [label="credit: 684
name: blk.29.attn_v.weight.fp32
operator_type: Quantize
"];
	685 [label="credit: 685
data type: FP32
name: blk.29.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	686 [label="credit: 686
data type: Q8_0
name: blk.29.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	687 [label="credit: 687
name: blk.29.attn_output.weight.fp32
operator_type: Quantize
"];
	688 [label="credit: 688
data type: FP32
name: blk.29.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	689 [label="credit: 689
data type: FP32
name: blk.29.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	690 [label="credit: 690
data type: Q8_0
name: blk.29.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	691 [label="credit: 691
name: blk.29.ffn_up.weight.fp32
operator_type: Quantize
"];
	692 [label="credit: 692
data type: FP32
name: blk.29.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	693 [label="credit: 693
data type: Q8_0
name: blk.29.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	694 [label="credit: 694
name: blk.29.ffn_gate.weight.fp32
operator_type: Quantize
"];
	695 [label="credit: 695
data type: FP32
name: blk.29.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	696 [label="credit: 696
data type: Q8_0
name: blk.29.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	697 [label="credit: 697
name: blk.29.ffn_down.weight.fp32
operator_type: Quantize
"];
	698 [label="credit: 698
data type: FP32
name: blk.29.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	699 [label="credit: 699
data type: FP32
name: blk.30.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	700 [label="credit: 700
data type: Q8_0
name: blk.30.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	701 [label="credit: 701
name: blk.30.attn_q.weight.fp32
operator_type: Quantize
"];
	702 [label="credit: 702
data type: FP32
name: blk.30.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	703 [label="credit: 703
data type: Q8_0
name: blk.30.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	704 [label="credit: 704
name: blk.30.attn_k.weight.fp32
operator_type: Quantize
"];
	705 [label="credit: 705
data type: FP32
name: blk.30.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	706 [label="credit: 706
data type: Q8_0
name: blk.30.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	707 [label="credit: 707
name: blk.30.attn_v.weight.fp32
operator_type: Quantize
"];
	708 [label="credit: 708
data type: FP32
name: blk.30.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	709 [label="credit: 709
data type: Q8_0
name: blk.30.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	710 [label="credit: 710
name: blk.30.attn_output.weight.fp32
operator_type: Quantize
"];
	711 [label="credit: 711
data type: FP32
name: blk.30.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	712 [label="credit: 712
data type: FP32
name: blk.30.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	713 [label="credit: 713
data type: Q8_0
name: blk.30.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	714 [label="credit: 714
name: blk.30.ffn_up.weight.fp32
operator_type: Quantize
"];
	715 [label="credit: 715
data type: FP32
name: blk.30.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	716 [label="credit: 716
data type: Q8_0
name: blk.30.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	717 [label="credit: 717
name: blk.30.ffn_gate.weight.fp32
operator_type: Quantize
"];
	718 [label="credit: 718
data type: FP32
name: blk.30.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	719 [label="credit: 719
data type: Q8_0
name: blk.30.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	720 [label="credit: 720
name: blk.30.ffn_down.weight.fp32
operator_type: Quantize
"];
	721 [label="credit: 721
data type: FP32
name: blk.30.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	722 [label="credit: 722
data type: FP32
name: blk.31.attn_norm.weight
shape: (4096) (4)
size: 16384
"];
	723 [label="credit: 723
data type: Q8_0
name: blk.31.attn_q.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	724 [label="credit: 724
name: blk.31.attn_q.weight.fp32
operator_type: Quantize
"];
	725 [label="credit: 725
data type: FP32
name: blk.31.attn_q.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	726 [label="credit: 726
data type: Q8_0
name: blk.31.attn_k.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	727 [label="credit: 727
name: blk.31.attn_k.weight.fp32
operator_type: Quantize
"];
	728 [label="credit: 728
data type: FP32
name: blk.31.attn_k.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	729 [label="credit: 729
data type: Q8_0
name: blk.31.attn_v.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	730 [label="credit: 730
name: blk.31.attn_v.weight.fp32
operator_type: Quantize
"];
	731 [label="credit: 731
data type: FP32
name: blk.31.attn_v.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	732 [label="credit: 732
data type: Q8_0
name: blk.31.attn_output.weight
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	733 [label="credit: 733
name: blk.31.attn_output.weight.fp32
operator_type: Quantize
"];
	734 [label="credit: 734
data type: FP32
name: blk.31.attn_output.weight.fp32- out
shape: (4096, 4096) (34, 139264)
size: 570425344
"];
	735 [label="credit: 735
data type: FP32
name: blk.31.ffn_norm.weight
shape: (4096) (4)
size: 16384
"];
	736 [label="credit: 736
data type: Q8_0
name: blk.31.ffn_up.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	737 [label="credit: 737
name: blk.31.ffn_up.weight.fp32
operator_type: Quantize
"];
	738 [label="credit: 738
data type: FP32
name: blk.31.ffn_up.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	739 [label="credit: 739
data type: Q8_0
name: blk.31.ffn_gate.weight
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	740 [label="credit: 740
name: blk.31.ffn_gate.weight.fp32
operator_type: Quantize
"];
	741 [label="credit: 741
data type: FP32
name: blk.31.ffn_gate.weight.fp32- out
shape: (4096, 11008) (34, 139264)
size: 1533018112
"];
	742 [label="credit: 742
data type: Q8_0
name: blk.31.ffn_down.weight
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	743 [label="credit: 743
name: blk.31.ffn_down.weight.fp32
operator_type: Quantize
"];
	744 [label="credit: 744
data type: FP32
name: blk.31.ffn_down.weight.fp32- out
shape: (11008, 4096) (34, 374272)
size: 1533018112
"];
	745 [label="credit: 745
data type: INT32
name: Input embedding
shape: (10) (4)
size: 40
"];
	746 [label="credit: 746
name: Input embedding
operator_type: GetRow
"];
	747 [label="credit: 747
data type: FP32
name: Input embedding- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	748 [label="credit: 748
data type: INT32
name: Input position
shape: (10) (4)
size: 40
"];
	749 [label="credit: 749
name: attn mask
operator_type: NormRMS
"];
	750 [label="credit: 750
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	751 [label="credit: 751
name: attn norm
operator_type: Mul
"];
	752 [label="credit: 752
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	753 [label="credit: 753
name: Qcur
operator_type: MatMul
"];
	754 [label="credit: 754
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	755 [label="credit: 755
name: Kcur
operator_type: MatMul
"];
	756 [label="credit: 756
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	757 [label="credit: 757
name: Vcur
operator_type: MatMul
"];
	758 [label="credit: 758
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	759 [label="credit: 759
name: Qcur - reshaped
operator_type: Reshape
"];
	760 [label="credit: 760
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	761 [label="credit: 761
name: Kcur - reshaped
operator_type: Reshape
"];
	762 [label="credit: 762
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	763 [label="credit: 763
name: Qcur - rope
operator_type: Rope
"];
	764 [label="credit: 764
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	765 [label="credit: 765
name: Kcur - rope
operator_type: Rope
"];
	766 [label="credit: 766
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	767 [label="credit: 767
name: Q - out
operator_type: Permute
"];
	768 [label="credit: 768
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	769 [label="credit: 769
name: K - out
operator_type: View
"];
	770 [label="credit: 770
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	771 [label="credit: 771
name: Attention Score
operator_type: MatMul
"];
	772 [label="credit: 772
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	773 [label="credit: 773
name: Attention Context
operator_type: Softmax
"];
	774 [label="credit: 774
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	775 [label="credit: 775
name: V - transpose
operator_type: Transpose
"];
	776 [label="credit: 776
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	777 [label="credit: 777
name: V - out
operator_type: View
"];
	778 [label="credit: 778
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	779 [label="credit: 779
name: K - Q - V
operator_type: MatMul
"];
	780 [label="credit: 780
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	781 [label="credit: 781
name: K - Q - V merged
operator_type: Permute
"];
	782 [label="credit: 782
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	783 [label="credit: 783
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	784 [label="credit: 784
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	785 [label="credit: 785
name: K - Q - V weight
operator_type: MatMul
"];
	786 [label="credit: 786
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	787 [label="credit: 787
name: FFN input
operator_type: Add
"];
	788 [label="credit: 788
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	789 [label="credit: 789
name: FFN - norm
operator_type: NormRMS
"];
	790 [label="credit: 790
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	791 [label="credit: 791
name: FFN - norm
operator_type: Mul
"];
	792 [label="credit: 792
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	793 [label="credit: 793
name: FFN up
operator_type: MatMul
"];
	794 [label="credit: 794
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	795 [label="credit: 795
name: FFN gate
operator_type: MatMul
"];
	796 [label="credit: 796
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	797 [label="credit: 797
name: FFN gate silu
operator_type: Silu
"];
	798 [label="credit: 798
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	799 [label="credit: 799
name: FFN par
operator_type: Mul
"];
	800 [label="credit: 800
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	801 [label="credit: 801
name: FFN down
operator_type: MatMul
"];
	802 [label="credit: 802
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	803 [label="credit: 803
name: Logit out
operator_type: Add
"];
	804 [label="credit: 804
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	805 [label="credit: 805
name: attn mask
operator_type: NormRMS
"];
	806 [label="credit: 806
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	807 [label="credit: 807
name: attn norm
operator_type: Mul
"];
	808 [label="credit: 808
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	809 [label="credit: 809
name: Qcur
operator_type: MatMul
"];
	810 [label="credit: 810
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	811 [label="credit: 811
name: Kcur
operator_type: MatMul
"];
	812 [label="credit: 812
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	813 [label="credit: 813
name: Vcur
operator_type: MatMul
"];
	814 [label="credit: 814
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	815 [label="credit: 815
name: Qcur - reshaped
operator_type: Reshape
"];
	816 [label="credit: 816
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	817 [label="credit: 817
name: Kcur - reshaped
operator_type: Reshape
"];
	818 [label="credit: 818
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	819 [label="credit: 819
name: Qcur - rope
operator_type: Rope
"];
	820 [label="credit: 820
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	821 [label="credit: 821
name: Kcur - rope
operator_type: Rope
"];
	822 [label="credit: 822
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	823 [label="credit: 823
name: Q - out
operator_type: Permute
"];
	824 [label="credit: 824
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	825 [label="credit: 825
name: K - out
operator_type: View
"];
	826 [label="credit: 826
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	827 [label="credit: 827
name: Attention Score
operator_type: MatMul
"];
	828 [label="credit: 828
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	829 [label="credit: 829
name: Attention Context
operator_type: Softmax
"];
	830 [label="credit: 830
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	831 [label="credit: 831
name: V - transpose
operator_type: Transpose
"];
	832 [label="credit: 832
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	833 [label="credit: 833
name: V - out
operator_type: View
"];
	834 [label="credit: 834
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	835 [label="credit: 835
name: K - Q - V
operator_type: MatMul
"];
	836 [label="credit: 836
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	837 [label="credit: 837
name: K - Q - V merged
operator_type: Permute
"];
	838 [label="credit: 838
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	839 [label="credit: 839
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	840 [label="credit: 840
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	841 [label="credit: 841
name: K - Q - V weight
operator_type: MatMul
"];
	842 [label="credit: 842
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	843 [label="credit: 843
name: FFN input
operator_type: Add
"];
	844 [label="credit: 844
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	845 [label="credit: 845
name: FFN - norm
operator_type: NormRMS
"];
	846 [label="credit: 846
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	847 [label="credit: 847
name: FFN - norm
operator_type: Mul
"];
	848 [label="credit: 848
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	849 [label="credit: 849
name: FFN up
operator_type: MatMul
"];
	850 [label="credit: 850
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	851 [label="credit: 851
name: FFN gate
operator_type: MatMul
"];
	852 [label="credit: 852
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	853 [label="credit: 853
name: FFN gate silu
operator_type: Silu
"];
	854 [label="credit: 854
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	855 [label="credit: 855
name: FFN par
operator_type: Mul
"];
	856 [label="credit: 856
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	857 [label="credit: 857
name: FFN down
operator_type: MatMul
"];
	858 [label="credit: 858
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	859 [label="credit: 859
name: Logit out
operator_type: Add
"];
	860 [label="credit: 860
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	861 [label="credit: 861
name: attn mask
operator_type: NormRMS
"];
	862 [label="credit: 862
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	863 [label="credit: 863
name: attn norm
operator_type: Mul
"];
	864 [label="credit: 864
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	865 [label="credit: 865
name: Qcur
operator_type: MatMul
"];
	866 [label="credit: 866
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	867 [label="credit: 867
name: Kcur
operator_type: MatMul
"];
	868 [label="credit: 868
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	869 [label="credit: 869
name: Vcur
operator_type: MatMul
"];
	870 [label="credit: 870
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	871 [label="credit: 871
name: Qcur - reshaped
operator_type: Reshape
"];
	872 [label="credit: 872
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	873 [label="credit: 873
name: Kcur - reshaped
operator_type: Reshape
"];
	874 [label="credit: 874
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	875 [label="credit: 875
name: Qcur - rope
operator_type: Rope
"];
	876 [label="credit: 876
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	877 [label="credit: 877
name: Kcur - rope
operator_type: Rope
"];
	878 [label="credit: 878
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	879 [label="credit: 879
name: Q - out
operator_type: Permute
"];
	880 [label="credit: 880
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	881 [label="credit: 881
name: K - out
operator_type: View
"];
	882 [label="credit: 882
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	883 [label="credit: 883
name: Attention Score
operator_type: MatMul
"];
	884 [label="credit: 884
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	885 [label="credit: 885
name: Attention Context
operator_type: Softmax
"];
	886 [label="credit: 886
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	887 [label="credit: 887
name: V - transpose
operator_type: Transpose
"];
	888 [label="credit: 888
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	889 [label="credit: 889
name: V - out
operator_type: View
"];
	890 [label="credit: 890
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	891 [label="credit: 891
name: K - Q - V
operator_type: MatMul
"];
	892 [label="credit: 892
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	893 [label="credit: 893
name: K - Q - V merged
operator_type: Permute
"];
	894 [label="credit: 894
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	895 [label="credit: 895
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	896 [label="credit: 896
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	897 [label="credit: 897
name: K - Q - V weight
operator_type: MatMul
"];
	898 [label="credit: 898
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	899 [label="credit: 899
name: FFN input
operator_type: Add
"];
	900 [label="credit: 900
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	901 [label="credit: 901
name: FFN - norm
operator_type: NormRMS
"];
	902 [label="credit: 902
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	903 [label="credit: 903
name: FFN - norm
operator_type: Mul
"];
	904 [label="credit: 904
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	905 [label="credit: 905
name: FFN up
operator_type: MatMul
"];
	906 [label="credit: 906
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	907 [label="credit: 907
name: FFN gate
operator_type: MatMul
"];
	908 [label="credit: 908
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	909 [label="credit: 909
name: FFN gate silu
operator_type: Silu
"];
	910 [label="credit: 910
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	911 [label="credit: 911
name: FFN par
operator_type: Mul
"];
	912 [label="credit: 912
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	913 [label="credit: 913
name: FFN down
operator_type: MatMul
"];
	914 [label="credit: 914
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	915 [label="credit: 915
name: Logit out
operator_type: Add
"];
	916 [label="credit: 916
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	917 [label="credit: 917
name: attn mask
operator_type: NormRMS
"];
	918 [label="credit: 918
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	919 [label="credit: 919
name: attn norm
operator_type: Mul
"];
	920 [label="credit: 920
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	921 [label="credit: 921
name: Qcur
operator_type: MatMul
"];
	922 [label="credit: 922
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	923 [label="credit: 923
name: Kcur
operator_type: MatMul
"];
	924 [label="credit: 924
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	925 [label="credit: 925
name: Vcur
operator_type: MatMul
"];
	926 [label="credit: 926
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	927 [label="credit: 927
name: Qcur - reshaped
operator_type: Reshape
"];
	928 [label="credit: 928
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	929 [label="credit: 929
name: Kcur - reshaped
operator_type: Reshape
"];
	930 [label="credit: 930
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	931 [label="credit: 931
name: Qcur - rope
operator_type: Rope
"];
	932 [label="credit: 932
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	933 [label="credit: 933
name: Kcur - rope
operator_type: Rope
"];
	934 [label="credit: 934
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	935 [label="credit: 935
name: Q - out
operator_type: Permute
"];
	936 [label="credit: 936
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	937 [label="credit: 937
name: K - out
operator_type: View
"];
	938 [label="credit: 938
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	939 [label="credit: 939
name: Attention Score
operator_type: MatMul
"];
	940 [label="credit: 940
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	941 [label="credit: 941
name: Attention Context
operator_type: Softmax
"];
	942 [label="credit: 942
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	943 [label="credit: 943
name: V - transpose
operator_type: Transpose
"];
	944 [label="credit: 944
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	945 [label="credit: 945
name: V - out
operator_type: View
"];
	946 [label="credit: 946
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	947 [label="credit: 947
name: K - Q - V
operator_type: MatMul
"];
	948 [label="credit: 948
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	949 [label="credit: 949
name: K - Q - V merged
operator_type: Permute
"];
	950 [label="credit: 950
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	951 [label="credit: 951
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	952 [label="credit: 952
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	953 [label="credit: 953
name: K - Q - V weight
operator_type: MatMul
"];
	954 [label="credit: 954
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	955 [label="credit: 955
name: FFN input
operator_type: Add
"];
	956 [label="credit: 956
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	957 [label="credit: 957
name: FFN - norm
operator_type: NormRMS
"];
	958 [label="credit: 958
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	959 [label="credit: 959
name: FFN - norm
operator_type: Mul
"];
	960 [label="credit: 960
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	961 [label="credit: 961
name: FFN up
operator_type: MatMul
"];
	962 [label="credit: 962
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	963 [label="credit: 963
name: FFN gate
operator_type: MatMul
"];
	964 [label="credit: 964
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	965 [label="credit: 965
name: FFN gate silu
operator_type: Silu
"];
	966 [label="credit: 966
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	967 [label="credit: 967
name: FFN par
operator_type: Mul
"];
	968 [label="credit: 968
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	969 [label="credit: 969
name: FFN down
operator_type: MatMul
"];
	970 [label="credit: 970
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	971 [label="credit: 971
name: Logit out
operator_type: Add
"];
	972 [label="credit: 972
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	973 [label="credit: 973
name: attn mask
operator_type: NormRMS
"];
	974 [label="credit: 974
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	975 [label="credit: 975
name: attn norm
operator_type: Mul
"];
	976 [label="credit: 976
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	977 [label="credit: 977
name: Qcur
operator_type: MatMul
"];
	978 [label="credit: 978
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	979 [label="credit: 979
name: Kcur
operator_type: MatMul
"];
	980 [label="credit: 980
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	981 [label="credit: 981
name: Vcur
operator_type: MatMul
"];
	982 [label="credit: 982
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	983 [label="credit: 983
name: Qcur - reshaped
operator_type: Reshape
"];
	984 [label="credit: 984
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	985 [label="credit: 985
name: Kcur - reshaped
operator_type: Reshape
"];
	986 [label="credit: 986
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	987 [label="credit: 987
name: Qcur - rope
operator_type: Rope
"];
	988 [label="credit: 988
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	989 [label="credit: 989
name: Kcur - rope
operator_type: Rope
"];
	990 [label="credit: 990
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	991 [label="credit: 991
name: Q - out
operator_type: Permute
"];
	992 [label="credit: 992
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	993 [label="credit: 993
name: K - out
operator_type: View
"];
	994 [label="credit: 994
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	995 [label="credit: 995
name: Attention Score
operator_type: MatMul
"];
	996 [label="credit: 996
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	997 [label="credit: 997
name: Attention Context
operator_type: Softmax
"];
	998 [label="credit: 998
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	999 [label="credit: 999
name: V - transpose
operator_type: Transpose
"];
	1000 [label="credit: 1000
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1001 [label="credit: 1001
name: V - out
operator_type: View
"];
	1002 [label="credit: 1002
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1003 [label="credit: 1003
name: K - Q - V
operator_type: MatMul
"];
	1004 [label="credit: 1004
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1005 [label="credit: 1005
name: K - Q - V merged
operator_type: Permute
"];
	1006 [label="credit: 1006
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1007 [label="credit: 1007
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1008 [label="credit: 1008
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1009 [label="credit: 1009
name: K - Q - V weight
operator_type: MatMul
"];
	1010 [label="credit: 1010
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1011 [label="credit: 1011
name: FFN input
operator_type: Add
"];
	1012 [label="credit: 1012
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1013 [label="credit: 1013
name: FFN - norm
operator_type: NormRMS
"];
	1014 [label="credit: 1014
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1015 [label="credit: 1015
name: FFN - norm
operator_type: Mul
"];
	1016 [label="credit: 1016
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1017 [label="credit: 1017
name: FFN up
operator_type: MatMul
"];
	1018 [label="credit: 1018
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1019 [label="credit: 1019
name: FFN gate
operator_type: MatMul
"];
	1020 [label="credit: 1020
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1021 [label="credit: 1021
name: FFN gate silu
operator_type: Silu
"];
	1022 [label="credit: 1022
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1023 [label="credit: 1023
name: FFN par
operator_type: Mul
"];
	1024 [label="credit: 1024
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1025 [label="credit: 1025
name: FFN down
operator_type: MatMul
"];
	1026 [label="credit: 1026
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1027 [label="credit: 1027
name: Logit out
operator_type: Add
"];
	1028 [label="credit: 1028
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1029 [label="credit: 1029
name: attn mask
operator_type: NormRMS
"];
	1030 [label="credit: 1030
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1031 [label="credit: 1031
name: attn norm
operator_type: Mul
"];
	1032 [label="credit: 1032
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1033 [label="credit: 1033
name: Qcur
operator_type: MatMul
"];
	1034 [label="credit: 1034
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1035 [label="credit: 1035
name: Kcur
operator_type: MatMul
"];
	1036 [label="credit: 1036
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1037 [label="credit: 1037
name: Vcur
operator_type: MatMul
"];
	1038 [label="credit: 1038
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1039 [label="credit: 1039
name: Qcur - reshaped
operator_type: Reshape
"];
	1040 [label="credit: 1040
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1041 [label="credit: 1041
name: Kcur - reshaped
operator_type: Reshape
"];
	1042 [label="credit: 1042
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1043 [label="credit: 1043
name: Qcur - rope
operator_type: Rope
"];
	1044 [label="credit: 1044
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1045 [label="credit: 1045
name: Kcur - rope
operator_type: Rope
"];
	1046 [label="credit: 1046
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1047 [label="credit: 1047
name: Q - out
operator_type: Permute
"];
	1048 [label="credit: 1048
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1049 [label="credit: 1049
name: K - out
operator_type: View
"];
	1050 [label="credit: 1050
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1051 [label="credit: 1051
name: Attention Score
operator_type: MatMul
"];
	1052 [label="credit: 1052
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1053 [label="credit: 1053
name: Attention Context
operator_type: Softmax
"];
	1054 [label="credit: 1054
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1055 [label="credit: 1055
name: V - transpose
operator_type: Transpose
"];
	1056 [label="credit: 1056
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1057 [label="credit: 1057
name: V - out
operator_type: View
"];
	1058 [label="credit: 1058
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1059 [label="credit: 1059
name: K - Q - V
operator_type: MatMul
"];
	1060 [label="credit: 1060
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1061 [label="credit: 1061
name: K - Q - V merged
operator_type: Permute
"];
	1062 [label="credit: 1062
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1063 [label="credit: 1063
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1064 [label="credit: 1064
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1065 [label="credit: 1065
name: K - Q - V weight
operator_type: MatMul
"];
	1066 [label="credit: 1066
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1067 [label="credit: 1067
name: FFN input
operator_type: Add
"];
	1068 [label="credit: 1068
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1069 [label="credit: 1069
name: FFN - norm
operator_type: NormRMS
"];
	1070 [label="credit: 1070
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1071 [label="credit: 1071
name: FFN - norm
operator_type: Mul
"];
	1072 [label="credit: 1072
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1073 [label="credit: 1073
name: FFN up
operator_type: MatMul
"];
	1074 [label="credit: 1074
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1075 [label="credit: 1075
name: FFN gate
operator_type: MatMul
"];
	1076 [label="credit: 1076
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1077 [label="credit: 1077
name: FFN gate silu
operator_type: Silu
"];
	1078 [label="credit: 1078
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1079 [label="credit: 1079
name: FFN par
operator_type: Mul
"];
	1080 [label="credit: 1080
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1081 [label="credit: 1081
name: FFN down
operator_type: MatMul
"];
	1082 [label="credit: 1082
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1083 [label="credit: 1083
name: Logit out
operator_type: Add
"];
	1084 [label="credit: 1084
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1085 [label="credit: 1085
name: attn mask
operator_type: NormRMS
"];
	1086 [label="credit: 1086
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1087 [label="credit: 1087
name: attn norm
operator_type: Mul
"];
	1088 [label="credit: 1088
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1089 [label="credit: 1089
name: Qcur
operator_type: MatMul
"];
	1090 [label="credit: 1090
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1091 [label="credit: 1091
name: Kcur
operator_type: MatMul
"];
	1092 [label="credit: 1092
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1093 [label="credit: 1093
name: Vcur
operator_type: MatMul
"];
	1094 [label="credit: 1094
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1095 [label="credit: 1095
name: Qcur - reshaped
operator_type: Reshape
"];
	1096 [label="credit: 1096
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1097 [label="credit: 1097
name: Kcur - reshaped
operator_type: Reshape
"];
	1098 [label="credit: 1098
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1099 [label="credit: 1099
name: Qcur - rope
operator_type: Rope
"];
	1100 [label="credit: 1100
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1101 [label="credit: 1101
name: Kcur - rope
operator_type: Rope
"];
	1102 [label="credit: 1102
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1103 [label="credit: 1103
name: Q - out
operator_type: Permute
"];
	1104 [label="credit: 1104
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1105 [label="credit: 1105
name: K - out
operator_type: View
"];
	1106 [label="credit: 1106
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1107 [label="credit: 1107
name: Attention Score
operator_type: MatMul
"];
	1108 [label="credit: 1108
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1109 [label="credit: 1109
name: Attention Context
operator_type: Softmax
"];
	1110 [label="credit: 1110
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1111 [label="credit: 1111
name: V - transpose
operator_type: Transpose
"];
	1112 [label="credit: 1112
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1113 [label="credit: 1113
name: V - out
operator_type: View
"];
	1114 [label="credit: 1114
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1115 [label="credit: 1115
name: K - Q - V
operator_type: MatMul
"];
	1116 [label="credit: 1116
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1117 [label="credit: 1117
name: K - Q - V merged
operator_type: Permute
"];
	1118 [label="credit: 1118
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1119 [label="credit: 1119
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1120 [label="credit: 1120
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1121 [label="credit: 1121
name: K - Q - V weight
operator_type: MatMul
"];
	1122 [label="credit: 1122
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1123 [label="credit: 1123
name: FFN input
operator_type: Add
"];
	1124 [label="credit: 1124
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1125 [label="credit: 1125
name: FFN - norm
operator_type: NormRMS
"];
	1126 [label="credit: 1126
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1127 [label="credit: 1127
name: FFN - norm
operator_type: Mul
"];
	1128 [label="credit: 1128
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1129 [label="credit: 1129
name: FFN up
operator_type: MatMul
"];
	1130 [label="credit: 1130
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1131 [label="credit: 1131
name: FFN gate
operator_type: MatMul
"];
	1132 [label="credit: 1132
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1133 [label="credit: 1133
name: FFN gate silu
operator_type: Silu
"];
	1134 [label="credit: 1134
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1135 [label="credit: 1135
name: FFN par
operator_type: Mul
"];
	1136 [label="credit: 1136
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1137 [label="credit: 1137
name: FFN down
operator_type: MatMul
"];
	1138 [label="credit: 1138
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1139 [label="credit: 1139
name: Logit out
operator_type: Add
"];
	1140 [label="credit: 1140
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1141 [label="credit: 1141
name: attn mask
operator_type: NormRMS
"];
	1142 [label="credit: 1142
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1143 [label="credit: 1143
name: attn norm
operator_type: Mul
"];
	1144 [label="credit: 1144
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1145 [label="credit: 1145
name: Qcur
operator_type: MatMul
"];
	1146 [label="credit: 1146
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1147 [label="credit: 1147
name: Kcur
operator_type: MatMul
"];
	1148 [label="credit: 1148
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1149 [label="credit: 1149
name: Vcur
operator_type: MatMul
"];
	1150 [label="credit: 1150
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1151 [label="credit: 1151
name: Qcur - reshaped
operator_type: Reshape
"];
	1152 [label="credit: 1152
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1153 [label="credit: 1153
name: Kcur - reshaped
operator_type: Reshape
"];
	1154 [label="credit: 1154
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1155 [label="credit: 1155
name: Qcur - rope
operator_type: Rope
"];
	1156 [label="credit: 1156
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1157 [label="credit: 1157
name: Kcur - rope
operator_type: Rope
"];
	1158 [label="credit: 1158
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1159 [label="credit: 1159
name: Q - out
operator_type: Permute
"];
	1160 [label="credit: 1160
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1161 [label="credit: 1161
name: K - out
operator_type: View
"];
	1162 [label="credit: 1162
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1163 [label="credit: 1163
name: Attention Score
operator_type: MatMul
"];
	1164 [label="credit: 1164
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1165 [label="credit: 1165
name: Attention Context
operator_type: Softmax
"];
	1166 [label="credit: 1166
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1167 [label="credit: 1167
name: V - transpose
operator_type: Transpose
"];
	1168 [label="credit: 1168
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1169 [label="credit: 1169
name: V - out
operator_type: View
"];
	1170 [label="credit: 1170
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1171 [label="credit: 1171
name: K - Q - V
operator_type: MatMul
"];
	1172 [label="credit: 1172
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1173 [label="credit: 1173
name: K - Q - V merged
operator_type: Permute
"];
	1174 [label="credit: 1174
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1175 [label="credit: 1175
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1176 [label="credit: 1176
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1177 [label="credit: 1177
name: K - Q - V weight
operator_type: MatMul
"];
	1178 [label="credit: 1178
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1179 [label="credit: 1179
name: FFN input
operator_type: Add
"];
	1180 [label="credit: 1180
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1181 [label="credit: 1181
name: FFN - norm
operator_type: NormRMS
"];
	1182 [label="credit: 1182
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1183 [label="credit: 1183
name: FFN - norm
operator_type: Mul
"];
	1184 [label="credit: 1184
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1185 [label="credit: 1185
name: FFN up
operator_type: MatMul
"];
	1186 [label="credit: 1186
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1187 [label="credit: 1187
name: FFN gate
operator_type: MatMul
"];
	1188 [label="credit: 1188
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1189 [label="credit: 1189
name: FFN gate silu
operator_type: Silu
"];
	1190 [label="credit: 1190
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1191 [label="credit: 1191
name: FFN par
operator_type: Mul
"];
	1192 [label="credit: 1192
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1193 [label="credit: 1193
name: FFN down
operator_type: MatMul
"];
	1194 [label="credit: 1194
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1195 [label="credit: 1195
name: Logit out
operator_type: Add
"];
	1196 [label="credit: 1196
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1197 [label="credit: 1197
name: attn mask
operator_type: NormRMS
"];
	1198 [label="credit: 1198
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1199 [label="credit: 1199
name: attn norm
operator_type: Mul
"];
	1200 [label="credit: 1200
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1201 [label="credit: 1201
name: Qcur
operator_type: MatMul
"];
	1202 [label="credit: 1202
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1203 [label="credit: 1203
name: Kcur
operator_type: MatMul
"];
	1204 [label="credit: 1204
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1205 [label="credit: 1205
name: Vcur
operator_type: MatMul
"];
	1206 [label="credit: 1206
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1207 [label="credit: 1207
name: Qcur - reshaped
operator_type: Reshape
"];
	1208 [label="credit: 1208
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1209 [label="credit: 1209
name: Kcur - reshaped
operator_type: Reshape
"];
	1210 [label="credit: 1210
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1211 [label="credit: 1211
name: Qcur - rope
operator_type: Rope
"];
	1212 [label="credit: 1212
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1213 [label="credit: 1213
name: Kcur - rope
operator_type: Rope
"];
	1214 [label="credit: 1214
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1215 [label="credit: 1215
name: Q - out
operator_type: Permute
"];
	1216 [label="credit: 1216
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1217 [label="credit: 1217
name: K - out
operator_type: View
"];
	1218 [label="credit: 1218
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1219 [label="credit: 1219
name: Attention Score
operator_type: MatMul
"];
	1220 [label="credit: 1220
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1221 [label="credit: 1221
name: Attention Context
operator_type: Softmax
"];
	1222 [label="credit: 1222
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1223 [label="credit: 1223
name: V - transpose
operator_type: Transpose
"];
	1224 [label="credit: 1224
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1225 [label="credit: 1225
name: V - out
operator_type: View
"];
	1226 [label="credit: 1226
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1227 [label="credit: 1227
name: K - Q - V
operator_type: MatMul
"];
	1228 [label="credit: 1228
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1229 [label="credit: 1229
name: K - Q - V merged
operator_type: Permute
"];
	1230 [label="credit: 1230
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1231 [label="credit: 1231
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1232 [label="credit: 1232
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1233 [label="credit: 1233
name: K - Q - V weight
operator_type: MatMul
"];
	1234 [label="credit: 1234
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1235 [label="credit: 1235
name: FFN input
operator_type: Add
"];
	1236 [label="credit: 1236
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1237 [label="credit: 1237
name: FFN - norm
operator_type: NormRMS
"];
	1238 [label="credit: 1238
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1239 [label="credit: 1239
name: FFN - norm
operator_type: Mul
"];
	1240 [label="credit: 1240
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1241 [label="credit: 1241
name: FFN up
operator_type: MatMul
"];
	1242 [label="credit: 1242
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1243 [label="credit: 1243
name: FFN gate
operator_type: MatMul
"];
	1244 [label="credit: 1244
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1245 [label="credit: 1245
name: FFN gate silu
operator_type: Silu
"];
	1246 [label="credit: 1246
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1247 [label="credit: 1247
name: FFN par
operator_type: Mul
"];
	1248 [label="credit: 1248
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1249 [label="credit: 1249
name: FFN down
operator_type: MatMul
"];
	1250 [label="credit: 1250
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1251 [label="credit: 1251
name: Logit out
operator_type: Add
"];
	1252 [label="credit: 1252
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1253 [label="credit: 1253
name: attn mask
operator_type: NormRMS
"];
	1254 [label="credit: 1254
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1255 [label="credit: 1255
name: attn norm
operator_type: Mul
"];
	1256 [label="credit: 1256
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1257 [label="credit: 1257
name: Qcur
operator_type: MatMul
"];
	1258 [label="credit: 1258
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1259 [label="credit: 1259
name: Kcur
operator_type: MatMul
"];
	1260 [label="credit: 1260
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1261 [label="credit: 1261
name: Vcur
operator_type: MatMul
"];
	1262 [label="credit: 1262
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1263 [label="credit: 1263
name: Qcur - reshaped
operator_type: Reshape
"];
	1264 [label="credit: 1264
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1265 [label="credit: 1265
name: Kcur - reshaped
operator_type: Reshape
"];
	1266 [label="credit: 1266
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1267 [label="credit: 1267
name: Qcur - rope
operator_type: Rope
"];
	1268 [label="credit: 1268
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1269 [label="credit: 1269
name: Kcur - rope
operator_type: Rope
"];
	1270 [label="credit: 1270
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1271 [label="credit: 1271
name: Q - out
operator_type: Permute
"];
	1272 [label="credit: 1272
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1273 [label="credit: 1273
name: K - out
operator_type: View
"];
	1274 [label="credit: 1274
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1275 [label="credit: 1275
name: Attention Score
operator_type: MatMul
"];
	1276 [label="credit: 1276
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1277 [label="credit: 1277
name: Attention Context
operator_type: Softmax
"];
	1278 [label="credit: 1278
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1279 [label="credit: 1279
name: V - transpose
operator_type: Transpose
"];
	1280 [label="credit: 1280
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1281 [label="credit: 1281
name: V - out
operator_type: View
"];
	1282 [label="credit: 1282
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1283 [label="credit: 1283
name: K - Q - V
operator_type: MatMul
"];
	1284 [label="credit: 1284
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1285 [label="credit: 1285
name: K - Q - V merged
operator_type: Permute
"];
	1286 [label="credit: 1286
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1287 [label="credit: 1287
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1288 [label="credit: 1288
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1289 [label="credit: 1289
name: K - Q - V weight
operator_type: MatMul
"];
	1290 [label="credit: 1290
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1291 [label="credit: 1291
name: FFN input
operator_type: Add
"];
	1292 [label="credit: 1292
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1293 [label="credit: 1293
name: FFN - norm
operator_type: NormRMS
"];
	1294 [label="credit: 1294
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1295 [label="credit: 1295
name: FFN - norm
operator_type: Mul
"];
	1296 [label="credit: 1296
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1297 [label="credit: 1297
name: FFN up
operator_type: MatMul
"];
	1298 [label="credit: 1298
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1299 [label="credit: 1299
name: FFN gate
operator_type: MatMul
"];
	1300 [label="credit: 1300
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1301 [label="credit: 1301
name: FFN gate silu
operator_type: Silu
"];
	1302 [label="credit: 1302
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1303 [label="credit: 1303
name: FFN par
operator_type: Mul
"];
	1304 [label="credit: 1304
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1305 [label="credit: 1305
name: FFN down
operator_type: MatMul
"];
	1306 [label="credit: 1306
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1307 [label="credit: 1307
name: Logit out
operator_type: Add
"];
	1308 [label="credit: 1308
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1309 [label="credit: 1309
name: attn mask
operator_type: NormRMS
"];
	1310 [label="credit: 1310
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1311 [label="credit: 1311
name: attn norm
operator_type: Mul
"];
	1312 [label="credit: 1312
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1313 [label="credit: 1313
name: Qcur
operator_type: MatMul
"];
	1314 [label="credit: 1314
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1315 [label="credit: 1315
name: Kcur
operator_type: MatMul
"];
	1316 [label="credit: 1316
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1317 [label="credit: 1317
name: Vcur
operator_type: MatMul
"];
	1318 [label="credit: 1318
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1319 [label="credit: 1319
name: Qcur - reshaped
operator_type: Reshape
"];
	1320 [label="credit: 1320
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1321 [label="credit: 1321
name: Kcur - reshaped
operator_type: Reshape
"];
	1322 [label="credit: 1322
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1323 [label="credit: 1323
name: Qcur - rope
operator_type: Rope
"];
	1324 [label="credit: 1324
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1325 [label="credit: 1325
name: Kcur - rope
operator_type: Rope
"];
	1326 [label="credit: 1326
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1327 [label="credit: 1327
name: Q - out
operator_type: Permute
"];
	1328 [label="credit: 1328
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1329 [label="credit: 1329
name: K - out
operator_type: View
"];
	1330 [label="credit: 1330
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1331 [label="credit: 1331
name: Attention Score
operator_type: MatMul
"];
	1332 [label="credit: 1332
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1333 [label="credit: 1333
name: Attention Context
operator_type: Softmax
"];
	1334 [label="credit: 1334
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1335 [label="credit: 1335
name: V - transpose
operator_type: Transpose
"];
	1336 [label="credit: 1336
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1337 [label="credit: 1337
name: V - out
operator_type: View
"];
	1338 [label="credit: 1338
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1339 [label="credit: 1339
name: K - Q - V
operator_type: MatMul
"];
	1340 [label="credit: 1340
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1341 [label="credit: 1341
name: K - Q - V merged
operator_type: Permute
"];
	1342 [label="credit: 1342
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1343 [label="credit: 1343
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1344 [label="credit: 1344
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1345 [label="credit: 1345
name: K - Q - V weight
operator_type: MatMul
"];
	1346 [label="credit: 1346
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1347 [label="credit: 1347
name: FFN input
operator_type: Add
"];
	1348 [label="credit: 1348
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1349 [label="credit: 1349
name: FFN - norm
operator_type: NormRMS
"];
	1350 [label="credit: 1350
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1351 [label="credit: 1351
name: FFN - norm
operator_type: Mul
"];
	1352 [label="credit: 1352
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1353 [label="credit: 1353
name: FFN up
operator_type: MatMul
"];
	1354 [label="credit: 1354
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1355 [label="credit: 1355
name: FFN gate
operator_type: MatMul
"];
	1356 [label="credit: 1356
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1357 [label="credit: 1357
name: FFN gate silu
operator_type: Silu
"];
	1358 [label="credit: 1358
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1359 [label="credit: 1359
name: FFN par
operator_type: Mul
"];
	1360 [label="credit: 1360
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1361 [label="credit: 1361
name: FFN down
operator_type: MatMul
"];
	1362 [label="credit: 1362
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1363 [label="credit: 1363
name: Logit out
operator_type: Add
"];
	1364 [label="credit: 1364
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1365 [label="credit: 1365
name: attn mask
operator_type: NormRMS
"];
	1366 [label="credit: 1366
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1367 [label="credit: 1367
name: attn norm
operator_type: Mul
"];
	1368 [label="credit: 1368
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1369 [label="credit: 1369
name: Qcur
operator_type: MatMul
"];
	1370 [label="credit: 1370
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1371 [label="credit: 1371
name: Kcur
operator_type: MatMul
"];
	1372 [label="credit: 1372
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1373 [label="credit: 1373
name: Vcur
operator_type: MatMul
"];
	1374 [label="credit: 1374
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1375 [label="credit: 1375
name: Qcur - reshaped
operator_type: Reshape
"];
	1376 [label="credit: 1376
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1377 [label="credit: 1377
name: Kcur - reshaped
operator_type: Reshape
"];
	1378 [label="credit: 1378
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1379 [label="credit: 1379
name: Qcur - rope
operator_type: Rope
"];
	1380 [label="credit: 1380
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1381 [label="credit: 1381
name: Kcur - rope
operator_type: Rope
"];
	1382 [label="credit: 1382
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1383 [label="credit: 1383
name: Q - out
operator_type: Permute
"];
	1384 [label="credit: 1384
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1385 [label="credit: 1385
name: K - out
operator_type: View
"];
	1386 [label="credit: 1386
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1387 [label="credit: 1387
name: Attention Score
operator_type: MatMul
"];
	1388 [label="credit: 1388
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1389 [label="credit: 1389
name: Attention Context
operator_type: Softmax
"];
	1390 [label="credit: 1390
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1391 [label="credit: 1391
name: V - transpose
operator_type: Transpose
"];
	1392 [label="credit: 1392
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1393 [label="credit: 1393
name: V - out
operator_type: View
"];
	1394 [label="credit: 1394
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1395 [label="credit: 1395
name: K - Q - V
operator_type: MatMul
"];
	1396 [label="credit: 1396
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1397 [label="credit: 1397
name: K - Q - V merged
operator_type: Permute
"];
	1398 [label="credit: 1398
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1399 [label="credit: 1399
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1400 [label="credit: 1400
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1401 [label="credit: 1401
name: K - Q - V weight
operator_type: MatMul
"];
	1402 [label="credit: 1402
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1403 [label="credit: 1403
name: FFN input
operator_type: Add
"];
	1404 [label="credit: 1404
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1405 [label="credit: 1405
name: FFN - norm
operator_type: NormRMS
"];
	1406 [label="credit: 1406
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1407 [label="credit: 1407
name: FFN - norm
operator_type: Mul
"];
	1408 [label="credit: 1408
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1409 [label="credit: 1409
name: FFN up
operator_type: MatMul
"];
	1410 [label="credit: 1410
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1411 [label="credit: 1411
name: FFN gate
operator_type: MatMul
"];
	1412 [label="credit: 1412
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1413 [label="credit: 1413
name: FFN gate silu
operator_type: Silu
"];
	1414 [label="credit: 1414
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1415 [label="credit: 1415
name: FFN par
operator_type: Mul
"];
	1416 [label="credit: 1416
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1417 [label="credit: 1417
name: FFN down
operator_type: MatMul
"];
	1418 [label="credit: 1418
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1419 [label="credit: 1419
name: Logit out
operator_type: Add
"];
	1420 [label="credit: 1420
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1421 [label="credit: 1421
name: attn mask
operator_type: NormRMS
"];
	1422 [label="credit: 1422
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1423 [label="credit: 1423
name: attn norm
operator_type: Mul
"];
	1424 [label="credit: 1424
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1425 [label="credit: 1425
name: Qcur
operator_type: MatMul
"];
	1426 [label="credit: 1426
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1427 [label="credit: 1427
name: Kcur
operator_type: MatMul
"];
	1428 [label="credit: 1428
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1429 [label="credit: 1429
name: Vcur
operator_type: MatMul
"];
	1430 [label="credit: 1430
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1431 [label="credit: 1431
name: Qcur - reshaped
operator_type: Reshape
"];
	1432 [label="credit: 1432
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1433 [label="credit: 1433
name: Kcur - reshaped
operator_type: Reshape
"];
	1434 [label="credit: 1434
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1435 [label="credit: 1435
name: Qcur - rope
operator_type: Rope
"];
	1436 [label="credit: 1436
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1437 [label="credit: 1437
name: Kcur - rope
operator_type: Rope
"];
	1438 [label="credit: 1438
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1439 [label="credit: 1439
name: Q - out
operator_type: Permute
"];
	1440 [label="credit: 1440
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1441 [label="credit: 1441
name: K - out
operator_type: View
"];
	1442 [label="credit: 1442
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1443 [label="credit: 1443
name: Attention Score
operator_type: MatMul
"];
	1444 [label="credit: 1444
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1445 [label="credit: 1445
name: Attention Context
operator_type: Softmax
"];
	1446 [label="credit: 1446
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1447 [label="credit: 1447
name: V - transpose
operator_type: Transpose
"];
	1448 [label="credit: 1448
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1449 [label="credit: 1449
name: V - out
operator_type: View
"];
	1450 [label="credit: 1450
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1451 [label="credit: 1451
name: K - Q - V
operator_type: MatMul
"];
	1452 [label="credit: 1452
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1453 [label="credit: 1453
name: K - Q - V merged
operator_type: Permute
"];
	1454 [label="credit: 1454
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1455 [label="credit: 1455
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1456 [label="credit: 1456
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1457 [label="credit: 1457
name: K - Q - V weight
operator_type: MatMul
"];
	1458 [label="credit: 1458
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1459 [label="credit: 1459
name: FFN input
operator_type: Add
"];
	1460 [label="credit: 1460
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1461 [label="credit: 1461
name: FFN - norm
operator_type: NormRMS
"];
	1462 [label="credit: 1462
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1463 [label="credit: 1463
name: FFN - norm
operator_type: Mul
"];
	1464 [label="credit: 1464
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1465 [label="credit: 1465
name: FFN up
operator_type: MatMul
"];
	1466 [label="credit: 1466
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1467 [label="credit: 1467
name: FFN gate
operator_type: MatMul
"];
	1468 [label="credit: 1468
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1469 [label="credit: 1469
name: FFN gate silu
operator_type: Silu
"];
	1470 [label="credit: 1470
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1471 [label="credit: 1471
name: FFN par
operator_type: Mul
"];
	1472 [label="credit: 1472
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1473 [label="credit: 1473
name: FFN down
operator_type: MatMul
"];
	1474 [label="credit: 1474
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1475 [label="credit: 1475
name: Logit out
operator_type: Add
"];
	1476 [label="credit: 1476
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1477 [label="credit: 1477
name: attn mask
operator_type: NormRMS
"];
	1478 [label="credit: 1478
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1479 [label="credit: 1479
name: attn norm
operator_type: Mul
"];
	1480 [label="credit: 1480
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1481 [label="credit: 1481
name: Qcur
operator_type: MatMul
"];
	1482 [label="credit: 1482
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1483 [label="credit: 1483
name: Kcur
operator_type: MatMul
"];
	1484 [label="credit: 1484
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1485 [label="credit: 1485
name: Vcur
operator_type: MatMul
"];
	1486 [label="credit: 1486
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1487 [label="credit: 1487
name: Qcur - reshaped
operator_type: Reshape
"];
	1488 [label="credit: 1488
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1489 [label="credit: 1489
name: Kcur - reshaped
operator_type: Reshape
"];
	1490 [label="credit: 1490
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1491 [label="credit: 1491
name: Qcur - rope
operator_type: Rope
"];
	1492 [label="credit: 1492
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1493 [label="credit: 1493
name: Kcur - rope
operator_type: Rope
"];
	1494 [label="credit: 1494
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1495 [label="credit: 1495
name: Q - out
operator_type: Permute
"];
	1496 [label="credit: 1496
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1497 [label="credit: 1497
name: K - out
operator_type: View
"];
	1498 [label="credit: 1498
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1499 [label="credit: 1499
name: Attention Score
operator_type: MatMul
"];
	1500 [label="credit: 1500
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1501 [label="credit: 1501
name: Attention Context
operator_type: Softmax
"];
	1502 [label="credit: 1502
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1503 [label="credit: 1503
name: V - transpose
operator_type: Transpose
"];
	1504 [label="credit: 1504
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1505 [label="credit: 1505
name: V - out
operator_type: View
"];
	1506 [label="credit: 1506
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1507 [label="credit: 1507
name: K - Q - V
operator_type: MatMul
"];
	1508 [label="credit: 1508
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1509 [label="credit: 1509
name: K - Q - V merged
operator_type: Permute
"];
	1510 [label="credit: 1510
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1511 [label="credit: 1511
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1512 [label="credit: 1512
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1513 [label="credit: 1513
name: K - Q - V weight
operator_type: MatMul
"];
	1514 [label="credit: 1514
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1515 [label="credit: 1515
name: FFN input
operator_type: Add
"];
	1516 [label="credit: 1516
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1517 [label="credit: 1517
name: FFN - norm
operator_type: NormRMS
"];
	1518 [label="credit: 1518
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1519 [label="credit: 1519
name: FFN - norm
operator_type: Mul
"];
	1520 [label="credit: 1520
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1521 [label="credit: 1521
name: FFN up
operator_type: MatMul
"];
	1522 [label="credit: 1522
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1523 [label="credit: 1523
name: FFN gate
operator_type: MatMul
"];
	1524 [label="credit: 1524
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1525 [label="credit: 1525
name: FFN gate silu
operator_type: Silu
"];
	1526 [label="credit: 1526
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1527 [label="credit: 1527
name: FFN par
operator_type: Mul
"];
	1528 [label="credit: 1528
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1529 [label="credit: 1529
name: FFN down
operator_type: MatMul
"];
	1530 [label="credit: 1530
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1531 [label="credit: 1531
name: Logit out
operator_type: Add
"];
	1532 [label="credit: 1532
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1533 [label="credit: 1533
name: attn mask
operator_type: NormRMS
"];
	1534 [label="credit: 1534
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1535 [label="credit: 1535
name: attn norm
operator_type: Mul
"];
	1536 [label="credit: 1536
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1537 [label="credit: 1537
name: Qcur
operator_type: MatMul
"];
	1538 [label="credit: 1538
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1539 [label="credit: 1539
name: Kcur
operator_type: MatMul
"];
	1540 [label="credit: 1540
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1541 [label="credit: 1541
name: Vcur
operator_type: MatMul
"];
	1542 [label="credit: 1542
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1543 [label="credit: 1543
name: Qcur - reshaped
operator_type: Reshape
"];
	1544 [label="credit: 1544
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1545 [label="credit: 1545
name: Kcur - reshaped
operator_type: Reshape
"];
	1546 [label="credit: 1546
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1547 [label="credit: 1547
name: Qcur - rope
operator_type: Rope
"];
	1548 [label="credit: 1548
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1549 [label="credit: 1549
name: Kcur - rope
operator_type: Rope
"];
	1550 [label="credit: 1550
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1551 [label="credit: 1551
name: Q - out
operator_type: Permute
"];
	1552 [label="credit: 1552
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1553 [label="credit: 1553
name: K - out
operator_type: View
"];
	1554 [label="credit: 1554
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1555 [label="credit: 1555
name: Attention Score
operator_type: MatMul
"];
	1556 [label="credit: 1556
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1557 [label="credit: 1557
name: Attention Context
operator_type: Softmax
"];
	1558 [label="credit: 1558
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1559 [label="credit: 1559
name: V - transpose
operator_type: Transpose
"];
	1560 [label="credit: 1560
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1561 [label="credit: 1561
name: V - out
operator_type: View
"];
	1562 [label="credit: 1562
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1563 [label="credit: 1563
name: K - Q - V
operator_type: MatMul
"];
	1564 [label="credit: 1564
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1565 [label="credit: 1565
name: K - Q - V merged
operator_type: Permute
"];
	1566 [label="credit: 1566
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1567 [label="credit: 1567
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1568 [label="credit: 1568
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1569 [label="credit: 1569
name: K - Q - V weight
operator_type: MatMul
"];
	1570 [label="credit: 1570
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1571 [label="credit: 1571
name: FFN input
operator_type: Add
"];
	1572 [label="credit: 1572
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1573 [label="credit: 1573
name: FFN - norm
operator_type: NormRMS
"];
	1574 [label="credit: 1574
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1575 [label="credit: 1575
name: FFN - norm
operator_type: Mul
"];
	1576 [label="credit: 1576
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1577 [label="credit: 1577
name: FFN up
operator_type: MatMul
"];
	1578 [label="credit: 1578
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1579 [label="credit: 1579
name: FFN gate
operator_type: MatMul
"];
	1580 [label="credit: 1580
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1581 [label="credit: 1581
name: FFN gate silu
operator_type: Silu
"];
	1582 [label="credit: 1582
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1583 [label="credit: 1583
name: FFN par
operator_type: Mul
"];
	1584 [label="credit: 1584
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1585 [label="credit: 1585
name: FFN down
operator_type: MatMul
"];
	1586 [label="credit: 1586
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1587 [label="credit: 1587
name: Logit out
operator_type: Add
"];
	1588 [label="credit: 1588
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1589 [label="credit: 1589
name: attn mask
operator_type: NormRMS
"];
	1590 [label="credit: 1590
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1591 [label="credit: 1591
name: attn norm
operator_type: Mul
"];
	1592 [label="credit: 1592
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1593 [label="credit: 1593
name: Qcur
operator_type: MatMul
"];
	1594 [label="credit: 1594
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1595 [label="credit: 1595
name: Kcur
operator_type: MatMul
"];
	1596 [label="credit: 1596
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1597 [label="credit: 1597
name: Vcur
operator_type: MatMul
"];
	1598 [label="credit: 1598
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1599 [label="credit: 1599
name: Qcur - reshaped
operator_type: Reshape
"];
	1600 [label="credit: 1600
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1601 [label="credit: 1601
name: Kcur - reshaped
operator_type: Reshape
"];
	1602 [label="credit: 1602
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1603 [label="credit: 1603
name: Qcur - rope
operator_type: Rope
"];
	1604 [label="credit: 1604
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1605 [label="credit: 1605
name: Kcur - rope
operator_type: Rope
"];
	1606 [label="credit: 1606
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1607 [label="credit: 1607
name: Q - out
operator_type: Permute
"];
	1608 [label="credit: 1608
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1609 [label="credit: 1609
name: K - out
operator_type: View
"];
	1610 [label="credit: 1610
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1611 [label="credit: 1611
name: Attention Score
operator_type: MatMul
"];
	1612 [label="credit: 1612
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1613 [label="credit: 1613
name: Attention Context
operator_type: Softmax
"];
	1614 [label="credit: 1614
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1615 [label="credit: 1615
name: V - transpose
operator_type: Transpose
"];
	1616 [label="credit: 1616
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1617 [label="credit: 1617
name: V - out
operator_type: View
"];
	1618 [label="credit: 1618
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1619 [label="credit: 1619
name: K - Q - V
operator_type: MatMul
"];
	1620 [label="credit: 1620
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1621 [label="credit: 1621
name: K - Q - V merged
operator_type: Permute
"];
	1622 [label="credit: 1622
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1623 [label="credit: 1623
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1624 [label="credit: 1624
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1625 [label="credit: 1625
name: K - Q - V weight
operator_type: MatMul
"];
	1626 [label="credit: 1626
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1627 [label="credit: 1627
name: FFN input
operator_type: Add
"];
	1628 [label="credit: 1628
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1629 [label="credit: 1629
name: FFN - norm
operator_type: NormRMS
"];
	1630 [label="credit: 1630
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1631 [label="credit: 1631
name: FFN - norm
operator_type: Mul
"];
	1632 [label="credit: 1632
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1633 [label="credit: 1633
name: FFN up
operator_type: MatMul
"];
	1634 [label="credit: 1634
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1635 [label="credit: 1635
name: FFN gate
operator_type: MatMul
"];
	1636 [label="credit: 1636
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1637 [label="credit: 1637
name: FFN gate silu
operator_type: Silu
"];
	1638 [label="credit: 1638
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1639 [label="credit: 1639
name: FFN par
operator_type: Mul
"];
	1640 [label="credit: 1640
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1641 [label="credit: 1641
name: FFN down
operator_type: MatMul
"];
	1642 [label="credit: 1642
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1643 [label="credit: 1643
name: Logit out
operator_type: Add
"];
	1644 [label="credit: 1644
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1645 [label="credit: 1645
name: attn mask
operator_type: NormRMS
"];
	1646 [label="credit: 1646
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1647 [label="credit: 1647
name: attn norm
operator_type: Mul
"];
	1648 [label="credit: 1648
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1649 [label="credit: 1649
name: Qcur
operator_type: MatMul
"];
	1650 [label="credit: 1650
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1651 [label="credit: 1651
name: Kcur
operator_type: MatMul
"];
	1652 [label="credit: 1652
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1653 [label="credit: 1653
name: Vcur
operator_type: MatMul
"];
	1654 [label="credit: 1654
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1655 [label="credit: 1655
name: Qcur - reshaped
operator_type: Reshape
"];
	1656 [label="credit: 1656
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1657 [label="credit: 1657
name: Kcur - reshaped
operator_type: Reshape
"];
	1658 [label="credit: 1658
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1659 [label="credit: 1659
name: Qcur - rope
operator_type: Rope
"];
	1660 [label="credit: 1660
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1661 [label="credit: 1661
name: Kcur - rope
operator_type: Rope
"];
	1662 [label="credit: 1662
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1663 [label="credit: 1663
name: Q - out
operator_type: Permute
"];
	1664 [label="credit: 1664
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1665 [label="credit: 1665
name: K - out
operator_type: View
"];
	1666 [label="credit: 1666
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1667 [label="credit: 1667
name: Attention Score
operator_type: MatMul
"];
	1668 [label="credit: 1668
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1669 [label="credit: 1669
name: Attention Context
operator_type: Softmax
"];
	1670 [label="credit: 1670
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1671 [label="credit: 1671
name: V - transpose
operator_type: Transpose
"];
	1672 [label="credit: 1672
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1673 [label="credit: 1673
name: V - out
operator_type: View
"];
	1674 [label="credit: 1674
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1675 [label="credit: 1675
name: K - Q - V
operator_type: MatMul
"];
	1676 [label="credit: 1676
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1677 [label="credit: 1677
name: K - Q - V merged
operator_type: Permute
"];
	1678 [label="credit: 1678
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1679 [label="credit: 1679
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1680 [label="credit: 1680
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1681 [label="credit: 1681
name: K - Q - V weight
operator_type: MatMul
"];
	1682 [label="credit: 1682
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1683 [label="credit: 1683
name: FFN input
operator_type: Add
"];
	1684 [label="credit: 1684
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1685 [label="credit: 1685
name: FFN - norm
operator_type: NormRMS
"];
	1686 [label="credit: 1686
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1687 [label="credit: 1687
name: FFN - norm
operator_type: Mul
"];
	1688 [label="credit: 1688
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1689 [label="credit: 1689
name: FFN up
operator_type: MatMul
"];
	1690 [label="credit: 1690
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1691 [label="credit: 1691
name: FFN gate
operator_type: MatMul
"];
	1692 [label="credit: 1692
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1693 [label="credit: 1693
name: FFN gate silu
operator_type: Silu
"];
	1694 [label="credit: 1694
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1695 [label="credit: 1695
name: FFN par
operator_type: Mul
"];
	1696 [label="credit: 1696
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1697 [label="credit: 1697
name: FFN down
operator_type: MatMul
"];
	1698 [label="credit: 1698
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1699 [label="credit: 1699
name: Logit out
operator_type: Add
"];
	1700 [label="credit: 1700
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1701 [label="credit: 1701
name: attn mask
operator_type: NormRMS
"];
	1702 [label="credit: 1702
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1703 [label="credit: 1703
name: attn norm
operator_type: Mul
"];
	1704 [label="credit: 1704
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1705 [label="credit: 1705
name: Qcur
operator_type: MatMul
"];
	1706 [label="credit: 1706
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1707 [label="credit: 1707
name: Kcur
operator_type: MatMul
"];
	1708 [label="credit: 1708
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1709 [label="credit: 1709
name: Vcur
operator_type: MatMul
"];
	1710 [label="credit: 1710
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1711 [label="credit: 1711
name: Qcur - reshaped
operator_type: Reshape
"];
	1712 [label="credit: 1712
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1713 [label="credit: 1713
name: Kcur - reshaped
operator_type: Reshape
"];
	1714 [label="credit: 1714
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1715 [label="credit: 1715
name: Qcur - rope
operator_type: Rope
"];
	1716 [label="credit: 1716
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1717 [label="credit: 1717
name: Kcur - rope
operator_type: Rope
"];
	1718 [label="credit: 1718
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1719 [label="credit: 1719
name: Q - out
operator_type: Permute
"];
	1720 [label="credit: 1720
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1721 [label="credit: 1721
name: K - out
operator_type: View
"];
	1722 [label="credit: 1722
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1723 [label="credit: 1723
name: Attention Score
operator_type: MatMul
"];
	1724 [label="credit: 1724
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1725 [label="credit: 1725
name: Attention Context
operator_type: Softmax
"];
	1726 [label="credit: 1726
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1727 [label="credit: 1727
name: V - transpose
operator_type: Transpose
"];
	1728 [label="credit: 1728
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1729 [label="credit: 1729
name: V - out
operator_type: View
"];
	1730 [label="credit: 1730
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1731 [label="credit: 1731
name: K - Q - V
operator_type: MatMul
"];
	1732 [label="credit: 1732
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1733 [label="credit: 1733
name: K - Q - V merged
operator_type: Permute
"];
	1734 [label="credit: 1734
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1735 [label="credit: 1735
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1736 [label="credit: 1736
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1737 [label="credit: 1737
name: K - Q - V weight
operator_type: MatMul
"];
	1738 [label="credit: 1738
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1739 [label="credit: 1739
name: FFN input
operator_type: Add
"];
	1740 [label="credit: 1740
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1741 [label="credit: 1741
name: FFN - norm
operator_type: NormRMS
"];
	1742 [label="credit: 1742
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1743 [label="credit: 1743
name: FFN - norm
operator_type: Mul
"];
	1744 [label="credit: 1744
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1745 [label="credit: 1745
name: FFN up
operator_type: MatMul
"];
	1746 [label="credit: 1746
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1747 [label="credit: 1747
name: FFN gate
operator_type: MatMul
"];
	1748 [label="credit: 1748
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1749 [label="credit: 1749
name: FFN gate silu
operator_type: Silu
"];
	1750 [label="credit: 1750
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1751 [label="credit: 1751
name: FFN par
operator_type: Mul
"];
	1752 [label="credit: 1752
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1753 [label="credit: 1753
name: FFN down
operator_type: MatMul
"];
	1754 [label="credit: 1754
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1755 [label="credit: 1755
name: Logit out
operator_type: Add
"];
	1756 [label="credit: 1756
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1757 [label="credit: 1757
name: attn mask
operator_type: NormRMS
"];
	1758 [label="credit: 1758
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1759 [label="credit: 1759
name: attn norm
operator_type: Mul
"];
	1760 [label="credit: 1760
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1761 [label="credit: 1761
name: Qcur
operator_type: MatMul
"];
	1762 [label="credit: 1762
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1763 [label="credit: 1763
name: Kcur
operator_type: MatMul
"];
	1764 [label="credit: 1764
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1765 [label="credit: 1765
name: Vcur
operator_type: MatMul
"];
	1766 [label="credit: 1766
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1767 [label="credit: 1767
name: Qcur - reshaped
operator_type: Reshape
"];
	1768 [label="credit: 1768
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1769 [label="credit: 1769
name: Kcur - reshaped
operator_type: Reshape
"];
	1770 [label="credit: 1770
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1771 [label="credit: 1771
name: Qcur - rope
operator_type: Rope
"];
	1772 [label="credit: 1772
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1773 [label="credit: 1773
name: Kcur - rope
operator_type: Rope
"];
	1774 [label="credit: 1774
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1775 [label="credit: 1775
name: Q - out
operator_type: Permute
"];
	1776 [label="credit: 1776
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1777 [label="credit: 1777
name: K - out
operator_type: View
"];
	1778 [label="credit: 1778
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1779 [label="credit: 1779
name: Attention Score
operator_type: MatMul
"];
	1780 [label="credit: 1780
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1781 [label="credit: 1781
name: Attention Context
operator_type: Softmax
"];
	1782 [label="credit: 1782
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1783 [label="credit: 1783
name: V - transpose
operator_type: Transpose
"];
	1784 [label="credit: 1784
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1785 [label="credit: 1785
name: V - out
operator_type: View
"];
	1786 [label="credit: 1786
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1787 [label="credit: 1787
name: K - Q - V
operator_type: MatMul
"];
	1788 [label="credit: 1788
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1789 [label="credit: 1789
name: K - Q - V merged
operator_type: Permute
"];
	1790 [label="credit: 1790
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1791 [label="credit: 1791
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1792 [label="credit: 1792
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1793 [label="credit: 1793
name: K - Q - V weight
operator_type: MatMul
"];
	1794 [label="credit: 1794
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1795 [label="credit: 1795
name: FFN input
operator_type: Add
"];
	1796 [label="credit: 1796
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1797 [label="credit: 1797
name: FFN - norm
operator_type: NormRMS
"];
	1798 [label="credit: 1798
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1799 [label="credit: 1799
name: FFN - norm
operator_type: Mul
"];
	1800 [label="credit: 1800
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1801 [label="credit: 1801
name: FFN up
operator_type: MatMul
"];
	1802 [label="credit: 1802
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1803 [label="credit: 1803
name: FFN gate
operator_type: MatMul
"];
	1804 [label="credit: 1804
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1805 [label="credit: 1805
name: FFN gate silu
operator_type: Silu
"];
	1806 [label="credit: 1806
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1807 [label="credit: 1807
name: FFN par
operator_type: Mul
"];
	1808 [label="credit: 1808
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1809 [label="credit: 1809
name: FFN down
operator_type: MatMul
"];
	1810 [label="credit: 1810
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1811 [label="credit: 1811
name: Logit out
operator_type: Add
"];
	1812 [label="credit: 1812
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1813 [label="credit: 1813
name: attn mask
operator_type: NormRMS
"];
	1814 [label="credit: 1814
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1815 [label="credit: 1815
name: attn norm
operator_type: Mul
"];
	1816 [label="credit: 1816
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1817 [label="credit: 1817
name: Qcur
operator_type: MatMul
"];
	1818 [label="credit: 1818
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1819 [label="credit: 1819
name: Kcur
operator_type: MatMul
"];
	1820 [label="credit: 1820
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1821 [label="credit: 1821
name: Vcur
operator_type: MatMul
"];
	1822 [label="credit: 1822
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1823 [label="credit: 1823
name: Qcur - reshaped
operator_type: Reshape
"];
	1824 [label="credit: 1824
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1825 [label="credit: 1825
name: Kcur - reshaped
operator_type: Reshape
"];
	1826 [label="credit: 1826
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1827 [label="credit: 1827
name: Qcur - rope
operator_type: Rope
"];
	1828 [label="credit: 1828
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1829 [label="credit: 1829
name: Kcur - rope
operator_type: Rope
"];
	1830 [label="credit: 1830
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1831 [label="credit: 1831
name: Q - out
operator_type: Permute
"];
	1832 [label="credit: 1832
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1833 [label="credit: 1833
name: K - out
operator_type: View
"];
	1834 [label="credit: 1834
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1835 [label="credit: 1835
name: Attention Score
operator_type: MatMul
"];
	1836 [label="credit: 1836
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1837 [label="credit: 1837
name: Attention Context
operator_type: Softmax
"];
	1838 [label="credit: 1838
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1839 [label="credit: 1839
name: V - transpose
operator_type: Transpose
"];
	1840 [label="credit: 1840
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1841 [label="credit: 1841
name: V - out
operator_type: View
"];
	1842 [label="credit: 1842
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1843 [label="credit: 1843
name: K - Q - V
operator_type: MatMul
"];
	1844 [label="credit: 1844
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1845 [label="credit: 1845
name: K - Q - V merged
operator_type: Permute
"];
	1846 [label="credit: 1846
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1847 [label="credit: 1847
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1848 [label="credit: 1848
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1849 [label="credit: 1849
name: K - Q - V weight
operator_type: MatMul
"];
	1850 [label="credit: 1850
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1851 [label="credit: 1851
name: FFN input
operator_type: Add
"];
	1852 [label="credit: 1852
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1853 [label="credit: 1853
name: FFN - norm
operator_type: NormRMS
"];
	1854 [label="credit: 1854
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1855 [label="credit: 1855
name: FFN - norm
operator_type: Mul
"];
	1856 [label="credit: 1856
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1857 [label="credit: 1857
name: FFN up
operator_type: MatMul
"];
	1858 [label="credit: 1858
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1859 [label="credit: 1859
name: FFN gate
operator_type: MatMul
"];
	1860 [label="credit: 1860
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1861 [label="credit: 1861
name: FFN gate silu
operator_type: Silu
"];
	1862 [label="credit: 1862
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1863 [label="credit: 1863
name: FFN par
operator_type: Mul
"];
	1864 [label="credit: 1864
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1865 [label="credit: 1865
name: FFN down
operator_type: MatMul
"];
	1866 [label="credit: 1866
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1867 [label="credit: 1867
name: Logit out
operator_type: Add
"];
	1868 [label="credit: 1868
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1869 [label="credit: 1869
name: attn mask
operator_type: NormRMS
"];
	1870 [label="credit: 1870
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1871 [label="credit: 1871
name: attn norm
operator_type: Mul
"];
	1872 [label="credit: 1872
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1873 [label="credit: 1873
name: Qcur
operator_type: MatMul
"];
	1874 [label="credit: 1874
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1875 [label="credit: 1875
name: Kcur
operator_type: MatMul
"];
	1876 [label="credit: 1876
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1877 [label="credit: 1877
name: Vcur
operator_type: MatMul
"];
	1878 [label="credit: 1878
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1879 [label="credit: 1879
name: Qcur - reshaped
operator_type: Reshape
"];
	1880 [label="credit: 1880
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1881 [label="credit: 1881
name: Kcur - reshaped
operator_type: Reshape
"];
	1882 [label="credit: 1882
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1883 [label="credit: 1883
name: Qcur - rope
operator_type: Rope
"];
	1884 [label="credit: 1884
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1885 [label="credit: 1885
name: Kcur - rope
operator_type: Rope
"];
	1886 [label="credit: 1886
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1887 [label="credit: 1887
name: Q - out
operator_type: Permute
"];
	1888 [label="credit: 1888
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1889 [label="credit: 1889
name: K - out
operator_type: View
"];
	1890 [label="credit: 1890
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1891 [label="credit: 1891
name: Attention Score
operator_type: MatMul
"];
	1892 [label="credit: 1892
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1893 [label="credit: 1893
name: Attention Context
operator_type: Softmax
"];
	1894 [label="credit: 1894
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1895 [label="credit: 1895
name: V - transpose
operator_type: Transpose
"];
	1896 [label="credit: 1896
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1897 [label="credit: 1897
name: V - out
operator_type: View
"];
	1898 [label="credit: 1898
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1899 [label="credit: 1899
name: K - Q - V
operator_type: MatMul
"];
	1900 [label="credit: 1900
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1901 [label="credit: 1901
name: K - Q - V merged
operator_type: Permute
"];
	1902 [label="credit: 1902
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1903 [label="credit: 1903
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1904 [label="credit: 1904
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1905 [label="credit: 1905
name: K - Q - V weight
operator_type: MatMul
"];
	1906 [label="credit: 1906
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1907 [label="credit: 1907
name: FFN input
operator_type: Add
"];
	1908 [label="credit: 1908
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1909 [label="credit: 1909
name: FFN - norm
operator_type: NormRMS
"];
	1910 [label="credit: 1910
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1911 [label="credit: 1911
name: FFN - norm
operator_type: Mul
"];
	1912 [label="credit: 1912
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1913 [label="credit: 1913
name: FFN up
operator_type: MatMul
"];
	1914 [label="credit: 1914
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1915 [label="credit: 1915
name: FFN gate
operator_type: MatMul
"];
	1916 [label="credit: 1916
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1917 [label="credit: 1917
name: FFN gate silu
operator_type: Silu
"];
	1918 [label="credit: 1918
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1919 [label="credit: 1919
name: FFN par
operator_type: Mul
"];
	1920 [label="credit: 1920
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1921 [label="credit: 1921
name: FFN down
operator_type: MatMul
"];
	1922 [label="credit: 1922
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1923 [label="credit: 1923
name: Logit out
operator_type: Add
"];
	1924 [label="credit: 1924
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1925 [label="credit: 1925
name: attn mask
operator_type: NormRMS
"];
	1926 [label="credit: 1926
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1927 [label="credit: 1927
name: attn norm
operator_type: Mul
"];
	1928 [label="credit: 1928
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1929 [label="credit: 1929
name: Qcur
operator_type: MatMul
"];
	1930 [label="credit: 1930
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1931 [label="credit: 1931
name: Kcur
operator_type: MatMul
"];
	1932 [label="credit: 1932
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1933 [label="credit: 1933
name: Vcur
operator_type: MatMul
"];
	1934 [label="credit: 1934
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1935 [label="credit: 1935
name: Qcur - reshaped
operator_type: Reshape
"];
	1936 [label="credit: 1936
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1937 [label="credit: 1937
name: Kcur - reshaped
operator_type: Reshape
"];
	1938 [label="credit: 1938
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1939 [label="credit: 1939
name: Qcur - rope
operator_type: Rope
"];
	1940 [label="credit: 1940
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1941 [label="credit: 1941
name: Kcur - rope
operator_type: Rope
"];
	1942 [label="credit: 1942
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1943 [label="credit: 1943
name: Q - out
operator_type: Permute
"];
	1944 [label="credit: 1944
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1945 [label="credit: 1945
name: K - out
operator_type: View
"];
	1946 [label="credit: 1946
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1947 [label="credit: 1947
name: Attention Score
operator_type: MatMul
"];
	1948 [label="credit: 1948
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1949 [label="credit: 1949
name: Attention Context
operator_type: Softmax
"];
	1950 [label="credit: 1950
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	1951 [label="credit: 1951
name: V - transpose
operator_type: Transpose
"];
	1952 [label="credit: 1952
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1953 [label="credit: 1953
name: V - out
operator_type: View
"];
	1954 [label="credit: 1954
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	1955 [label="credit: 1955
name: K - Q - V
operator_type: MatMul
"];
	1956 [label="credit: 1956
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	1957 [label="credit: 1957
name: K - Q - V merged
operator_type: Permute
"];
	1958 [label="credit: 1958
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1959 [label="credit: 1959
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	1960 [label="credit: 1960
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1961 [label="credit: 1961
name: K - Q - V weight
operator_type: MatMul
"];
	1962 [label="credit: 1962
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1963 [label="credit: 1963
name: FFN input
operator_type: Add
"];
	1964 [label="credit: 1964
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1965 [label="credit: 1965
name: FFN - norm
operator_type: NormRMS
"];
	1966 [label="credit: 1966
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1967 [label="credit: 1967
name: FFN - norm
operator_type: Mul
"];
	1968 [label="credit: 1968
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1969 [label="credit: 1969
name: FFN up
operator_type: MatMul
"];
	1970 [label="credit: 1970
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1971 [label="credit: 1971
name: FFN gate
operator_type: MatMul
"];
	1972 [label="credit: 1972
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1973 [label="credit: 1973
name: FFN gate silu
operator_type: Silu
"];
	1974 [label="credit: 1974
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1975 [label="credit: 1975
name: FFN par
operator_type: Mul
"];
	1976 [label="credit: 1976
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	1977 [label="credit: 1977
name: FFN down
operator_type: MatMul
"];
	1978 [label="credit: 1978
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1979 [label="credit: 1979
name: Logit out
operator_type: Add
"];
	1980 [label="credit: 1980
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1981 [label="credit: 1981
name: attn mask
operator_type: NormRMS
"];
	1982 [label="credit: 1982
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1983 [label="credit: 1983
name: attn norm
operator_type: Mul
"];
	1984 [label="credit: 1984
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	1985 [label="credit: 1985
name: Qcur
operator_type: MatMul
"];
	1986 [label="credit: 1986
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1987 [label="credit: 1987
name: Kcur
operator_type: MatMul
"];
	1988 [label="credit: 1988
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1989 [label="credit: 1989
name: Vcur
operator_type: MatMul
"];
	1990 [label="credit: 1990
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	1991 [label="credit: 1991
name: Qcur - reshaped
operator_type: Reshape
"];
	1992 [label="credit: 1992
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1993 [label="credit: 1993
name: Kcur - reshaped
operator_type: Reshape
"];
	1994 [label="credit: 1994
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1995 [label="credit: 1995
name: Qcur - rope
operator_type: Rope
"];
	1996 [label="credit: 1996
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1997 [label="credit: 1997
name: Kcur - rope
operator_type: Rope
"];
	1998 [label="credit: 1998
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	1999 [label="credit: 1999
name: Q - out
operator_type: Permute
"];
	2000 [label="credit: 2000
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2001 [label="credit: 2001
name: K - out
operator_type: View
"];
	2002 [label="credit: 2002
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2003 [label="credit: 2003
name: Attention Score
operator_type: MatMul
"];
	2004 [label="credit: 2004
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2005 [label="credit: 2005
name: Attention Context
operator_type: Softmax
"];
	2006 [label="credit: 2006
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2007 [label="credit: 2007
name: V - transpose
operator_type: Transpose
"];
	2008 [label="credit: 2008
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2009 [label="credit: 2009
name: V - out
operator_type: View
"];
	2010 [label="credit: 2010
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2011 [label="credit: 2011
name: K - Q - V
operator_type: MatMul
"];
	2012 [label="credit: 2012
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2013 [label="credit: 2013
name: K - Q - V merged
operator_type: Permute
"];
	2014 [label="credit: 2014
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2015 [label="credit: 2015
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2016 [label="credit: 2016
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2017 [label="credit: 2017
name: K - Q - V weight
operator_type: MatMul
"];
	2018 [label="credit: 2018
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2019 [label="credit: 2019
name: FFN input
operator_type: Add
"];
	2020 [label="credit: 2020
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2021 [label="credit: 2021
name: FFN - norm
operator_type: NormRMS
"];
	2022 [label="credit: 2022
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2023 [label="credit: 2023
name: FFN - norm
operator_type: Mul
"];
	2024 [label="credit: 2024
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2025 [label="credit: 2025
name: FFN up
operator_type: MatMul
"];
	2026 [label="credit: 2026
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2027 [label="credit: 2027
name: FFN gate
operator_type: MatMul
"];
	2028 [label="credit: 2028
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2029 [label="credit: 2029
name: FFN gate silu
operator_type: Silu
"];
	2030 [label="credit: 2030
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2031 [label="credit: 2031
name: FFN par
operator_type: Mul
"];
	2032 [label="credit: 2032
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2033 [label="credit: 2033
name: FFN down
operator_type: MatMul
"];
	2034 [label="credit: 2034
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2035 [label="credit: 2035
name: Logit out
operator_type: Add
"];
	2036 [label="credit: 2036
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2037 [label="credit: 2037
name: attn mask
operator_type: NormRMS
"];
	2038 [label="credit: 2038
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2039 [label="credit: 2039
name: attn norm
operator_type: Mul
"];
	2040 [label="credit: 2040
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2041 [label="credit: 2041
name: Qcur
operator_type: MatMul
"];
	2042 [label="credit: 2042
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2043 [label="credit: 2043
name: Kcur
operator_type: MatMul
"];
	2044 [label="credit: 2044
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2045 [label="credit: 2045
name: Vcur
operator_type: MatMul
"];
	2046 [label="credit: 2046
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2047 [label="credit: 2047
name: Qcur - reshaped
operator_type: Reshape
"];
	2048 [label="credit: 2048
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2049 [label="credit: 2049
name: Kcur - reshaped
operator_type: Reshape
"];
	2050 [label="credit: 2050
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2051 [label="credit: 2051
name: Qcur - rope
operator_type: Rope
"];
	2052 [label="credit: 2052
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2053 [label="credit: 2053
name: Kcur - rope
operator_type: Rope
"];
	2054 [label="credit: 2054
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2055 [label="credit: 2055
name: Q - out
operator_type: Permute
"];
	2056 [label="credit: 2056
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2057 [label="credit: 2057
name: K - out
operator_type: View
"];
	2058 [label="credit: 2058
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2059 [label="credit: 2059
name: Attention Score
operator_type: MatMul
"];
	2060 [label="credit: 2060
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2061 [label="credit: 2061
name: Attention Context
operator_type: Softmax
"];
	2062 [label="credit: 2062
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2063 [label="credit: 2063
name: V - transpose
operator_type: Transpose
"];
	2064 [label="credit: 2064
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2065 [label="credit: 2065
name: V - out
operator_type: View
"];
	2066 [label="credit: 2066
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2067 [label="credit: 2067
name: K - Q - V
operator_type: MatMul
"];
	2068 [label="credit: 2068
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2069 [label="credit: 2069
name: K - Q - V merged
operator_type: Permute
"];
	2070 [label="credit: 2070
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2071 [label="credit: 2071
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2072 [label="credit: 2072
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2073 [label="credit: 2073
name: K - Q - V weight
operator_type: MatMul
"];
	2074 [label="credit: 2074
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2075 [label="credit: 2075
name: FFN input
operator_type: Add
"];
	2076 [label="credit: 2076
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2077 [label="credit: 2077
name: FFN - norm
operator_type: NormRMS
"];
	2078 [label="credit: 2078
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2079 [label="credit: 2079
name: FFN - norm
operator_type: Mul
"];
	2080 [label="credit: 2080
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2081 [label="credit: 2081
name: FFN up
operator_type: MatMul
"];
	2082 [label="credit: 2082
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2083 [label="credit: 2083
name: FFN gate
operator_type: MatMul
"];
	2084 [label="credit: 2084
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2085 [label="credit: 2085
name: FFN gate silu
operator_type: Silu
"];
	2086 [label="credit: 2086
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2087 [label="credit: 2087
name: FFN par
operator_type: Mul
"];
	2088 [label="credit: 2088
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2089 [label="credit: 2089
name: FFN down
operator_type: MatMul
"];
	2090 [label="credit: 2090
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2091 [label="credit: 2091
name: Logit out
operator_type: Add
"];
	2092 [label="credit: 2092
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2093 [label="credit: 2093
name: attn mask
operator_type: NormRMS
"];
	2094 [label="credit: 2094
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2095 [label="credit: 2095
name: attn norm
operator_type: Mul
"];
	2096 [label="credit: 2096
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2097 [label="credit: 2097
name: Qcur
operator_type: MatMul
"];
	2098 [label="credit: 2098
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2099 [label="credit: 2099
name: Kcur
operator_type: MatMul
"];
	2100 [label="credit: 2100
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2101 [label="credit: 2101
name: Vcur
operator_type: MatMul
"];
	2102 [label="credit: 2102
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2103 [label="credit: 2103
name: Qcur - reshaped
operator_type: Reshape
"];
	2104 [label="credit: 2104
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2105 [label="credit: 2105
name: Kcur - reshaped
operator_type: Reshape
"];
	2106 [label="credit: 2106
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2107 [label="credit: 2107
name: Qcur - rope
operator_type: Rope
"];
	2108 [label="credit: 2108
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2109 [label="credit: 2109
name: Kcur - rope
operator_type: Rope
"];
	2110 [label="credit: 2110
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2111 [label="credit: 2111
name: Q - out
operator_type: Permute
"];
	2112 [label="credit: 2112
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2113 [label="credit: 2113
name: K - out
operator_type: View
"];
	2114 [label="credit: 2114
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2115 [label="credit: 2115
name: Attention Score
operator_type: MatMul
"];
	2116 [label="credit: 2116
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2117 [label="credit: 2117
name: Attention Context
operator_type: Softmax
"];
	2118 [label="credit: 2118
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2119 [label="credit: 2119
name: V - transpose
operator_type: Transpose
"];
	2120 [label="credit: 2120
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2121 [label="credit: 2121
name: V - out
operator_type: View
"];
	2122 [label="credit: 2122
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2123 [label="credit: 2123
name: K - Q - V
operator_type: MatMul
"];
	2124 [label="credit: 2124
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2125 [label="credit: 2125
name: K - Q - V merged
operator_type: Permute
"];
	2126 [label="credit: 2126
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2127 [label="credit: 2127
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2128 [label="credit: 2128
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2129 [label="credit: 2129
name: K - Q - V weight
operator_type: MatMul
"];
	2130 [label="credit: 2130
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2131 [label="credit: 2131
name: FFN input
operator_type: Add
"];
	2132 [label="credit: 2132
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2133 [label="credit: 2133
name: FFN - norm
operator_type: NormRMS
"];
	2134 [label="credit: 2134
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2135 [label="credit: 2135
name: FFN - norm
operator_type: Mul
"];
	2136 [label="credit: 2136
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2137 [label="credit: 2137
name: FFN up
operator_type: MatMul
"];
	2138 [label="credit: 2138
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2139 [label="credit: 2139
name: FFN gate
operator_type: MatMul
"];
	2140 [label="credit: 2140
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2141 [label="credit: 2141
name: FFN gate silu
operator_type: Silu
"];
	2142 [label="credit: 2142
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2143 [label="credit: 2143
name: FFN par
operator_type: Mul
"];
	2144 [label="credit: 2144
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2145 [label="credit: 2145
name: FFN down
operator_type: MatMul
"];
	2146 [label="credit: 2146
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2147 [label="credit: 2147
name: Logit out
operator_type: Add
"];
	2148 [label="credit: 2148
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2149 [label="credit: 2149
name: attn mask
operator_type: NormRMS
"];
	2150 [label="credit: 2150
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2151 [label="credit: 2151
name: attn norm
operator_type: Mul
"];
	2152 [label="credit: 2152
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2153 [label="credit: 2153
name: Qcur
operator_type: MatMul
"];
	2154 [label="credit: 2154
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2155 [label="credit: 2155
name: Kcur
operator_type: MatMul
"];
	2156 [label="credit: 2156
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2157 [label="credit: 2157
name: Vcur
operator_type: MatMul
"];
	2158 [label="credit: 2158
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2159 [label="credit: 2159
name: Qcur - reshaped
operator_type: Reshape
"];
	2160 [label="credit: 2160
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2161 [label="credit: 2161
name: Kcur - reshaped
operator_type: Reshape
"];
	2162 [label="credit: 2162
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2163 [label="credit: 2163
name: Qcur - rope
operator_type: Rope
"];
	2164 [label="credit: 2164
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2165 [label="credit: 2165
name: Kcur - rope
operator_type: Rope
"];
	2166 [label="credit: 2166
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2167 [label="credit: 2167
name: Q - out
operator_type: Permute
"];
	2168 [label="credit: 2168
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2169 [label="credit: 2169
name: K - out
operator_type: View
"];
	2170 [label="credit: 2170
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2171 [label="credit: 2171
name: Attention Score
operator_type: MatMul
"];
	2172 [label="credit: 2172
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2173 [label="credit: 2173
name: Attention Context
operator_type: Softmax
"];
	2174 [label="credit: 2174
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2175 [label="credit: 2175
name: V - transpose
operator_type: Transpose
"];
	2176 [label="credit: 2176
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2177 [label="credit: 2177
name: V - out
operator_type: View
"];
	2178 [label="credit: 2178
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2179 [label="credit: 2179
name: K - Q - V
operator_type: MatMul
"];
	2180 [label="credit: 2180
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2181 [label="credit: 2181
name: K - Q - V merged
operator_type: Permute
"];
	2182 [label="credit: 2182
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2183 [label="credit: 2183
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2184 [label="credit: 2184
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2185 [label="credit: 2185
name: K - Q - V weight
operator_type: MatMul
"];
	2186 [label="credit: 2186
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2187 [label="credit: 2187
name: FFN input
operator_type: Add
"];
	2188 [label="credit: 2188
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2189 [label="credit: 2189
name: FFN - norm
operator_type: NormRMS
"];
	2190 [label="credit: 2190
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2191 [label="credit: 2191
name: FFN - norm
operator_type: Mul
"];
	2192 [label="credit: 2192
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2193 [label="credit: 2193
name: FFN up
operator_type: MatMul
"];
	2194 [label="credit: 2194
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2195 [label="credit: 2195
name: FFN gate
operator_type: MatMul
"];
	2196 [label="credit: 2196
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2197 [label="credit: 2197
name: FFN gate silu
operator_type: Silu
"];
	2198 [label="credit: 2198
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2199 [label="credit: 2199
name: FFN par
operator_type: Mul
"];
	2200 [label="credit: 2200
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2201 [label="credit: 2201
name: FFN down
operator_type: MatMul
"];
	2202 [label="credit: 2202
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2203 [label="credit: 2203
name: Logit out
operator_type: Add
"];
	2204 [label="credit: 2204
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2205 [label="credit: 2205
name: attn mask
operator_type: NormRMS
"];
	2206 [label="credit: 2206
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2207 [label="credit: 2207
name: attn norm
operator_type: Mul
"];
	2208 [label="credit: 2208
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2209 [label="credit: 2209
name: Qcur
operator_type: MatMul
"];
	2210 [label="credit: 2210
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2211 [label="credit: 2211
name: Kcur
operator_type: MatMul
"];
	2212 [label="credit: 2212
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2213 [label="credit: 2213
name: Vcur
operator_type: MatMul
"];
	2214 [label="credit: 2214
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2215 [label="credit: 2215
name: Qcur - reshaped
operator_type: Reshape
"];
	2216 [label="credit: 2216
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2217 [label="credit: 2217
name: Kcur - reshaped
operator_type: Reshape
"];
	2218 [label="credit: 2218
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2219 [label="credit: 2219
name: Qcur - rope
operator_type: Rope
"];
	2220 [label="credit: 2220
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2221 [label="credit: 2221
name: Kcur - rope
operator_type: Rope
"];
	2222 [label="credit: 2222
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2223 [label="credit: 2223
name: Q - out
operator_type: Permute
"];
	2224 [label="credit: 2224
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2225 [label="credit: 2225
name: K - out
operator_type: View
"];
	2226 [label="credit: 2226
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2227 [label="credit: 2227
name: Attention Score
operator_type: MatMul
"];
	2228 [label="credit: 2228
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2229 [label="credit: 2229
name: Attention Context
operator_type: Softmax
"];
	2230 [label="credit: 2230
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2231 [label="credit: 2231
name: V - transpose
operator_type: Transpose
"];
	2232 [label="credit: 2232
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2233 [label="credit: 2233
name: V - out
operator_type: View
"];
	2234 [label="credit: 2234
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2235 [label="credit: 2235
name: K - Q - V
operator_type: MatMul
"];
	2236 [label="credit: 2236
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2237 [label="credit: 2237
name: K - Q - V merged
operator_type: Permute
"];
	2238 [label="credit: 2238
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2239 [label="credit: 2239
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2240 [label="credit: 2240
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2241 [label="credit: 2241
name: K - Q - V weight
operator_type: MatMul
"];
	2242 [label="credit: 2242
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2243 [label="credit: 2243
name: FFN input
operator_type: Add
"];
	2244 [label="credit: 2244
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2245 [label="credit: 2245
name: FFN - norm
operator_type: NormRMS
"];
	2246 [label="credit: 2246
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2247 [label="credit: 2247
name: FFN - norm
operator_type: Mul
"];
	2248 [label="credit: 2248
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2249 [label="credit: 2249
name: FFN up
operator_type: MatMul
"];
	2250 [label="credit: 2250
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2251 [label="credit: 2251
name: FFN gate
operator_type: MatMul
"];
	2252 [label="credit: 2252
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2253 [label="credit: 2253
name: FFN gate silu
operator_type: Silu
"];
	2254 [label="credit: 2254
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2255 [label="credit: 2255
name: FFN par
operator_type: Mul
"];
	2256 [label="credit: 2256
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2257 [label="credit: 2257
name: FFN down
operator_type: MatMul
"];
	2258 [label="credit: 2258
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2259 [label="credit: 2259
name: Logit out
operator_type: Add
"];
	2260 [label="credit: 2260
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2261 [label="credit: 2261
name: attn mask
operator_type: NormRMS
"];
	2262 [label="credit: 2262
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2263 [label="credit: 2263
name: attn norm
operator_type: Mul
"];
	2264 [label="credit: 2264
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2265 [label="credit: 2265
name: Qcur
operator_type: MatMul
"];
	2266 [label="credit: 2266
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2267 [label="credit: 2267
name: Kcur
operator_type: MatMul
"];
	2268 [label="credit: 2268
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2269 [label="credit: 2269
name: Vcur
operator_type: MatMul
"];
	2270 [label="credit: 2270
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2271 [label="credit: 2271
name: Qcur - reshaped
operator_type: Reshape
"];
	2272 [label="credit: 2272
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2273 [label="credit: 2273
name: Kcur - reshaped
operator_type: Reshape
"];
	2274 [label="credit: 2274
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2275 [label="credit: 2275
name: Qcur - rope
operator_type: Rope
"];
	2276 [label="credit: 2276
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2277 [label="credit: 2277
name: Kcur - rope
operator_type: Rope
"];
	2278 [label="credit: 2278
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2279 [label="credit: 2279
name: Q - out
operator_type: Permute
"];
	2280 [label="credit: 2280
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2281 [label="credit: 2281
name: K - out
operator_type: View
"];
	2282 [label="credit: 2282
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2283 [label="credit: 2283
name: Attention Score
operator_type: MatMul
"];
	2284 [label="credit: 2284
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2285 [label="credit: 2285
name: Attention Context
operator_type: Softmax
"];
	2286 [label="credit: 2286
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2287 [label="credit: 2287
name: V - transpose
operator_type: Transpose
"];
	2288 [label="credit: 2288
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2289 [label="credit: 2289
name: V - out
operator_type: View
"];
	2290 [label="credit: 2290
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2291 [label="credit: 2291
name: K - Q - V
operator_type: MatMul
"];
	2292 [label="credit: 2292
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2293 [label="credit: 2293
name: K - Q - V merged
operator_type: Permute
"];
	2294 [label="credit: 2294
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2295 [label="credit: 2295
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2296 [label="credit: 2296
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2297 [label="credit: 2297
name: K - Q - V weight
operator_type: MatMul
"];
	2298 [label="credit: 2298
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2299 [label="credit: 2299
name: FFN input
operator_type: Add
"];
	2300 [label="credit: 2300
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2301 [label="credit: 2301
name: FFN - norm
operator_type: NormRMS
"];
	2302 [label="credit: 2302
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2303 [label="credit: 2303
name: FFN - norm
operator_type: Mul
"];
	2304 [label="credit: 2304
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2305 [label="credit: 2305
name: FFN up
operator_type: MatMul
"];
	2306 [label="credit: 2306
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2307 [label="credit: 2307
name: FFN gate
operator_type: MatMul
"];
	2308 [label="credit: 2308
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2309 [label="credit: 2309
name: FFN gate silu
operator_type: Silu
"];
	2310 [label="credit: 2310
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2311 [label="credit: 2311
name: FFN par
operator_type: Mul
"];
	2312 [label="credit: 2312
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2313 [label="credit: 2313
name: FFN down
operator_type: MatMul
"];
	2314 [label="credit: 2314
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2315 [label="credit: 2315
name: Logit out
operator_type: Add
"];
	2316 [label="credit: 2316
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2317 [label="credit: 2317
name: attn mask
operator_type: NormRMS
"];
	2318 [label="credit: 2318
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2319 [label="credit: 2319
name: attn norm
operator_type: Mul
"];
	2320 [label="credit: 2320
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2321 [label="credit: 2321
name: Qcur
operator_type: MatMul
"];
	2322 [label="credit: 2322
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2323 [label="credit: 2323
name: Kcur
operator_type: MatMul
"];
	2324 [label="credit: 2324
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2325 [label="credit: 2325
name: Vcur
operator_type: MatMul
"];
	2326 [label="credit: 2326
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2327 [label="credit: 2327
name: Qcur - reshaped
operator_type: Reshape
"];
	2328 [label="credit: 2328
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2329 [label="credit: 2329
name: Kcur - reshaped
operator_type: Reshape
"];
	2330 [label="credit: 2330
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2331 [label="credit: 2331
name: Qcur - rope
operator_type: Rope
"];
	2332 [label="credit: 2332
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2333 [label="credit: 2333
name: Kcur - rope
operator_type: Rope
"];
	2334 [label="credit: 2334
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2335 [label="credit: 2335
name: Q - out
operator_type: Permute
"];
	2336 [label="credit: 2336
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2337 [label="credit: 2337
name: K - out
operator_type: View
"];
	2338 [label="credit: 2338
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2339 [label="credit: 2339
name: Attention Score
operator_type: MatMul
"];
	2340 [label="credit: 2340
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2341 [label="credit: 2341
name: Attention Context
operator_type: Softmax
"];
	2342 [label="credit: 2342
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2343 [label="credit: 2343
name: V - transpose
operator_type: Transpose
"];
	2344 [label="credit: 2344
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2345 [label="credit: 2345
name: V - out
operator_type: View
"];
	2346 [label="credit: 2346
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2347 [label="credit: 2347
name: K - Q - V
operator_type: MatMul
"];
	2348 [label="credit: 2348
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2349 [label="credit: 2349
name: K - Q - V merged
operator_type: Permute
"];
	2350 [label="credit: 2350
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2351 [label="credit: 2351
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2352 [label="credit: 2352
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2353 [label="credit: 2353
name: K - Q - V weight
operator_type: MatMul
"];
	2354 [label="credit: 2354
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2355 [label="credit: 2355
name: FFN input
operator_type: Add
"];
	2356 [label="credit: 2356
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2357 [label="credit: 2357
name: FFN - norm
operator_type: NormRMS
"];
	2358 [label="credit: 2358
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2359 [label="credit: 2359
name: FFN - norm
operator_type: Mul
"];
	2360 [label="credit: 2360
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2361 [label="credit: 2361
name: FFN up
operator_type: MatMul
"];
	2362 [label="credit: 2362
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2363 [label="credit: 2363
name: FFN gate
operator_type: MatMul
"];
	2364 [label="credit: 2364
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2365 [label="credit: 2365
name: FFN gate silu
operator_type: Silu
"];
	2366 [label="credit: 2366
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2367 [label="credit: 2367
name: FFN par
operator_type: Mul
"];
	2368 [label="credit: 2368
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2369 [label="credit: 2369
name: FFN down
operator_type: MatMul
"];
	2370 [label="credit: 2370
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2371 [label="credit: 2371
name: Logit out
operator_type: Add
"];
	2372 [label="credit: 2372
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2373 [label="credit: 2373
name: attn mask
operator_type: NormRMS
"];
	2374 [label="credit: 2374
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2375 [label="credit: 2375
name: attn norm
operator_type: Mul
"];
	2376 [label="credit: 2376
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2377 [label="credit: 2377
name: Qcur
operator_type: MatMul
"];
	2378 [label="credit: 2378
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2379 [label="credit: 2379
name: Kcur
operator_type: MatMul
"];
	2380 [label="credit: 2380
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2381 [label="credit: 2381
name: Vcur
operator_type: MatMul
"];
	2382 [label="credit: 2382
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2383 [label="credit: 2383
name: Qcur - reshaped
operator_type: Reshape
"];
	2384 [label="credit: 2384
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2385 [label="credit: 2385
name: Kcur - reshaped
operator_type: Reshape
"];
	2386 [label="credit: 2386
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2387 [label="credit: 2387
name: Qcur - rope
operator_type: Rope
"];
	2388 [label="credit: 2388
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2389 [label="credit: 2389
name: Kcur - rope
operator_type: Rope
"];
	2390 [label="credit: 2390
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2391 [label="credit: 2391
name: Q - out
operator_type: Permute
"];
	2392 [label="credit: 2392
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2393 [label="credit: 2393
name: K - out
operator_type: View
"];
	2394 [label="credit: 2394
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2395 [label="credit: 2395
name: Attention Score
operator_type: MatMul
"];
	2396 [label="credit: 2396
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2397 [label="credit: 2397
name: Attention Context
operator_type: Softmax
"];
	2398 [label="credit: 2398
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2399 [label="credit: 2399
name: V - transpose
operator_type: Transpose
"];
	2400 [label="credit: 2400
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2401 [label="credit: 2401
name: V - out
operator_type: View
"];
	2402 [label="credit: 2402
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2403 [label="credit: 2403
name: K - Q - V
operator_type: MatMul
"];
	2404 [label="credit: 2404
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2405 [label="credit: 2405
name: K - Q - V merged
operator_type: Permute
"];
	2406 [label="credit: 2406
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2407 [label="credit: 2407
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2408 [label="credit: 2408
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2409 [label="credit: 2409
name: K - Q - V weight
operator_type: MatMul
"];
	2410 [label="credit: 2410
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2411 [label="credit: 2411
name: FFN input
operator_type: Add
"];
	2412 [label="credit: 2412
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2413 [label="credit: 2413
name: FFN - norm
operator_type: NormRMS
"];
	2414 [label="credit: 2414
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2415 [label="credit: 2415
name: FFN - norm
operator_type: Mul
"];
	2416 [label="credit: 2416
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2417 [label="credit: 2417
name: FFN up
operator_type: MatMul
"];
	2418 [label="credit: 2418
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2419 [label="credit: 2419
name: FFN gate
operator_type: MatMul
"];
	2420 [label="credit: 2420
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2421 [label="credit: 2421
name: FFN gate silu
operator_type: Silu
"];
	2422 [label="credit: 2422
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2423 [label="credit: 2423
name: FFN par
operator_type: Mul
"];
	2424 [label="credit: 2424
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2425 [label="credit: 2425
name: FFN down
operator_type: MatMul
"];
	2426 [label="credit: 2426
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2427 [label="credit: 2427
name: Logit out
operator_type: Add
"];
	2428 [label="credit: 2428
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2429 [label="credit: 2429
name: attn mask
operator_type: NormRMS
"];
	2430 [label="credit: 2430
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2431 [label="credit: 2431
name: attn norm
operator_type: Mul
"];
	2432 [label="credit: 2432
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2433 [label="credit: 2433
name: Qcur
operator_type: MatMul
"];
	2434 [label="credit: 2434
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2435 [label="credit: 2435
name: Kcur
operator_type: MatMul
"];
	2436 [label="credit: 2436
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2437 [label="credit: 2437
name: Vcur
operator_type: MatMul
"];
	2438 [label="credit: 2438
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2439 [label="credit: 2439
name: Qcur - reshaped
operator_type: Reshape
"];
	2440 [label="credit: 2440
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2441 [label="credit: 2441
name: Kcur - reshaped
operator_type: Reshape
"];
	2442 [label="credit: 2442
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2443 [label="credit: 2443
name: Qcur - rope
operator_type: Rope
"];
	2444 [label="credit: 2444
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2445 [label="credit: 2445
name: Kcur - rope
operator_type: Rope
"];
	2446 [label="credit: 2446
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2447 [label="credit: 2447
name: Q - out
operator_type: Permute
"];
	2448 [label="credit: 2448
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2449 [label="credit: 2449
name: K - out
operator_type: View
"];
	2450 [label="credit: 2450
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2451 [label="credit: 2451
name: Attention Score
operator_type: MatMul
"];
	2452 [label="credit: 2452
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2453 [label="credit: 2453
name: Attention Context
operator_type: Softmax
"];
	2454 [label="credit: 2454
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2455 [label="credit: 2455
name: V - transpose
operator_type: Transpose
"];
	2456 [label="credit: 2456
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2457 [label="credit: 2457
name: V - out
operator_type: View
"];
	2458 [label="credit: 2458
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2459 [label="credit: 2459
name: K - Q - V
operator_type: MatMul
"];
	2460 [label="credit: 2460
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2461 [label="credit: 2461
name: K - Q - V merged
operator_type: Permute
"];
	2462 [label="credit: 2462
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2463 [label="credit: 2463
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2464 [label="credit: 2464
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2465 [label="credit: 2465
name: K - Q - V weight
operator_type: MatMul
"];
	2466 [label="credit: 2466
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2467 [label="credit: 2467
name: FFN input
operator_type: Add
"];
	2468 [label="credit: 2468
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2469 [label="credit: 2469
name: FFN - norm
operator_type: NormRMS
"];
	2470 [label="credit: 2470
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2471 [label="credit: 2471
name: FFN - norm
operator_type: Mul
"];
	2472 [label="credit: 2472
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2473 [label="credit: 2473
name: FFN up
operator_type: MatMul
"];
	2474 [label="credit: 2474
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2475 [label="credit: 2475
name: FFN gate
operator_type: MatMul
"];
	2476 [label="credit: 2476
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2477 [label="credit: 2477
name: FFN gate silu
operator_type: Silu
"];
	2478 [label="credit: 2478
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2479 [label="credit: 2479
name: FFN par
operator_type: Mul
"];
	2480 [label="credit: 2480
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2481 [label="credit: 2481
name: FFN down
operator_type: MatMul
"];
	2482 [label="credit: 2482
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2483 [label="credit: 2483
name: Logit out
operator_type: Add
"];
	2484 [label="credit: 2484
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2485 [label="credit: 2485
name: attn mask
operator_type: NormRMS
"];
	2486 [label="credit: 2486
data type: FP32
name: attn mask- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2487 [label="credit: 2487
name: attn norm
operator_type: Mul
"];
	2488 [label="credit: 2488
data type: FP32
name: attn norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2489 [label="credit: 2489
name: Qcur
operator_type: MatMul
"];
	2490 [label="credit: 2490
data type: FP32
name: Qcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2491 [label="credit: 2491
name: Kcur
operator_type: MatMul
"];
	2492 [label="credit: 2492
data type: FP32
name: Kcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2493 [label="credit: 2493
name: Vcur
operator_type: MatMul
"];
	2494 [label="credit: 2494
data type: FP32
name: Vcur- out
shape: (10, 4096) (4, 40)
size: 163840
"];
	2495 [label="credit: 2495
name: Qcur - reshaped
operator_type: Reshape
"];
	2496 [label="credit: 2496
data type: FP32
name: Qcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2497 [label="credit: 2497
name: Kcur - reshaped
operator_type: Reshape
"];
	2498 [label="credit: 2498
data type: FP32
name: Kcur - reshaped- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2499 [label="credit: 2499
name: Qcur - rope
operator_type: Rope
"];
	2500 [label="credit: 2500
data type: FP32
name: Qcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2501 [label="credit: 2501
name: Kcur - rope
operator_type: Rope
"];
	2502 [label="credit: 2502
data type: FP32
name: Kcur - rope- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2503 [label="credit: 2503
name: Q - out
operator_type: Permute
"];
	2504 [label="credit: 2504
data type: FP32
name: Q - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2505 [label="credit: 2505
name: K - out
operator_type: View
"];
	2506 [label="credit: 2506
data type: FP32
name: K - out- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2507 [label="credit: 2507
name: Attention Score
operator_type: MatMul
"];
	2508 [label="credit: 2508
data type: FP32
name: Attention Score- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2509 [label="credit: 2509
name: Attention Context
operator_type: Softmax
"];
	2510 [label="credit: 2510
data type: FP32
name: Attention Context- out
shape: (10, 10, 32) (4, 40, 400)
size: 12800
"];
	2511 [label="credit: 2511
name: V - transpose
operator_type: Transpose
"];
	2512 [label="credit: 2512
data type: FP32
name: V - transpose- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2513 [label="credit: 2513
name: V - out
operator_type: View
"];
	2514 [label="credit: 2514
data type: FP32
name: V - out- out
shape: (10, 128, 32) (4, 40, 5120)
size: 163840
"];
	2515 [label="credit: 2515
name: K - Q - V
operator_type: MatMul
"];
	2516 [label="credit: 2516
data type: FP32
name: K - Q - V- out
shape: (128, 10, 32) (4, 512, 5120)
size: 163840
"];
	2517 [label="credit: 2517
name: K - Q - V merged
operator_type: Permute
"];
	2518 [label="credit: 2518
data type: FP32
name: K - Q - V merged- out
shape: (128, 32, 10) (4, 512, 16384)
size: 163840
"];
	2519 [label="credit: 2519
name: K - Q - V merged contiguous
operator_type: Contiguous
"];
	2520 [label="credit: 2520
data type: FP32
name: K - Q - V merged contiguous- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2521 [label="credit: 2521
name: K - Q - V weight
operator_type: MatMul
"];
	2522 [label="credit: 2522
data type: FP32
name: K - Q - V weight- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2523 [label="credit: 2523
name: FFN input
operator_type: Add
"];
	2524 [label="credit: 2524
data type: FP32
name: FFN input- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2525 [label="credit: 2525
name: FFN - norm
operator_type: NormRMS
"];
	2526 [label="credit: 2526
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2527 [label="credit: 2527
name: FFN - norm
operator_type: Mul
"];
	2528 [label="credit: 2528
data type: FP32
name: FFN - norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2529 [label="credit: 2529
name: FFN up
operator_type: MatMul
"];
	2530 [label="credit: 2530
data type: FP32
name: FFN up- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2531 [label="credit: 2531
name: FFN gate
operator_type: MatMul
"];
	2532 [label="credit: 2532
data type: FP32
name: FFN gate- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2533 [label="credit: 2533
name: FFN gate silu
operator_type: Silu
"];
	2534 [label="credit: 2534
data type: FP32
name: FFN gate silu- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2535 [label="credit: 2535
name: FFN par
operator_type: Mul
"];
	2536 [label="credit: 2536
data type: FP32
name: FFN par- out
shape: (11008, 10) (4, 44032)
size: 440320
"];
	2537 [label="credit: 2537
name: FFN down
operator_type: MatMul
"];
	2538 [label="credit: 2538
data type: FP32
name: FFN down- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2539 [label="credit: 2539
name: Logit out
operator_type: Add
"];
	2540 [label="credit: 2540
data type: FP32
name: Logit out- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2541 [label="credit: 2541
name: result norm buffer
operator_type: NormRMS
"];
	2542 [label="credit: 2542
data type: FP32
name: result norm buffer- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2543 [label="credit: 2543
name: result norm
operator_type: Mul
"];
	2544 [label="credit: 2544
data type: FP32
name: result norm- out
shape: (4096, 10) (4, 16384)
size: 163840
"];
	2545 [label="credit: 2545
name: result output
operator_type: MatMul
"];
	2546 [label="credit: 2546
data type: FP32
name: result output- out
shape: (32000, 10) (4, 128000)
size: 1280000
"];

	0 -> 2
	0 -> 5
	0 -> 6
	0 -> 9
	0 -> 10
	0 -> 13
	0 -> 16
	0 -> 19
	0 -> 22
	0 -> 23
	0 -> 26
	0 -> 29
	0 -> 32
	0 -> 33
	0 -> 36
	0 -> 39
	0 -> 42
	0 -> 45
	0 -> 46
	0 -> 49
	0 -> 52
	0 -> 55
	0 -> 56
	0 -> 59
	0 -> 62
	0 -> 65
	0 -> 68
	0 -> 69
	0 -> 72
	0 -> 75
	0 -> 78
	0 -> 79
	0 -> 82
	0 -> 85
	0 -> 88
	0 -> 91
	0 -> 92
	0 -> 95
	0 -> 98
	0 -> 101
	0 -> 102
	0 -> 105
	0 -> 108
	0 -> 111
	0 -> 114
	0 -> 115
	0 -> 118
	0 -> 121
	0 -> 124
	0 -> 125
	0 -> 128
	0 -> 131
	0 -> 134
	0 -> 137
	0 -> 138
	0 -> 141
	0 -> 144
	0 -> 147
	0 -> 148
	0 -> 151
	0 -> 154
	0 -> 157
	0 -> 160
	0 -> 161
	0 -> 164
	0 -> 167
	0 -> 170
	0 -> 171
	0 -> 174
	0 -> 177
	0 -> 180
	0 -> 183
	0 -> 184
	0 -> 187
	0 -> 190
	0 -> 193
	0 -> 194
	0 -> 197
	0 -> 200
	0 -> 203
	0 -> 206
	0 -> 207
	0 -> 210
	0 -> 213
	0 -> 216
	0 -> 217
	0 -> 220
	0 -> 223
	0 -> 226
	0 -> 229
	0 -> 230
	0 -> 233
	0 -> 236
	0 -> 239
	0 -> 240
	0 -> 243
	0 -> 246
	0 -> 249
	0 -> 252
	0 -> 253
	0 -> 256
	0 -> 259
	0 -> 262
	0 -> 263
	0 -> 266
	0 -> 269
	0 -> 272
	0 -> 275
	0 -> 276
	0 -> 279
	0 -> 282
	0 -> 285
	0 -> 286
	0 -> 289
	0 -> 292
	0 -> 295
	0 -> 298
	0 -> 299
	0 -> 302
	0 -> 305
	0 -> 308
	0 -> 309
	0 -> 312
	0 -> 315
	0 -> 318
	0 -> 321
	0 -> 322
	0 -> 325
	0 -> 328
	0 -> 331
	0 -> 332
	0 -> 335
	0 -> 338
	0 -> 341
	0 -> 344
	0 -> 345
	0 -> 348
	0 -> 351
	0 -> 354
	0 -> 355
	0 -> 358
	0 -> 361
	0 -> 364
	0 -> 367
	0 -> 368
	0 -> 371
	0 -> 374
	0 -> 377
	0 -> 378
	0 -> 381
	0 -> 384
	0 -> 387
	0 -> 390
	0 -> 391
	0 -> 394
	0 -> 397
	0 -> 400
	0 -> 401
	0 -> 404
	0 -> 407
	0 -> 410
	0 -> 413
	0 -> 414
	0 -> 417
	0 -> 420
	0 -> 423
	0 -> 424
	0 -> 427
	0 -> 430
	0 -> 433
	0 -> 436
	0 -> 437
	0 -> 440
	0 -> 443
	0 -> 446
	0 -> 447
	0 -> 450
	0 -> 453
	0 -> 456
	0 -> 459
	0 -> 460
	0 -> 463
	0 -> 466
	0 -> 469
	0 -> 470
	0 -> 473
	0 -> 476
	0 -> 479
	0 -> 482
	0 -> 483
	0 -> 486
	0 -> 489
	0 -> 492
	0 -> 493
	0 -> 496
	0 -> 499
	0 -> 502
	0 -> 505
	0 -> 506
	0 -> 509
	0 -> 512
	0 -> 515
	0 -> 516
	0 -> 519
	0 -> 522
	0 -> 525
	0 -> 528
	0 -> 529
	0 -> 532
	0 -> 535
	0 -> 538
	0 -> 539
	0 -> 542
	0 -> 545
	0 -> 548
	0 -> 551
	0 -> 552
	0 -> 555
	0 -> 558
	0 -> 561
	0 -> 562
	0 -> 565
	0 -> 568
	0 -> 571
	0 -> 574
	0 -> 575
	0 -> 578
	0 -> 581
	0 -> 584
	0 -> 585
	0 -> 588
	0 -> 591
	0 -> 594
	0 -> 597
	0 -> 598
	0 -> 601
	0 -> 604
	0 -> 607
	0 -> 608
	0 -> 611
	0 -> 614
	0 -> 617
	0 -> 620
	0 -> 621
	0 -> 624
	0 -> 627
	0 -> 630
	0 -> 631
	0 -> 634
	0 -> 637
	0 -> 640
	0 -> 643
	0 -> 644
	0 -> 647
	0 -> 650
	0 -> 653
	0 -> 654
	0 -> 657
	0 -> 660
	0 -> 663
	0 -> 666
	0 -> 667
	0 -> 670
	0 -> 673
	0 -> 676
	0 -> 677
	0 -> 680
	0 -> 683
	0 -> 686
	0 -> 689
	0 -> 690
	0 -> 693
	0 -> 696
	0 -> 699
	0 -> 700
	0 -> 703
	0 -> 706
	0 -> 709
	0 -> 712
	0 -> 713
	0 -> 716
	0 -> 719
	0 -> 722
	0 -> 723
	0 -> 726
	0 -> 729
	0 -> 732
	0 -> 735
	0 -> 736
	0 -> 739
	0 -> 742
	0 -> 745
	0 -> 748
	2 -> 3
	3 -> 4
	4 -> 746
	5 -> 2543
	6 -> 7
	7 -> 8
	8 -> 2545
	9 -> 751
	10 -> 11
	11 -> 12
	12 -> 753
	13 -> 14
	14 -> 15
	15 -> 755
	16 -> 17
	17 -> 18
	18 -> 757
	19 -> 20
	20 -> 21
	21 -> 785
	22 -> 791
	23 -> 24
	24 -> 25
	25 -> 793
	26 -> 27
	27 -> 28
	28 -> 795
	29 -> 30
	30 -> 31
	31 -> 801
	32 -> 807
	33 -> 34
	34 -> 35
	35 -> 809
	36 -> 37
	37 -> 38
	38 -> 811
	39 -> 40
	40 -> 41
	41 -> 813
	42 -> 43
	43 -> 44
	44 -> 841
	45 -> 847
	46 -> 47
	47 -> 48
	48 -> 849
	49 -> 50
	50 -> 51
	51 -> 851
	52 -> 53
	53 -> 54
	54 -> 857
	55 -> 863
	56 -> 57
	57 -> 58
	58 -> 865
	59 -> 60
	60 -> 61
	61 -> 867
	62 -> 63
	63 -> 64
	64 -> 869
	65 -> 66
	66 -> 67
	67 -> 897
	68 -> 903
	69 -> 70
	70 -> 71
	71 -> 905
	72 -> 73
	73 -> 74
	74 -> 907
	75 -> 76
	76 -> 77
	77 -> 913
	78 -> 919
	79 -> 80
	80 -> 81
	81 -> 921
	82 -> 83
	83 -> 84
	84 -> 923
	85 -> 86
	86 -> 87
	87 -> 925
	88 -> 89
	89 -> 90
	90 -> 953
	91 -> 959
	92 -> 93
	93 -> 94
	94 -> 961
	95 -> 96
	96 -> 97
	97 -> 963
	98 -> 99
	99 -> 100
	100 -> 969
	101 -> 975
	102 -> 103
	103 -> 104
	104 -> 977
	105 -> 106
	106 -> 107
	107 -> 979
	108 -> 109
	109 -> 110
	110 -> 981
	111 -> 112
	112 -> 113
	113 -> 1009
	114 -> 1015
	115 -> 116
	116 -> 117
	117 -> 1017
	118 -> 119
	119 -> 120
	120 -> 1019
	121 -> 122
	122 -> 123
	123 -> 1025
	124 -> 1031
	125 -> 126
	126 -> 127
	127 -> 1033
	128 -> 129
	129 -> 130
	130 -> 1035
	131 -> 132
	132 -> 133
	133 -> 1037
	134 -> 135
	135 -> 136
	136 -> 1065
	137 -> 1071
	138 -> 139
	139 -> 140
	140 -> 1073
	141 -> 142
	142 -> 143
	143 -> 1075
	144 -> 145
	145 -> 146
	146 -> 1081
	147 -> 1087
	148 -> 149
	149 -> 150
	150 -> 1089
	151 -> 152
	152 -> 153
	153 -> 1091
	154 -> 155
	155 -> 156
	156 -> 1093
	157 -> 158
	158 -> 159
	159 -> 1121
	160 -> 1127
	161 -> 162
	162 -> 163
	163 -> 1129
	164 -> 165
	165 -> 166
	166 -> 1131
	167 -> 168
	168 -> 169
	169 -> 1137
	170 -> 1143
	171 -> 172
	172 -> 173
	173 -> 1145
	174 -> 175
	175 -> 176
	176 -> 1147
	177 -> 178
	178 -> 179
	179 -> 1149
	180 -> 181
	181 -> 182
	182 -> 1177
	183 -> 1183
	184 -> 185
	185 -> 186
	186 -> 1185
	187 -> 188
	188 -> 189
	189 -> 1187
	190 -> 191
	191 -> 192
	192 -> 1193
	193 -> 1199
	194 -> 195
	195 -> 196
	196 -> 1201
	197 -> 198
	198 -> 199
	199 -> 1203
	200 -> 201
	201 -> 202
	202 -> 1205
	203 -> 204
	204 -> 205
	205 -> 1233
	206 -> 1239
	207 -> 208
	208 -> 209
	209 -> 1241
	210 -> 211
	211 -> 212
	212 -> 1243
	213 -> 214
	214 -> 215
	215 -> 1249
	216 -> 1255
	217 -> 218
	218 -> 219
	219 -> 1257
	220 -> 221
	221 -> 222
	222 -> 1259
	223 -> 224
	224 -> 225
	225 -> 1261
	226 -> 227
	227 -> 228
	228 -> 1289
	229 -> 1295
	230 -> 231
	231 -> 232
	232 -> 1297
	233 -> 234
	234 -> 235
	235 -> 1299
	236 -> 237
	237 -> 238
	238 -> 1305
	239 -> 1311
	240 -> 241
	241 -> 242
	242 -> 1313
	243 -> 244
	244 -> 245
	245 -> 1315
	246 -> 247
	247 -> 248
	248 -> 1317
	249 -> 250
	250 -> 251
	251 -> 1345
	252 -> 1351
	253 -> 254
	254 -> 255
	255 -> 1353
	256 -> 257
	257 -> 258
	258 -> 1355
	259 -> 260
	260 -> 261
	261 -> 1361
	262 -> 1367
	263 -> 264
	264 -> 265
	265 -> 1369
	266 -> 267
	267 -> 268
	268 -> 1371
	269 -> 270
	270 -> 271
	271 -> 1373
	272 -> 273
	273 -> 274
	274 -> 1401
	275 -> 1407
	276 -> 277
	277 -> 278
	278 -> 1409
	279 -> 280
	280 -> 281
	281 -> 1411
	282 -> 283
	283 -> 284
	284 -> 1417
	285 -> 1423
	286 -> 287
	287 -> 288
	288 -> 1425
	289 -> 290
	290 -> 291
	291 -> 1427
	292 -> 293
	293 -> 294
	294 -> 1429
	295 -> 296
	296 -> 297
	297 -> 1457
	298 -> 1463
	299 -> 300
	300 -> 301
	301 -> 1465
	302 -> 303
	303 -> 304
	304 -> 1467
	305 -> 306
	306 -> 307
	307 -> 1473
	308 -> 1479
	309 -> 310
	310 -> 311
	311 -> 1481
	312 -> 313
	313 -> 314
	314 -> 1483
	315 -> 316
	316 -> 317
	317 -> 1485
	318 -> 319
	319 -> 320
	320 -> 1513
	321 -> 1519
	322 -> 323
	323 -> 324
	324 -> 1521
	325 -> 326
	326 -> 327
	327 -> 1523
	328 -> 329
	329 -> 330
	330 -> 1529
	331 -> 1535
	332 -> 333
	333 -> 334
	334 -> 1537
	335 -> 336
	336 -> 337
	337 -> 1539
	338 -> 339
	339 -> 340
	340 -> 1541
	341 -> 342
	342 -> 343
	343 -> 1569
	344 -> 1575
	345 -> 346
	346 -> 347
	347 -> 1577
	348 -> 349
	349 -> 350
	350 -> 1579
	351 -> 352
	352 -> 353
	353 -> 1585
	354 -> 1591
	355 -> 356
	356 -> 357
	357 -> 1593
	358 -> 359
	359 -> 360
	360 -> 1595
	361 -> 362
	362 -> 363
	363 -> 1597
	364 -> 365
	365 -> 366
	366 -> 1625
	367 -> 1631
	368 -> 369
	369 -> 370
	370 -> 1633
	371 -> 372
	372 -> 373
	373 -> 1635
	374 -> 375
	375 -> 376
	376 -> 1641
	377 -> 1647
	378 -> 379
	379 -> 380
	380 -> 1649
	381 -> 382
	382 -> 383
	383 -> 1651
	384 -> 385
	385 -> 386
	386 -> 1653
	387 -> 388
	388 -> 389
	389 -> 1681
	390 -> 1687
	391 -> 392
	392 -> 393
	393 -> 1689
	394 -> 395
	395 -> 396
	396 -> 1691
	397 -> 398
	398 -> 399
	399 -> 1697
	400 -> 1703
	401 -> 402
	402 -> 403
	403 -> 1705
	404 -> 405
	405 -> 406
	406 -> 1707
	407 -> 408
	408 -> 409
	409 -> 1709
	410 -> 411
	411 -> 412
	412 -> 1737
	413 -> 1743
	414 -> 415
	415 -> 416
	416 -> 1745
	417 -> 418
	418 -> 419
	419 -> 1747
	420 -> 421
	421 -> 422
	422 -> 1753
	423 -> 1759
	424 -> 425
	425 -> 426
	426 -> 1761
	427 -> 428
	428 -> 429
	429 -> 1763
	430 -> 431
	431 -> 432
	432 -> 1765
	433 -> 434
	434 -> 435
	435 -> 1793
	436 -> 1799
	437 -> 438
	438 -> 439
	439 -> 1801
	440 -> 441
	441 -> 442
	442 -> 1803
	443 -> 444
	444 -> 445
	445 -> 1809
	446 -> 1815
	447 -> 448
	448 -> 449
	449 -> 1817
	450 -> 451
	451 -> 452
	452 -> 1819
	453 -> 454
	454 -> 455
	455 -> 1821
	456 -> 457
	457 -> 458
	458 -> 1849
	459 -> 1855
	460 -> 461
	461 -> 462
	462 -> 1857
	463 -> 464
	464 -> 465
	465 -> 1859
	466 -> 467
	467 -> 468
	468 -> 1865
	469 -> 1871
	470 -> 471
	471 -> 472
	472 -> 1873
	473 -> 474
	474 -> 475
	475 -> 1875
	476 -> 477
	477 -> 478
	478 -> 1877
	479 -> 480
	480 -> 481
	481 -> 1905
	482 -> 1911
	483 -> 484
	484 -> 485
	485 -> 1913
	486 -> 487
	487 -> 488
	488 -> 1915
	489 -> 490
	490 -> 491
	491 -> 1921
	492 -> 1927
	493 -> 494
	494 -> 495
	495 -> 1929
	496 -> 497
	497 -> 498
	498 -> 1931
	499 -> 500
	500 -> 501
	501 -> 1933
	502 -> 503
	503 -> 504
	504 -> 1961
	505 -> 1967
	506 -> 507
	507 -> 508
	508 -> 1969
	509 -> 510
	510 -> 511
	511 -> 1971
	512 -> 513
	513 -> 514
	514 -> 1977
	515 -> 1983
	516 -> 517
	517 -> 518
	518 -> 1985
	519 -> 520
	520 -> 521
	521 -> 1987
	522 -> 523
	523 -> 524
	524 -> 1989
	525 -> 526
	526 -> 527
	527 -> 2017
	528 -> 2023
	529 -> 530
	530 -> 531
	531 -> 2025
	532 -> 533
	533 -> 534
	534 -> 2027
	535 -> 536
	536 -> 537
	537 -> 2033
	538 -> 2039
	539 -> 540
	540 -> 541
	541 -> 2041
	542 -> 543
	543 -> 544
	544 -> 2043
	545 -> 546
	546 -> 547
	547 -> 2045
	548 -> 549
	549 -> 550
	550 -> 2073
	551 -> 2079
	552 -> 553
	553 -> 554
	554 -> 2081
	555 -> 556
	556 -> 557
	557 -> 2083
	558 -> 559
	559 -> 560
	560 -> 2089
	561 -> 2095
	562 -> 563
	563 -> 564
	564 -> 2097
	565 -> 566
	566 -> 567
	567 -> 2099
	568 -> 569
	569 -> 570
	570 -> 2101
	571 -> 572
	572 -> 573
	573 -> 2129
	574 -> 2135
	575 -> 576
	576 -> 577
	577 -> 2137
	578 -> 579
	579 -> 580
	580 -> 2139
	581 -> 582
	582 -> 583
	583 -> 2145
	584 -> 2151
	585 -> 586
	586 -> 587
	587 -> 2153
	588 -> 589
	589 -> 590
	590 -> 2155
	591 -> 592
	592 -> 593
	593 -> 2157
	594 -> 595
	595 -> 596
	596 -> 2185
	597 -> 2191
	598 -> 599
	599 -> 600
	600 -> 2193
	601 -> 602
	602 -> 603
	603 -> 2195
	604 -> 605
	605 -> 606
	606 -> 2201
	607 -> 2207
	608 -> 609
	609 -> 610
	610 -> 2209
	611 -> 612
	612 -> 613
	613 -> 2211
	614 -> 615
	615 -> 616
	616 -> 2213
	617 -> 618
	618 -> 619
	619 -> 2241
	620 -> 2247
	621 -> 622
	622 -> 623
	623 -> 2249
	624 -> 625
	625 -> 626
	626 -> 2251
	627 -> 628
	628 -> 629
	629 -> 2257
	630 -> 2263
	631 -> 632
	632 -> 633
	633 -> 2265
	634 -> 635
	635 -> 636
	636 -> 2267
	637 -> 638
	638 -> 639
	639 -> 2269
	640 -> 641
	641 -> 642
	642 -> 2297
	643 -> 2303
	644 -> 645
	645 -> 646
	646 -> 2305
	647 -> 648
	648 -> 649
	649 -> 2307
	650 -> 651
	651 -> 652
	652 -> 2313
	653 -> 2319
	654 -> 655
	655 -> 656
	656 -> 2321
	657 -> 658
	658 -> 659
	659 -> 2323
	660 -> 661
	661 -> 662
	662 -> 2325
	663 -> 664
	664 -> 665
	665 -> 2353
	666 -> 2359
	667 -> 668
	668 -> 669
	669 -> 2361
	670 -> 671
	671 -> 672
	672 -> 2363
	673 -> 674
	674 -> 675
	675 -> 2369
	676 -> 2375
	677 -> 678
	678 -> 679
	679 -> 2377
	680 -> 681
	681 -> 682
	682 -> 2379
	683 -> 684
	684 -> 685
	685 -> 2381
	686 -> 687
	687 -> 688
	688 -> 2409
	689 -> 2415
	690 -> 691
	691 -> 692
	692 -> 2417
	693 -> 694
	694 -> 695
	695 -> 2419
	696 -> 697
	697 -> 698
	698 -> 2425
	699 -> 2431
	700 -> 701
	701 -> 702
	702 -> 2433
	703 -> 704
	704 -> 705
	705 -> 2435
	706 -> 707
	707 -> 708
	708 -> 2437
	709 -> 710
	710 -> 711
	711 -> 2465
	712 -> 2471
	713 -> 714
	714 -> 715
	715 -> 2473
	716 -> 717
	717 -> 718
	718 -> 2475
	719 -> 720
	720 -> 721
	721 -> 2481
	722 -> 2487
	723 -> 724
	724 -> 725
	725 -> 2489
	726 -> 727
	727 -> 728
	728 -> 2491
	729 -> 730
	730 -> 731
	731 -> 2493
	732 -> 733
	733 -> 734
	734 -> 2521
	735 -> 2527
	736 -> 737
	737 -> 738
	738 -> 2529
	739 -> 740
	740 -> 741
	741 -> 2531
	742 -> 743
	743 -> 744
	744 -> 2537
	745 -> 746
	746 -> 747
	747 -> 749
	747 -> 787
	748 -> 763
	748 -> 765
	748 -> 819
	748 -> 821
	748 -> 875
	748 -> 877
	748 -> 931
	748 -> 933
	748 -> 987
	748 -> 989
	748 -> 1043
	748 -> 1045
	748 -> 1099
	748 -> 1101
	748 -> 1155
	748 -> 1157
	748 -> 1211
	748 -> 1213
	748 -> 1267
	748 -> 1269
	748 -> 1323
	748 -> 1325
	748 -> 1379
	748 -> 1381
	748 -> 1435
	748 -> 1437
	748 -> 1491
	748 -> 1493
	748 -> 1547
	748 -> 1549
	748 -> 1603
	748 -> 1605
	748 -> 1659
	748 -> 1661
	748 -> 1715
	748 -> 1717
	748 -> 1771
	748 -> 1773
	748 -> 1827
	748 -> 1829
	748 -> 1883
	748 -> 1885
	748 -> 1939
	748 -> 1941
	748 -> 1995
	748 -> 1997
	748 -> 2051
	748 -> 2053
	748 -> 2107
	748 -> 2109
	748 -> 2163
	748 -> 2165
	748 -> 2219
	748 -> 2221
	748 -> 2275
	748 -> 2277
	748 -> 2331
	748 -> 2333
	748 -> 2387
	748 -> 2389
	748 -> 2443
	748 -> 2445
	748 -> 2499
	748 -> 2501
	749 -> 750
	750 -> 751
	751 -> 752
	752 -> 753
	752 -> 755
	752 -> 757
	753 -> 754
	754 -> 759
	755 -> 756
	756 -> 761
	757 -> 758
	758 -> 775
	759 -> 760
	760 -> 763
	761 -> 762
	762 -> 765
	763 -> 764
	764 -> 767
	765 -> 766
	766 -> 769
	767 -> 768
	768 -> 771
	769 -> 770
	770 -> 771
	771 -> 772
	772 -> 773
	773 -> 774
	774 -> 779
	775 -> 776
	776 -> 777
	777 -> 778
	778 -> 779
	779 -> 780
	780 -> 781
	781 -> 782
	782 -> 783
	783 -> 784
	784 -> 785
	785 -> 786
	786 -> 787
	787 -> 788
	788 -> 789
	788 -> 803
	789 -> 790
	790 -> 791
	791 -> 792
	792 -> 793
	792 -> 795
	793 -> 794
	794 -> 799
	795 -> 796
	796 -> 797
	797 -> 798
	798 -> 799
	799 -> 800
	800 -> 801
	801 -> 802
	802 -> 803
	803 -> 804
	804 -> 805
	804 -> 843
	805 -> 806
	806 -> 807
	807 -> 808
	808 -> 809
	808 -> 811
	808 -> 813
	809 -> 810
	810 -> 815
	811 -> 812
	812 -> 817
	813 -> 814
	814 -> 831
	815 -> 816
	816 -> 819
	817 -> 818
	818 -> 821
	819 -> 820
	820 -> 823
	821 -> 822
	822 -> 825
	823 -> 824
	824 -> 827
	825 -> 826
	826 -> 827
	827 -> 828
	828 -> 829
	829 -> 830
	830 -> 835
	831 -> 832
	832 -> 833
	833 -> 834
	834 -> 835
	835 -> 836
	836 -> 837
	837 -> 838
	838 -> 839
	839 -> 840
	840 -> 841
	841 -> 842
	842 -> 843
	843 -> 844
	844 -> 845
	844 -> 859
	845 -> 846
	846 -> 847
	847 -> 848
	848 -> 849
	848 -> 851
	849 -> 850
	850 -> 855
	851 -> 852
	852 -> 853
	853 -> 854
	854 -> 855
	855 -> 856
	856 -> 857
	857 -> 858
	858 -> 859
	859 -> 860
	860 -> 861
	860 -> 899
	861 -> 862
	862 -> 863
	863 -> 864
	864 -> 865
	864 -> 867
	864 -> 869
	865 -> 866
	866 -> 871
	867 -> 868
	868 -> 873
	869 -> 870
	870 -> 887
	871 -> 872
	872 -> 875
	873 -> 874
	874 -> 877
	875 -> 876
	876 -> 879
	877 -> 878
	878 -> 881
	879 -> 880
	880 -> 883
	881 -> 882
	882 -> 883
	883 -> 884
	884 -> 885
	885 -> 886
	886 -> 891
	887 -> 888
	888 -> 889
	889 -> 890
	890 -> 891
	891 -> 892
	892 -> 893
	893 -> 894
	894 -> 895
	895 -> 896
	896 -> 897
	897 -> 898
	898 -> 899
	899 -> 900
	900 -> 901
	900 -> 915
	901 -> 902
	902 -> 903
	903 -> 904
	904 -> 905
	904 -> 907
	905 -> 906
	906 -> 911
	907 -> 908
	908 -> 909
	909 -> 910
	910 -> 911
	911 -> 912
	912 -> 913
	913 -> 914
	914 -> 915
	915 -> 916
	916 -> 917
	916 -> 955
	917 -> 918
	918 -> 919
	919 -> 920
	920 -> 921
	920 -> 923
	920 -> 925
	921 -> 922
	922 -> 927
	923 -> 924
	924 -> 929
	925 -> 926
	926 -> 943
	927 -> 928
	928 -> 931
	929 -> 930
	930 -> 933
	931 -> 932
	932 -> 935
	933 -> 934
	934 -> 937
	935 -> 936
	936 -> 939
	937 -> 938
	938 -> 939
	939 -> 940
	940 -> 941
	941 -> 942
	942 -> 947
	943 -> 944
	944 -> 945
	945 -> 946
	946 -> 947
	947 -> 948
	948 -> 949
	949 -> 950
	950 -> 951
	951 -> 952
	952 -> 953
	953 -> 954
	954 -> 955
	955 -> 956
	956 -> 957
	956 -> 971
	957 -> 958
	958 -> 959
	959 -> 960
	960 -> 961
	960 -> 963
	961 -> 962
	962 -> 967
	963 -> 964
	964 -> 965
	965 -> 966
	966 -> 967
	967 -> 968
	968 -> 969
	969 -> 970
	970 -> 971
	971 -> 972
	972 -> 973
	972 -> 1011
	973 -> 974
	974 -> 975
	975 -> 976
	976 -> 977
	976 -> 979
	976 -> 981
	977 -> 978
	978 -> 983
	979 -> 980
	980 -> 985
	981 -> 982
	982 -> 999
	983 -> 984
	984 -> 987
	985 -> 986
	986 -> 989
	987 -> 988
	988 -> 991
	989 -> 990
	990 -> 993
	991 -> 992
	992 -> 995
	993 -> 994
	994 -> 995
	995 -> 996
	996 -> 997
	997 -> 998
	998 -> 1003
	999 -> 1000
	1000 -> 1001
	1001 -> 1002
	1002 -> 1003
	1003 -> 1004
	1004 -> 1005
	1005 -> 1006
	1006 -> 1007
	1007 -> 1008
	1008 -> 1009
	1009 -> 1010
	1010 -> 1011
	1011 -> 1012
	1012 -> 1013
	1012 -> 1027
	1013 -> 1014
	1014 -> 1015
	1015 -> 1016
	1016 -> 1017
	1016 -> 1019
	1017 -> 1018
	1018 -> 1023
	1019 -> 1020
	1020 -> 1021
	1021 -> 1022
	1022 -> 1023
	1023 -> 1024
	1024 -> 1025
	1025 -> 1026
	1026 -> 1027
	1027 -> 1028
	1028 -> 1029
	1028 -> 1067
	1029 -> 1030
	1030 -> 1031
	1031 -> 1032
	1032 -> 1033
	1032 -> 1035
	1032 -> 1037
	1033 -> 1034
	1034 -> 1039
	1035 -> 1036
	1036 -> 1041
	1037 -> 1038
	1038 -> 1055
	1039 -> 1040
	1040 -> 1043
	1041 -> 1042
	1042 -> 1045
	1043 -> 1044
	1044 -> 1047
	1045 -> 1046
	1046 -> 1049
	1047 -> 1048
	1048 -> 1051
	1049 -> 1050
	1050 -> 1051
	1051 -> 1052
	1052 -> 1053
	1053 -> 1054
	1054 -> 1059
	1055 -> 1056
	1056 -> 1057
	1057 -> 1058
	1058 -> 1059
	1059 -> 1060
	1060 -> 1061
	1061 -> 1062
	1062 -> 1063
	1063 -> 1064
	1064 -> 1065
	1065 -> 1066
	1066 -> 1067
	1067 -> 1068
	1068 -> 1069
	1068 -> 1083
	1069 -> 1070
	1070 -> 1071
	1071 -> 1072
	1072 -> 1073
	1072 -> 1075
	1073 -> 1074
	1074 -> 1079
	1075 -> 1076
	1076 -> 1077
	1077 -> 1078
	1078 -> 1079
	1079 -> 1080
	1080 -> 1081
	1081 -> 1082
	1082 -> 1083
	1083 -> 1084
	1084 -> 1085
	1084 -> 1123
	1085 -> 1086
	1086 -> 1087
	1087 -> 1088
	1088 -> 1089
	1088 -> 1091
	1088 -> 1093
	1089 -> 1090
	1090 -> 1095
	1091 -> 1092
	1092 -> 1097
	1093 -> 1094
	1094 -> 1111
	1095 -> 1096
	1096 -> 1099
	1097 -> 1098
	1098 -> 1101
	1099 -> 1100
	1100 -> 1103
	1101 -> 1102
	1102 -> 1105
	1103 -> 1104
	1104 -> 1107
	1105 -> 1106
	1106 -> 1107
	1107 -> 1108
	1108 -> 1109
	1109 -> 1110
	1110 -> 1115
	1111 -> 1112
	1112 -> 1113
	1113 -> 1114
	1114 -> 1115
	1115 -> 1116
	1116 -> 1117
	1117 -> 1118
	1118 -> 1119
	1119 -> 1120
	1120 -> 1121
	1121 -> 1122
	1122 -> 1123
	1123 -> 1124
	1124 -> 1125
	1124 -> 1139
	1125 -> 1126
	1126 -> 1127
	1127 -> 1128
	1128 -> 1129
	1128 -> 1131
	1129 -> 1130
	1130 -> 1135
	1131 -> 1132
	1132 -> 1133
	1133 -> 1134
	1134 -> 1135
	1135 -> 1136
	1136 -> 1137
	1137 -> 1138
	1138 -> 1139
	1139 -> 1140
	1140 -> 1141
	1140 -> 1179
	1141 -> 1142
	1142 -> 1143
	1143 -> 1144
	1144 -> 1145
	1144 -> 1147
	1144 -> 1149
	1145 -> 1146
	1146 -> 1151
	1147 -> 1148
	1148 -> 1153
	1149 -> 1150
	1150 -> 1167
	1151 -> 1152
	1152 -> 1155
	1153 -> 1154
	1154 -> 1157
	1155 -> 1156
	1156 -> 1159
	1157 -> 1158
	1158 -> 1161
	1159 -> 1160
	1160 -> 1163
	1161 -> 1162
	1162 -> 1163
	1163 -> 1164
	1164 -> 1165
	1165 -> 1166
	1166 -> 1171
	1167 -> 1168
	1168 -> 1169
	1169 -> 1170
	1170 -> 1171
	1171 -> 1172
	1172 -> 1173
	1173 -> 1174
	1174 -> 1175
	1175 -> 1176
	1176 -> 1177
	1177 -> 1178
	1178 -> 1179
	1179 -> 1180
	1180 -> 1181
	1180 -> 1195
	1181 -> 1182
	1182 -> 1183
	1183 -> 1184
	1184 -> 1185
	1184 -> 1187
	1185 -> 1186
	1186 -> 1191
	1187 -> 1188
	1188 -> 1189
	1189 -> 1190
	1190 -> 1191
	1191 -> 1192
	1192 -> 1193
	1193 -> 1194
	1194 -> 1195
	1195 -> 1196
	1196 -> 1197
	1196 -> 1235
	1197 -> 1198
	1198 -> 1199
	1199 -> 1200
	1200 -> 1201
	1200 -> 1203
	1200 -> 1205
	1201 -> 1202
	1202 -> 1207
	1203 -> 1204
	1204 -> 1209
	1205 -> 1206
	1206 -> 1223
	1207 -> 1208
	1208 -> 1211
	1209 -> 1210
	1210 -> 1213
	1211 -> 1212
	1212 -> 1215
	1213 -> 1214
	1214 -> 1217
	1215 -> 1216
	1216 -> 1219
	1217 -> 1218
	1218 -> 1219
	1219 -> 1220
	1220 -> 1221
	1221 -> 1222
	1222 -> 1227
	1223 -> 1224
	1224 -> 1225
	1225 -> 1226
	1226 -> 1227
	1227 -> 1228
	1228 -> 1229
	1229 -> 1230
	1230 -> 1231
	1231 -> 1232
	1232 -> 1233
	1233 -> 1234
	1234 -> 1235
	1235 -> 1236
	1236 -> 1237
	1236 -> 1251
	1237 -> 1238
	1238 -> 1239
	1239 -> 1240
	1240 -> 1241
	1240 -> 1243
	1241 -> 1242
	1242 -> 1247
	1243 -> 1244
	1244 -> 1245
	1245 -> 1246
	1246 -> 1247
	1247 -> 1248
	1248 -> 1249
	1249 -> 1250
	1250 -> 1251
	1251 -> 1252
	1252 -> 1253
	1252 -> 1291
	1253 -> 1254
	1254 -> 1255
	1255 -> 1256
	1256 -> 1257
	1256 -> 1259
	1256 -> 1261
	1257 -> 1258
	1258 -> 1263
	1259 -> 1260
	1260 -> 1265
	1261 -> 1262
	1262 -> 1279
	1263 -> 1264
	1264 -> 1267
	1265 -> 1266
	1266 -> 1269
	1267 -> 1268
	1268 -> 1271
	1269 -> 1270
	1270 -> 1273
	1271 -> 1272
	1272 -> 1275
	1273 -> 1274
	1274 -> 1275
	1275 -> 1276
	1276 -> 1277
	1277 -> 1278
	1278 -> 1283
	1279 -> 1280
	1280 -> 1281
	1281 -> 1282
	1282 -> 1283
	1283 -> 1284
	1284 -> 1285
	1285 -> 1286
	1286 -> 1287
	1287 -> 1288
	1288 -> 1289
	1289 -> 1290
	1290 -> 1291
	1291 -> 1292
	1292 -> 1293
	1292 -> 1307
	1293 -> 1294
	1294 -> 1295
	1295 -> 1296
	1296 -> 1297
	1296 -> 1299
	1297 -> 1298
	1298 -> 1303
	1299 -> 1300
	1300 -> 1301
	1301 -> 1302
	1302 -> 1303
	1303 -> 1304
	1304 -> 1305
	1305 -> 1306
	1306 -> 1307
	1307 -> 1308
	1308 -> 1309
	1308 -> 1347
	1309 -> 1310
	1310 -> 1311
	1311 -> 1312
	1312 -> 1313
	1312 -> 1315
	1312 -> 1317
	1313 -> 1314
	1314 -> 1319
	1315 -> 1316
	1316 -> 1321
	1317 -> 1318
	1318 -> 1335
	1319 -> 1320
	1320 -> 1323
	1321 -> 1322
	1322 -> 1325
	1323 -> 1324
	1324 -> 1327
	1325 -> 1326
	1326 -> 1329
	1327 -> 1328
	1328 -> 1331
	1329 -> 1330
	1330 -> 1331
	1331 -> 1332
	1332 -> 1333
	1333 -> 1334
	1334 -> 1339
	1335 -> 1336
	1336 -> 1337
	1337 -> 1338
	1338 -> 1339
	1339 -> 1340
	1340 -> 1341
	1341 -> 1342
	1342 -> 1343
	1343 -> 1344
	1344 -> 1345
	1345 -> 1346
	1346 -> 1347
	1347 -> 1348
	1348 -> 1349
	1348 -> 1363
	1349 -> 1350
	1350 -> 1351
	1351 -> 1352
	1352 -> 1353
	1352 -> 1355
	1353 -> 1354
	1354 -> 1359
	1355 -> 1356
	1356 -> 1357
	1357 -> 1358
	1358 -> 1359
	1359 -> 1360
	1360 -> 1361
	1361 -> 1362
	1362 -> 1363
	1363 -> 1364
	1364 -> 1365
	1364 -> 1403
	1365 -> 1366
	1366 -> 1367
	1367 -> 1368
	1368 -> 1369
	1368 -> 1371
	1368 -> 1373
	1369 -> 1370
	1370 -> 1375
	1371 -> 1372
	1372 -> 1377
	1373 -> 1374
	1374 -> 1391
	1375 -> 1376
	1376 -> 1379
	1377 -> 1378
	1378 -> 1381
	1379 -> 1380
	1380 -> 1383
	1381 -> 1382
	1382 -> 1385
	1383 -> 1384
	1384 -> 1387
	1385 -> 1386
	1386 -> 1387
	1387 -> 1388
	1388 -> 1389
	1389 -> 1390
	1390 -> 1395
	1391 -> 1392
	1392 -> 1393
	1393 -> 1394
	1394 -> 1395
	1395 -> 1396
	1396 -> 1397
	1397 -> 1398
	1398 -> 1399
	1399 -> 1400
	1400 -> 1401
	1401 -> 1402
	1402 -> 1403
	1403 -> 1404
	1404 -> 1405
	1404 -> 1419
	1405 -> 1406
	1406 -> 1407
	1407 -> 1408
	1408 -> 1409
	1408 -> 1411
	1409 -> 1410
	1410 -> 1415
	1411 -> 1412
	1412 -> 1413
	1413 -> 1414
	1414 -> 1415
	1415 -> 1416
	1416 -> 1417
	1417 -> 1418
	1418 -> 1419
	1419 -> 1420
	1420 -> 1421
	1420 -> 1459
	1421 -> 1422
	1422 -> 1423
	1423 -> 1424
	1424 -> 1425
	1424 -> 1427
	1424 -> 1429
	1425 -> 1426
	1426 -> 1431
	1427 -> 1428
	1428 -> 1433
	1429 -> 1430
	1430 -> 1447
	1431 -> 1432
	1432 -> 1435
	1433 -> 1434
	1434 -> 1437
	1435 -> 1436
	1436 -> 1439
	1437 -> 1438
	1438 -> 1441
	1439 -> 1440
	1440 -> 1443
	1441 -> 1442
	1442 -> 1443
	1443 -> 1444
	1444 -> 1445
	1445 -> 1446
	1446 -> 1451
	1447 -> 1448
	1448 -> 1449
	1449 -> 1450
	1450 -> 1451
	1451 -> 1452
	1452 -> 1453
	1453 -> 1454
	1454 -> 1455
	1455 -> 1456
	1456 -> 1457
	1457 -> 1458
	1458 -> 1459
	1459 -> 1460
	1460 -> 1461
	1460 -> 1475
	1461 -> 1462
	1462 -> 1463
	1463 -> 1464
	1464 -> 1465
	1464 -> 1467
	1465 -> 1466
	1466 -> 1471
	1467 -> 1468
	1468 -> 1469
	1469 -> 1470
	1470 -> 1471
	1471 -> 1472
	1472 -> 1473
	1473 -> 1474
	1474 -> 1475
	1475 -> 1476
	1476 -> 1477
	1476 -> 1515
	1477 -> 1478
	1478 -> 1479
	1479 -> 1480
	1480 -> 1481
	1480 -> 1483
	1480 -> 1485
	1481 -> 1482
	1482 -> 1487
	1483 -> 1484
	1484 -> 1489
	1485 -> 1486
	1486 -> 1503
	1487 -> 1488
	1488 -> 1491
	1489 -> 1490
	1490 -> 1493
	1491 -> 1492
	1492 -> 1495
	1493 -> 1494
	1494 -> 1497
	1495 -> 1496
	1496 -> 1499
	1497 -> 1498
	1498 -> 1499
	1499 -> 1500
	1500 -> 1501
	1501 -> 1502
	1502 -> 1507
	1503 -> 1504
	1504 -> 1505
	1505 -> 1506
	1506 -> 1507
	1507 -> 1508
	1508 -> 1509
	1509 -> 1510
	1510 -> 1511
	1511 -> 1512
	1512 -> 1513
	1513 -> 1514
	1514 -> 1515
	1515 -> 1516
	1516 -> 1517
	1516 -> 1531
	1517 -> 1518
	1518 -> 1519
	1519 -> 1520
	1520 -> 1521
	1520 -> 1523
	1521 -> 1522
	1522 -> 1527
	1523 -> 1524
	1524 -> 1525
	1525 -> 1526
	1526 -> 1527
	1527 -> 1528
	1528 -> 1529
	1529 -> 1530
	1530 -> 1531
	1531 -> 1532
	1532 -> 1533
	1532 -> 1571
	1533 -> 1534
	1534 -> 1535
	1535 -> 1536
	1536 -> 1537
	1536 -> 1539
	1536 -> 1541
	1537 -> 1538
	1538 -> 1543
	1539 -> 1540
	1540 -> 1545
	1541 -> 1542
	1542 -> 1559
	1543 -> 1544
	1544 -> 1547
	1545 -> 1546
	1546 -> 1549
	1547 -> 1548
	1548 -> 1551
	1549 -> 1550
	1550 -> 1553
	1551 -> 1552
	1552 -> 1555
	1553 -> 1554
	1554 -> 1555
	1555 -> 1556
	1556 -> 1557
	1557 -> 1558
	1558 -> 1563
	1559 -> 1560
	1560 -> 1561
	1561 -> 1562
	1562 -> 1563
	1563 -> 1564
	1564 -> 1565
	1565 -> 1566
	1566 -> 1567
	1567 -> 1568
	1568 -> 1569
	1569 -> 1570
	1570 -> 1571
	1571 -> 1572
	1572 -> 1573
	1572 -> 1587
	1573 -> 1574
	1574 -> 1575
	1575 -> 1576
	1576 -> 1577
	1576 -> 1579
	1577 -> 1578
	1578 -> 1583
	1579 -> 1580
	1580 -> 1581
	1581 -> 1582
	1582 -> 1583
	1583 -> 1584
	1584 -> 1585
	1585 -> 1586
	1586 -> 1587
	1587 -> 1588
	1588 -> 1589
	1588 -> 1627
	1589 -> 1590
	1590 -> 1591
	1591 -> 1592
	1592 -> 1593
	1592 -> 1595
	1592 -> 1597
	1593 -> 1594
	1594 -> 1599
	1595 -> 1596
	1596 -> 1601
	1597 -> 1598
	1598 -> 1615
	1599 -> 1600
	1600 -> 1603
	1601 -> 1602
	1602 -> 1605
	1603 -> 1604
	1604 -> 1607
	1605 -> 1606
	1606 -> 1609
	1607 -> 1608
	1608 -> 1611
	1609 -> 1610
	1610 -> 1611
	1611 -> 1612
	1612 -> 1613
	1613 -> 1614
	1614 -> 1619
	1615 -> 1616
	1616 -> 1617
	1617 -> 1618
	1618 -> 1619
	1619 -> 1620
	1620 -> 1621
	1621 -> 1622
	1622 -> 1623
	1623 -> 1624
	1624 -> 1625
	1625 -> 1626
	1626 -> 1627
	1627 -> 1628
	1628 -> 1629
	1628 -> 1643
	1629 -> 1630
	1630 -> 1631
	1631 -> 1632
	1632 -> 1633
	1632 -> 1635
	1633 -> 1634
	1634 -> 1639
	1635 -> 1636
	1636 -> 1637
	1637 -> 1638
	1638 -> 1639
	1639 -> 1640
	1640 -> 1641
	1641 -> 1642
	1642 -> 1643
	1643 -> 1644
	1644 -> 1645
	1644 -> 1683
	1645 -> 1646
	1646 -> 1647
	1647 -> 1648
	1648 -> 1649
	1648 -> 1651
	1648 -> 1653
	1649 -> 1650
	1650 -> 1655
	1651 -> 1652
	1652 -> 1657
	1653 -> 1654
	1654 -> 1671
	1655 -> 1656
	1656 -> 1659
	1657 -> 1658
	1658 -> 1661
	1659 -> 1660
	1660 -> 1663
	1661 -> 1662
	1662 -> 1665
	1663 -> 1664
	1664 -> 1667
	1665 -> 1666
	1666 -> 1667
	1667 -> 1668
	1668 -> 1669
	1669 -> 1670
	1670 -> 1675
	1671 -> 1672
	1672 -> 1673
	1673 -> 1674
	1674 -> 1675
	1675 -> 1676
	1676 -> 1677
	1677 -> 1678
	1678 -> 1679
	1679 -> 1680
	1680 -> 1681
	1681 -> 1682
	1682 -> 1683
	1683 -> 1684
	1684 -> 1685
	1684 -> 1699
	1685 -> 1686
	1686 -> 1687
	1687 -> 1688
	1688 -> 1689
	1688 -> 1691
	1689 -> 1690
	1690 -> 1695
	1691 -> 1692
	1692 -> 1693
	1693 -> 1694
	1694 -> 1695
	1695 -> 1696
	1696 -> 1697
	1697 -> 1698
	1698 -> 1699
	1699 -> 1700
	1700 -> 1701
	1700 -> 1739
	1701 -> 1702
	1702 -> 1703
	1703 -> 1704
	1704 -> 1705
	1704 -> 1707
	1704 -> 1709
	1705 -> 1706
	1706 -> 1711
	1707 -> 1708
	1708 -> 1713
	1709 -> 1710
	1710 -> 1727
	1711 -> 1712
	1712 -> 1715
	1713 -> 1714
	1714 -> 1717
	1715 -> 1716
	1716 -> 1719
	1717 -> 1718
	1718 -> 1721
	1719 -> 1720
	1720 -> 1723
	1721 -> 1722
	1722 -> 1723
	1723 -> 1724
	1724 -> 1725
	1725 -> 1726
	1726 -> 1731
	1727 -> 1728
	1728 -> 1729
	1729 -> 1730
	1730 -> 1731
	1731 -> 1732
	1732 -> 1733
	1733 -> 1734
	1734 -> 1735
	1735 -> 1736
	1736 -> 1737
	1737 -> 1738
	1738 -> 1739
	1739 -> 1740
	1740 -> 1741
	1740 -> 1755
	1741 -> 1742
	1742 -> 1743
	1743 -> 1744
	1744 -> 1745
	1744 -> 1747
	1745 -> 1746
	1746 -> 1751
	1747 -> 1748
	1748 -> 1749
	1749 -> 1750
	1750 -> 1751
	1751 -> 1752
	1752 -> 1753
	1753 -> 1754
	1754 -> 1755
	1755 -> 1756
	1756 -> 1757
	1756 -> 1795
	1757 -> 1758
	1758 -> 1759
	1759 -> 1760
	1760 -> 1761
	1760 -> 1763
	1760 -> 1765
	1761 -> 1762
	1762 -> 1767
	1763 -> 1764
	1764 -> 1769
	1765 -> 1766
	1766 -> 1783
	1767 -> 1768
	1768 -> 1771
	1769 -> 1770
	1770 -> 1773
	1771 -> 1772
	1772 -> 1775
	1773 -> 1774
	1774 -> 1777
	1775 -> 1776
	1776 -> 1779
	1777 -> 1778
	1778 -> 1779
	1779 -> 1780
	1780 -> 1781
	1781 -> 1782
	1782 -> 1787
	1783 -> 1784
	1784 -> 1785
	1785 -> 1786
	1786 -> 1787
	1787 -> 1788
	1788 -> 1789
	1789 -> 1790
	1790 -> 1791
	1791 -> 1792
	1792 -> 1793
	1793 -> 1794
	1794 -> 1795
	1795 -> 1796
	1796 -> 1797
	1796 -> 1811
	1797 -> 1798
	1798 -> 1799
	1799 -> 1800
	1800 -> 1801
	1800 -> 1803
	1801 -> 1802
	1802 -> 1807
	1803 -> 1804
	1804 -> 1805
	1805 -> 1806
	1806 -> 1807
	1807 -> 1808
	1808 -> 1809
	1809 -> 1810
	1810 -> 1811
	1811 -> 1812
	1812 -> 1813
	1812 -> 1851
	1813 -> 1814
	1814 -> 1815
	1815 -> 1816
	1816 -> 1817
	1816 -> 1819
	1816 -> 1821
	1817 -> 1818
	1818 -> 1823
	1819 -> 1820
	1820 -> 1825
	1821 -> 1822
	1822 -> 1839
	1823 -> 1824
	1824 -> 1827
	1825 -> 1826
	1826 -> 1829
	1827 -> 1828
	1828 -> 1831
	1829 -> 1830
	1830 -> 1833
	1831 -> 1832
	1832 -> 1835
	1833 -> 1834
	1834 -> 1835
	1835 -> 1836
	1836 -> 1837
	1837 -> 1838
	1838 -> 1843
	1839 -> 1840
	1840 -> 1841
	1841 -> 1842
	1842 -> 1843
	1843 -> 1844
	1844 -> 1845
	1845 -> 1846
	1846 -> 1847
	1847 -> 1848
	1848 -> 1849
	1849 -> 1850
	1850 -> 1851
	1851 -> 1852
	1852 -> 1853
	1852 -> 1867
	1853 -> 1854
	1854 -> 1855
	1855 -> 1856
	1856 -> 1857
	1856 -> 1859
	1857 -> 1858
	1858 -> 1863
	1859 -> 1860
	1860 -> 1861
	1861 -> 1862
	1862 -> 1863
	1863 -> 1864
	1864 -> 1865
	1865 -> 1866
	1866 -> 1867
	1867 -> 1868
	1868 -> 1869
	1868 -> 1907
	1869 -> 1870
	1870 -> 1871
	1871 -> 1872
	1872 -> 1873
	1872 -> 1875
	1872 -> 1877
	1873 -> 1874
	1874 -> 1879
	1875 -> 1876
	1876 -> 1881
	1877 -> 1878
	1878 -> 1895
	1879 -> 1880
	1880 -> 1883
	1881 -> 1882
	1882 -> 1885
	1883 -> 1884
	1884 -> 1887
	1885 -> 1886
	1886 -> 1889
	1887 -> 1888
	1888 -> 1891
	1889 -> 1890
	1890 -> 1891
	1891 -> 1892
	1892 -> 1893
	1893 -> 1894
	1894 -> 1899
	1895 -> 1896
	1896 -> 1897
	1897 -> 1898
	1898 -> 1899
	1899 -> 1900
	1900 -> 1901
	1901 -> 1902
	1902 -> 1903
	1903 -> 1904
	1904 -> 1905
	1905 -> 1906
	1906 -> 1907
	1907 -> 1908
	1908 -> 1909
	1908 -> 1923
	1909 -> 1910
	1910 -> 1911
	1911 -> 1912
	1912 -> 1913
	1912 -> 1915
	1913 -> 1914
	1914 -> 1919
	1915 -> 1916
	1916 -> 1917
	1917 -> 1918
	1918 -> 1919
	1919 -> 1920
	1920 -> 1921
	1921 -> 1922
	1922 -> 1923
	1923 -> 1924
	1924 -> 1925
	1924 -> 1963
	1925 -> 1926
	1926 -> 1927
	1927 -> 1928
	1928 -> 1929
	1928 -> 1931
	1928 -> 1933
	1929 -> 1930
	1930 -> 1935
	1931 -> 1932
	1932 -> 1937
	1933 -> 1934
	1934 -> 1951
	1935 -> 1936
	1936 -> 1939
	1937 -> 1938
	1938 -> 1941
	1939 -> 1940
	1940 -> 1943
	1941 -> 1942
	1942 -> 1945
	1943 -> 1944
	1944 -> 1947
	1945 -> 1946
	1946 -> 1947
	1947 -> 1948
	1948 -> 1949
	1949 -> 1950
	1950 -> 1955
	1951 -> 1952
	1952 -> 1953
	1953 -> 1954
	1954 -> 1955
	1955 -> 1956
	1956 -> 1957
	1957 -> 1958
	1958 -> 1959
	1959 -> 1960
	1960 -> 1961
	1961 -> 1962
	1962 -> 1963
	1963 -> 1964
	1964 -> 1965
	1964 -> 1979
	1965 -> 1966
	1966 -> 1967
	1967 -> 1968
	1968 -> 1969
	1968 -> 1971
	1969 -> 1970
	1970 -> 1975
	1971 -> 1972
	1972 -> 1973
	1973 -> 1974
	1974 -> 1975
	1975 -> 1976
	1976 -> 1977
	1977 -> 1978
	1978 -> 1979
	1979 -> 1980
	1980 -> 1981
	1980 -> 2019
	1981 -> 1982
	1982 -> 1983
	1983 -> 1984
	1984 -> 1985
	1984 -> 1987
	1984 -> 1989
	1985 -> 1986
	1986 -> 1991
	1987 -> 1988
	1988 -> 1993
	1989 -> 1990
	1990 -> 2007
	1991 -> 1992
	1992 -> 1995
	1993 -> 1994
	1994 -> 1997
	1995 -> 1996
	1996 -> 1999
	1997 -> 1998
	1998 -> 2001
	1999 -> 2000
	2000 -> 2003
	2001 -> 2002
	2002 -> 2003
	2003 -> 2004
	2004 -> 2005
	2005 -> 2006
	2006 -> 2011
	2007 -> 2008
	2008 -> 2009
	2009 -> 2010
	2010 -> 2011
	2011 -> 2012
	2012 -> 2013
	2013 -> 2014
	2014 -> 2015
	2015 -> 2016
	2016 -> 2017
	2017 -> 2018
	2018 -> 2019
	2019 -> 2020
	2020 -> 2021
	2020 -> 2035
	2021 -> 2022
	2022 -> 2023
	2023 -> 2024
	2024 -> 2025
	2024 -> 2027
	2025 -> 2026
	2026 -> 2031
	2027 -> 2028
	2028 -> 2029
	2029 -> 2030
	2030 -> 2031
	2031 -> 2032
	2032 -> 2033
	2033 -> 2034
	2034 -> 2035
	2035 -> 2036
	2036 -> 2037
	2036 -> 2075
	2037 -> 2038
	2038 -> 2039
	2039 -> 2040
	2040 -> 2041
	2040 -> 2043
	2040 -> 2045
	2041 -> 2042
	2042 -> 2047
	2043 -> 2044
	2044 -> 2049
	2045 -> 2046
	2046 -> 2063
	2047 -> 2048
	2048 -> 2051
	2049 -> 2050
	2050 -> 2053
	2051 -> 2052
	2052 -> 2055
	2053 -> 2054
	2054 -> 2057
	2055 -> 2056
	2056 -> 2059
	2057 -> 2058
	2058 -> 2059
	2059 -> 2060
	2060 -> 2061
	2061 -> 2062
	2062 -> 2067
	2063 -> 2064
	2064 -> 2065
	2065 -> 2066
	2066 -> 2067
	2067 -> 2068
	2068 -> 2069
	2069 -> 2070
	2070 -> 2071
	2071 -> 2072
	2072 -> 2073
	2073 -> 2074
	2074 -> 2075
	2075 -> 2076
	2076 -> 2077
	2076 -> 2091
	2077 -> 2078
	2078 -> 2079
	2079 -> 2080
	2080 -> 2081
	2080 -> 2083
	2081 -> 2082
	2082 -> 2087
	2083 -> 2084
	2084 -> 2085
	2085 -> 2086
	2086 -> 2087
	2087 -> 2088
	2088 -> 2089
	2089 -> 2090
	2090 -> 2091
	2091 -> 2092
	2092 -> 2093
	2092 -> 2131
	2093 -> 2094
	2094 -> 2095
	2095 -> 2096
	2096 -> 2097
	2096 -> 2099
	2096 -> 2101
	2097 -> 2098
	2098 -> 2103
	2099 -> 2100
	2100 -> 2105
	2101 -> 2102
	2102 -> 2119
	2103 -> 2104
	2104 -> 2107
	2105 -> 2106
	2106 -> 2109
	2107 -> 2108
	2108 -> 2111
	2109 -> 2110
	2110 -> 2113
	2111 -> 2112
	2112 -> 2115
	2113 -> 2114
	2114 -> 2115
	2115 -> 2116
	2116 -> 2117
	2117 -> 2118
	2118 -> 2123
	2119 -> 2120
	2120 -> 2121
	2121 -> 2122
	2122 -> 2123
	2123 -> 2124
	2124 -> 2125
	2125 -> 2126
	2126 -> 2127
	2127 -> 2128
	2128 -> 2129
	2129 -> 2130
	2130 -> 2131
	2131 -> 2132
	2132 -> 2133
	2132 -> 2147
	2133 -> 2134
	2134 -> 2135
	2135 -> 2136
	2136 -> 2137
	2136 -> 2139
	2137 -> 2138
	2138 -> 2143
	2139 -> 2140
	2140 -> 2141
	2141 -> 2142
	2142 -> 2143
	2143 -> 2144
	2144 -> 2145
	2145 -> 2146
	2146 -> 2147
	2147 -> 2148
	2148 -> 2149
	2148 -> 2187
	2149 -> 2150
	2150 -> 2151
	2151 -> 2152
	2152 -> 2153
	2152 -> 2155
	2152 -> 2157
	2153 -> 2154
	2154 -> 2159
	2155 -> 2156
	2156 -> 2161
	2157 -> 2158
	2158 -> 2175
	2159 -> 2160
	2160 -> 2163
	2161 -> 2162
	2162 -> 2165
	2163 -> 2164
	2164 -> 2167
	2165 -> 2166
	2166 -> 2169
	2167 -> 2168
	2168 -> 2171
	2169 -> 2170
	2170 -> 2171
	2171 -> 2172
	2172 -> 2173
	2173 -> 2174
	2174 -> 2179
	2175 -> 2176
	2176 -> 2177
	2177 -> 2178
	2178 -> 2179
	2179 -> 2180
	2180 -> 2181
	2181 -> 2182
	2182 -> 2183
	2183 -> 2184
	2184 -> 2185
	2185 -> 2186
	2186 -> 2187
	2187 -> 2188
	2188 -> 2189
	2188 -> 2203
	2189 -> 2190
	2190 -> 2191
	2191 -> 2192
	2192 -> 2193
	2192 -> 2195
	2193 -> 2194
	2194 -> 2199
	2195 -> 2196
	2196 -> 2197
	2197 -> 2198
	2198 -> 2199
	2199 -> 2200
	2200 -> 2201
	2201 -> 2202
	2202 -> 2203
	2203 -> 2204
	2204 -> 2205
	2204 -> 2243
	2205 -> 2206
	2206 -> 2207
	2207 -> 2208
	2208 -> 2209
	2208 -> 2211
	2208 -> 2213
	2209 -> 2210
	2210 -> 2215
	2211 -> 2212
	2212 -> 2217
	2213 -> 2214
	2214 -> 2231
	2215 -> 2216
	2216 -> 2219
	2217 -> 2218
	2218 -> 2221
	2219 -> 2220
	2220 -> 2223
	2221 -> 2222
	2222 -> 2225
	2223 -> 2224
	2224 -> 2227
	2225 -> 2226
	2226 -> 2227
	2227 -> 2228
	2228 -> 2229
	2229 -> 2230
	2230 -> 2235
	2231 -> 2232
	2232 -> 2233
	2233 -> 2234
	2234 -> 2235
	2235 -> 2236
	2236 -> 2237
	2237 -> 2238
	2238 -> 2239
	2239 -> 2240
	2240 -> 2241
	2241 -> 2242
	2242 -> 2243
	2243 -> 2244
	2244 -> 2245
	2244 -> 2259
	2245 -> 2246
	2246 -> 2247
	2247 -> 2248
	2248 -> 2249
	2248 -> 2251
	2249 -> 2250
	2250 -> 2255
	2251 -> 2252
	2252 -> 2253
	2253 -> 2254
	2254 -> 2255
	2255 -> 2256
	2256 -> 2257
	2257 -> 2258
	2258 -> 2259
	2259 -> 2260
	2260 -> 2261
	2260 -> 2299
	2261 -> 2262
	2262 -> 2263
	2263 -> 2264
	2264 -> 2265
	2264 -> 2267
	2264 -> 2269
	2265 -> 2266
	2266 -> 2271
	2267 -> 2268
	2268 -> 2273
	2269 -> 2270
	2270 -> 2287
	2271 -> 2272
	2272 -> 2275
	2273 -> 2274
	2274 -> 2277
	2275 -> 2276
	2276 -> 2279
	2277 -> 2278
	2278 -> 2281
	2279 -> 2280
	2280 -> 2283
	2281 -> 2282
	2282 -> 2283
	2283 -> 2284
	2284 -> 2285
	2285 -> 2286
	2286 -> 2291
	2287 -> 2288
	2288 -> 2289
	2289 -> 2290
	2290 -> 2291
	2291 -> 2292
	2292 -> 2293
	2293 -> 2294
	2294 -> 2295
	2295 -> 2296
	2296 -> 2297
	2297 -> 2298
	2298 -> 2299
	2299 -> 2300
	2300 -> 2301
	2300 -> 2315
	2301 -> 2302
	2302 -> 2303
	2303 -> 2304
	2304 -> 2305
	2304 -> 2307
	2305 -> 2306
	2306 -> 2311
	2307 -> 2308
	2308 -> 2309
	2309 -> 2310
	2310 -> 2311
	2311 -> 2312
	2312 -> 2313
	2313 -> 2314
	2314 -> 2315
	2315 -> 2316
	2316 -> 2317
	2316 -> 2355
	2317 -> 2318
	2318 -> 2319
	2319 -> 2320
	2320 -> 2321
	2320 -> 2323
	2320 -> 2325
	2321 -> 2322
	2322 -> 2327
	2323 -> 2324
	2324 -> 2329
	2325 -> 2326
	2326 -> 2343
	2327 -> 2328
	2328 -> 2331
	2329 -> 2330
	2330 -> 2333
	2331 -> 2332
	2332 -> 2335
	2333 -> 2334
	2334 -> 2337
	2335 -> 2336
	2336 -> 2339
	2337 -> 2338
	2338 -> 2339
	2339 -> 2340
	2340 -> 2341
	2341 -> 2342
	2342 -> 2347
	2343 -> 2344
	2344 -> 2345
	2345 -> 2346
	2346 -> 2347
	2347 -> 2348
	2348 -> 2349
	2349 -> 2350
	2350 -> 2351
	2351 -> 2352
	2352 -> 2353
	2353 -> 2354
	2354 -> 2355
	2355 -> 2356
	2356 -> 2357
	2356 -> 2371
	2357 -> 2358
	2358 -> 2359
	2359 -> 2360
	2360 -> 2361
	2360 -> 2363
	2361 -> 2362
	2362 -> 2367
	2363 -> 2364
	2364 -> 2365
	2365 -> 2366
	2366 -> 2367
	2367 -> 2368
	2368 -> 2369
	2369 -> 2370
	2370 -> 2371
	2371 -> 2372
	2372 -> 2373
	2372 -> 2411
	2373 -> 2374
	2374 -> 2375
	2375 -> 2376
	2376 -> 2377
	2376 -> 2379
	2376 -> 2381
	2377 -> 2378
	2378 -> 2383
	2379 -> 2380
	2380 -> 2385
	2381 -> 2382
	2382 -> 2399
	2383 -> 2384
	2384 -> 2387
	2385 -> 2386
	2386 -> 2389
	2387 -> 2388
	2388 -> 2391
	2389 -> 2390
	2390 -> 2393
	2391 -> 2392
	2392 -> 2395
	2393 -> 2394
	2394 -> 2395
	2395 -> 2396
	2396 -> 2397
	2397 -> 2398
	2398 -> 2403
	2399 -> 2400
	2400 -> 2401
	2401 -> 2402
	2402 -> 2403
	2403 -> 2404
	2404 -> 2405
	2405 -> 2406
	2406 -> 2407
	2407 -> 2408
	2408 -> 2409
	2409 -> 2410
	2410 -> 2411
	2411 -> 2412
	2412 -> 2413
	2412 -> 2427
	2413 -> 2414
	2414 -> 2415
	2415 -> 2416
	2416 -> 2417
	2416 -> 2419
	2417 -> 2418
	2418 -> 2423
	2419 -> 2420
	2420 -> 2421
	2421 -> 2422
	2422 -> 2423
	2423 -> 2424
	2424 -> 2425
	2425 -> 2426
	2426 -> 2427
	2427 -> 2428
	2428 -> 2429
	2428 -> 2467
	2429 -> 2430
	2430 -> 2431
	2431 -> 2432
	2432 -> 2433
	2432 -> 2435
	2432 -> 2437
	2433 -> 2434
	2434 -> 2439
	2435 -> 2436
	2436 -> 2441
	2437 -> 2438
	2438 -> 2455
	2439 -> 2440
	2440 -> 2443
	2441 -> 2442
	2442 -> 2445
	2443 -> 2444
	2444 -> 2447
	2445 -> 2446
	2446 -> 2449
	2447 -> 2448
	2448 -> 2451
	2449 -> 2450
	2450 -> 2451
	2451 -> 2452
	2452 -> 2453
	2453 -> 2454
	2454 -> 2459
	2455 -> 2456
	2456 -> 2457
	2457 -> 2458
	2458 -> 2459
	2459 -> 2460
	2460 -> 2461
	2461 -> 2462
	2462 -> 2463
	2463 -> 2464
	2464 -> 2465
	2465 -> 2466
	2466 -> 2467
	2467 -> 2468
	2468 -> 2469
	2468 -> 2483
	2469 -> 2470
	2470 -> 2471
	2471 -> 2472
	2472 -> 2473
	2472 -> 2475
	2473 -> 2474
	2474 -> 2479
	2475 -> 2476
	2476 -> 2477
	2477 -> 2478
	2478 -> 2479
	2479 -> 2480
	2480 -> 2481
	2481 -> 2482
	2482 -> 2483
	2483 -> 2484
	2484 -> 2485
	2484 -> 2523
	2485 -> 2486
	2486 -> 2487
	2487 -> 2488
	2488 -> 2489
	2488 -> 2491
	2488 -> 2493
	2489 -> 2490
	2490 -> 2495
	2491 -> 2492
	2492 -> 2497
	2493 -> 2494
	2494 -> 2511
	2495 -> 2496
	2496 -> 2499
	2497 -> 2498
	2498 -> 2501
	2499 -> 2500
	2500 -> 2503
	2501 -> 2502
	2502 -> 2505
	2503 -> 2504
	2504 -> 2507
	2505 -> 2506
	2506 -> 2507
	2507 -> 2508
	2508 -> 2509
	2509 -> 2510
	2510 -> 2515
	2511 -> 2512
	2512 -> 2513
	2513 -> 2514
	2514 -> 2515
	2515 -> 2516
	2516 -> 2517
	2517 -> 2518
	2518 -> 2519
	2519 -> 2520
	2520 -> 2521
	2521 -> 2522
	2522 -> 2523
	2523 -> 2524
	2524 -> 2525
	2524 -> 2539
	2525 -> 2526
	2526 -> 2527
	2527 -> 2528
	2528 -> 2529
	2528 -> 2531
	2529 -> 2530
	2530 -> 2535
	2531 -> 2532
	2532 -> 2533
	2533 -> 2534
	2534 -> 2535
	2535 -> 2536
	2536 -> 2537
	2537 -> 2538
	2538 -> 2539
	2539 -> 2540
	2540 -> 2541
	2541 -> 2542
	2542 -> 2543
	2543 -> 2544
	2544 -> 2545
	2545 -> 2546
	2546 -> 1
} 
